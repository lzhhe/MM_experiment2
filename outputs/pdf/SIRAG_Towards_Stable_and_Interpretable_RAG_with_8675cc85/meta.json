{
  "pdf_path_rel": "../../../papers/NLP/SIRAG Towards Stable and Interpretable RAG with.pdf",
  "pdf_path_abs": "E:\\Desktop\\M502083B å¤šæ¨¡æ€æœºå™¨å­¦ä¹ \\MM_experiment2\\papers\\NLP\\SIRAG Towards Stable and Interpretable RAG with.pdf",
  "num_pages": 5,
  "full_text_clean": "XXX-X-XXXX-XXXX-X/XX/$XX.00 Â©20XX IEEE SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework Junlin Wangâ€  Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China 953620519@qq.com Zehao Wuâ€  School of Automation Science and Engineering South China University of Technology Guangzhou, China auzhwu@mail.scut.edu.cn Shaowei Lu Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China 416149366@qq.com Yanlan Li Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China 191732947@qq.com Xinghao Huang* Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China *Corresponding author: ouxiangpoul@126.com â€ : These authors contributed equally to this work Abstractâ€”Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access external knowledge sources, but the effectiveness of RAG relies on the coordination between the retriever and the generator. Since these components are developed independently, their interaction is often suboptimal: the retriever may return irrelevant or redundant documents, while the generator may fail to fully leverage retrieved evidence. In this work, we propose a process-supervised multi-agent framework to bridge the gap between retriever and generator. The framework introduces two lightweight agents: a Decision Maker, which determines when to continue retrieval or stop for answer generation, and a Knowledge Selector, which filters retrieved documents to retain only the most useful evidence. To provide finegrained supervision, we employ an LLM-as-a-Judge that evaluates each intermediate action with process-level rewards, ensuring more accurate credit assignment than relying solely on final answer correctness. We further adopt a tree-structured rollout strategy to explore diverse reasoning paths, and train both agents with Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on single-hop and multi-hop question answering benchmarks show that our approach achieves higher accuracy, more stable convergence, and produces more interpretable reasoning trajectories compared with standard RAG baselines. Importantly, the proposed framework is modular and plug-and-play, requiring no modification to the retriever or generator, making it practical for real-world RAG applications. Keywordsâ€”Retrieval-Augmented Generation (RAG), MultiAgent Cooperation, Proximal Policy Optimization (PPO) I. INTRODUCTION Large language models (LLMs) have demonstrated remarkable capabilities in knowledge-intensive tasks such as open-domain question answering and reasoning. However, they remain limited by their static parametric knowledge and are prone to hallucinations.[1]-[3] To mitigate these issues, Retrieval-Augmented Generation (RAG) has become a widely adopted paradigm [4], where an external retriever fetches potentially relevant documents to ground the generation processs[5]. Despite its success, the effectiveness of RAG still hinges on the tight coordination between the retriever and the generator. Since these components are typically developed independently, they often suffer from semantic and functional misalignment: the retriever may provide documents that are irrelevant or redundant, while the generator may fail to formulate effective queries or fully leverage the retrieved evidence. Existing approaches attempt to bridge this gap through retriever fine-tuning, generator adaptation, or introducing intermediate modules such as rerankers or query rewriters[6]- [11]. While these methods offer partial improvements, they also face notable limitations: retriever fine-tuning requires curated data and cannot be easily applied to commercial search engines[12][13], generator fine-tuning is computationally expensive and risks degrading pretrained capabilities[14], and task-specific intermediate modules usually optimize only a single stage, leading to suboptimal coordination across the entire pipeline[6][14]. Thus, there is a strong need for a lightweight yet general mechanism that can flexibly coordinate retriever and generator behaviors without retraining them.\n\nFigure 1: Overall framework of SIRAG. Left: The proposed lightweight multi-agent system with collaborative strategies. Right: The end-to-end optimization pipeline for our multi-agent system. To address this challenge, we propose a process-supervised multi-agent framework that explicitly models and optimizes the decision process connecting retrieval and generation. Our framework introduces two lightweight agents: a Decision Maker, which decides whether to continue retrieval, reformulate queries, or hand over to the generator, and a Knowledge Selector, which filters retrieved documents to ensure that only the most relevant and useful evidence is passed to the generator. These agents interact cooperatively to form reasoning trajectories that bridge the gap between retriever and generator. A key novelty of our approach is the use of LLM-as-Judge to provide process-level supervision. Instead of relying solely on the correctness of the final generated answer as a sparse reward signal, we employ a strong LLM to evaluate each intermediate action, assigning fine-grained credit to decisions such as query formulation and evidence selection. This not only mitigates the credit assignment problem but also improves training stability and interpretability. To further enhance learning efficiency, we adopt a tree-structured rollout strategy to explore multiple reasoning paths, and optimize agent policies end-to-end with Proximal Policy Optimization (PPO). We validate our method on multiple single-hop and multihop question answering benchmarks. Experimental results demonstrate that our process-supervised proxy framework achieves superior answer accuracy compared to strong baselines, while also yielding more interpretable reasoning trajectories. Our key contributions are as follows: ï¬ We propose a multi-agent framework with Decision Maker and Knowledge Selector agents to coordinate retrieverâ€“ generator interactions. ï¬ We introduce LLM-as-a-Judge process supervision, providing fine-grained rewards for intermediate actions to improve stability and interpretability. ï¬ We develop a tree-structured rollout with PPO optimization to enable effective end-to-end training of cooperative agents. ï¬ Experiments on diverse QA benchmarks show improved accuracy and reasoning quality, while maintaining plugand-play modularity. II. METHODS In this section, we provide a detailed explanation of the implementation of SIRAG, with the overall architecture illustrated in Figure 1. A. Problem Formulation We consider the task of retrieval-augmented generation(RAG), where the goal is to answer a question ğ‘ğ‘ by leveraging both an external retriever and a generator. Let â„›(ğ‘ğ‘) denote the retriever returning a set of documents, and ğ’¢ğ’¢(â‹…) denote the generator producing the final answer. The key challenge is to ensure that the retrieval process and the generator are well aligned so that the retrieved documents are useful for the generation step. Instead of directly optimizing the retriever or the generator, we introduce a multi-agent proxy system to control the intermediate decision-making process. Formally, we design two cooperative agents: ï¬ Decision Maker (DM): decides whether to issue a new query, terminate retrieval, or hand over to the generator. ï¬ Knowledge Selector (KS): filters the retrieved documents to select the most relevant subset for the generator. Each agent operate in a partially observable environment and interacts sequentially, forming an action trajectory ğœğœ= {(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)}ğ‘¡ğ‘¡=1 ğ‘‡ğ‘‡ . The objective is to learn optimal policies ğœ‹ğœ‹ğœƒğœƒ ğ·ğ·ğ·ğ· and ğœ‹ğœ‹ğœƒğœƒ ğ¾ğ¾ğ¾ğ¾ that maximize expected task performance. B. Multi-agent interaction At each step, the Decision Maker observes the current question, accumulated evidence, and reasoning state ğ‘ ğ‘ ğ‘¡ğ‘¡ ğ·ğ·ğ·ğ· , and chooses one of the following actions: ğ‘ğ‘ğ‘¡ğ‘¡ ğ·ğ·ğ·ğ·âˆˆ{ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘), ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†&ğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğº} If a retrieval action is chosen, a query is sent to the retriever, and the retrieved documents are passed to the Knowledge\n\nSelector. The Knowledge Selector then observes (ğ‘ğ‘, â„›(ğ‘ğ‘), ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) and selects a subset of documents: ğ‘ğ‘ğ‘¡ğ‘¡ ğ¾ğ¾ğ¾ğ¾= ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘‘ğ‘‘1, ğ‘‘ğ‘‘2, â€¦ , ğ‘‘ğ‘‘ğ‘›ğ‘›) âŠ†â„›(ğ‘ğ‘) These filtered documents are appended to the evidence pool. The process repeats until the Decision Maker outputs Stop & Generate, in which case all accumulated evidence is passed to the generator ğ’¢ğ’¢ for answer synthesis. C. LLM-as-a-Judge: Process-Level Reward Modeling To collect diverse trajectories for training, we adopt a treestructured rollout strategy. For each input question, the Decision Maker is forced to explore multiple reasoning strategies at the top level (e.g., retrieving vs. stopping early), while deeper levels are expanded stochastically. This results in a decision tree where each path corresponds to a possible reasoning trajectory: ğœğœ= {(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)}ğ‘¡ğ‘¡=1 ğ‘‡ğ‘‡ , ğ‘…ğ‘…(ğœğœ) âˆˆàµ›0,1àµŸ àµ«1àµ¯ Where ğ‘…ğ‘…(ğœğœ) is the system-level reward (final answer correctness). This rollout ensures that both simple and complex strategies are evaluated, providing a richer training signal than single-path exploration. System-level rewards alone are sparse and insufficient for credit assignment. To address this, we propose using a strong LLM (e.g., GPT-4, Qwen2-72B) as a process supervisor to evaluate the quality of each intermediate action. For each node (ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) in the rollout tree, the LLM judge provides a score: ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) âˆˆàµ£0,1àµ§ àµ«2àµ¯ reflecting whether the action is reasonable, informative, and consistent with the question intent. The final credit for each action combines system- and process-level signals: ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) = ğ›¼ğ›¼âˆ™ğ‘…ğ‘…(ğœğœ) + ğ›½ğ›½âˆ™ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) àµ«3àµ¯ Where ğ›¼ğ›¼, ğ›½ğ›½ are trade-off coefficients. D. Policy Optimization with PPO We train both agents with Proximal Policy Optimization (PPO), a widely used reinforcement learning algorithm. Having obtained the credit rewards that reflect each agentâ€™s contribution, we develop an optimization framework to guide end-to-end training across all agents. The key idea is to use these credit signals for optimizing the collaborative behavior of the entire system. The optimization objective for our multi-agent system can be formulated as maximizing the expected credit rewards: ğ’¥ğ’¥(ğœƒğœƒ) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡ ğ‘–ğ‘–, ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘–ğ‘–àµ¯ ğ‘¡ğ‘¡ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– àµ© àµ«4àµ¯ Since each agentâ€™s action is a sequence of tokens, we decompose this optimization using Proximal Policy Optimization (PPO) [16][17][18] as follows: â„’ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†= à·â„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ ğ‘–ğ‘– (ğœƒğœƒ, ğœ‘ğœ‘) ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– àµ«5àµ¯ Specifically, for each agent ğ‘–ğ‘–, we define: â„’ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ ğ‘–ğ‘– (ğœƒğœƒ) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·min (ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘–(ğœƒğœƒ)ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– , ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘–(ğœƒğœƒ), ğ‘šğ‘š ğ‘¡ğ‘¡ 1 âˆ’ğœ€ğœ€, 1 + ğœ€ğœ€)ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– )àµ© (6) Where ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘–(ğœƒğœƒ) = ğœ‹ğœ‹ğœƒğœƒ(ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– |ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– ) ğœ‹ğœ‹ğœƒğœƒğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ(ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– |ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– ) is the probability ratio, ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– represents the concatenation of current state and the first ğ‘šğ‘šâˆ’1 tokens in the action sequence for agent ğ‘–ğ‘– at time step ğ‘¡ğ‘¡, and ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– denotes its ğ‘šğ‘š-ğ‘¡ğ‘¡â„ token. We compute the advantage estimate using GAE[15]:ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– = âˆ‘ (ğ›¾ğ›¾ğ›¾ğ›¾)ğ‘™ğ‘™ğ›¿ğ›¿ğ‘¡ğ‘¡,ğ‘šğ‘š+ğ‘™ğ‘™ ğ‘–ğ‘– ğ‘€ğ‘€âˆ’ğ‘šğ‘šâˆ’1 ğ‘™ğ‘™=0 , where ğ‘€ğ‘€ is the token length of the action sequence. To estimate state values across the multi-agent system, we employ a centralized state-value function ğ‘‰ğ‘‰ğœ™ğœ™ that takes each agentâ€™s state ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– as input. The value function is optimized to minimize the mean squared error: â„’ğ‘‰ğ‘‰ ğ‘–ğ‘–(ğœ™ğœ™) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·àµ«ğ‘‰ğ‘‰ğœ™ğœ™àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– àµ¯âˆ’ğºğºà· ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– àµ¯ 2 ğ‘šğ‘š ğ‘¡ğ‘¡ àµ© àµ«7àµ¯ Where ğºğºà· ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– = ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– + ğ‘‰ğ‘‰ğœ™ğœ™àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– àµ¯ is the empirical return. The final optimization objective combines the policy and value losses: â„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ ğ‘–ğ‘– (ğœƒğœƒ, ğœ™ğœ™) = â„’ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ ğ‘–ğ‘– (ğœƒğœƒ) + ğ‘ğ‘ğ‘£ğ‘£â„’ğ‘‰ğ‘‰ ğ‘–ğ‘–(ğœ™ğœ™) àµ«8àµ¯ Where ğ‘ğ‘ğ‘£ğ‘£ controls the weight of the value loss. This joint objective enables end-to-end training of both policy and value networks across all agents. III. RESULTS&DISCUSSION In this section, we first present the datasets and implementation details, followed by a comparison of our SIRAG with the latest RAG methods with analysis. A. Datasets To comprehensively evaluate our SIRAG, we experiment on both single-hop datasets including Natural Questions (NQ) and PopQA, as well as multi-hop datasets including 2WikiMultiHopQA (2Wiki), and HotpotQA (HQA). For each dataset, we only use 100 randomly sampled questions instead of the full training set. B. Experimental Details Following Asai et al.[8], we construct our retrieval system using the 2018 Wikipedia dump as the knowledge source and use contriever-msmarco as our dense retriever. We utilize Qwen2.5-7B-Instruct as fixed LLM server, while Qwen2-0.5B is trained as candidate lightweight agent for efficient edge deployment. In the warm-up phase, we collect 4 solutions for each question with Qwen2.5-7B-Instruct. We use a learning rate of 4e-5, with 2 epochs and a batch size of 4. For the RL phase, we set learning rate of 5e-7 for policy model and 5e-6 for value model with a batch size of 2 and maximal depth of 5.\n\nC. Performance Comparison We report the performance on both single-hop and multi-hop datasets in Table 1. First, our SIRAG consistently outperforms various baselines across different datasets, achieving superior average performance of 48.23% with lightweight agents of only 0.5B parameters. This demonstrates the effectiveness of our agent-centric alignment approach in bridging the gap between the retriever and the LLM. Second, compared to single-hop datasets, our method yields particularly notable gains in challenging multi-hop reasoning tasks. Specifically, SIRAG achieves significant improvements on multi-hop datasets (2Wiki +9.3%, HQA +9.2%), while maintaining strong performance on single-hop tasks (PopQA +1.8%). This significant performance gain suggests that, even without training original RAG system, our SIRAG effectively enhances the coordination between the retriever and LLM, which is particularly crucial for addressing complex multi-hop tasks. Third, although retriever fine-tuning method requires fewer tuned parameters, it does not overcome the limitations of standard RAG systems in handling complex cognitive and multi-hop reasoning tasks. Both LLM fine-tuning and intermediate module methods show promising results, but are constrained by either large tuning parameters (7B/72B) or inconsistent performance across different reasoning datasets. In contrast, our SIRAG achieves consistent improvements across almost all datasets with only 0.5B additional parameters, demonstrating both efficiency and effectiveness in enhancing RAG systems. Table 1 Main results. Comparison of EM(%) on four datasets. method Agent tuned params 2Wiki HQA NQ PopQA Average direct / - 31.6 35.4 43.3 24.3 33.65 standard / - 27.3 42.1 50.8 30.0 37.55 Reranker[19] 7B 7B 26.4 37.2 47.6 20.9 33.02 Query-Writer[6] 1.5B 1.5B 32.3 36.8 53.1 30.5 38.18 selfRAG[8] / 7B 35.9 45.5 50.4 31.5 40.83 SIRAG(ours) 0.5b 0.5b 45.2 54.7 51.7 33.3 46.23 D. Ablation Study To thoroughly evaluate the effectiveness of different components in our training process, we conduct comprehensive ablation studies across four in-domain datasets. Specifically, we examine the following variants: (1) â€œw/o LLM judgeâ€: A variant without the tree-structured rollout and LLM-as-a-judge credit assignment, meaning that we directly optimize each agent using the system-level reward (a single trajectory). (2) â€œw/o RLâ€: The performance in the supervised warm-up phase. The experimental results reveal several key findings. First, removing the tree-structured rollout (and LLM-as-a-judge credit assignment) leads to unstable performance during the RL phase, occasionally degrading below the supervised warm-up model. This degradation can be attributed to the direct use of systemlevel rewards as supervised signals for all agents, which fails to accurately assess individual agent contributions and may mask detrimental actions within successful trajectories. In contrast, our LLM-as-a-judge credit assignment mechanism enables reward allocation in probabilistic expectation through treestructured exploration, ensuring that each agent receives appropriate feedback for its specific actions. Second, comparing with the supervised warm-up model (â€œw/o RLâ€), our SIRAG achieves substantial improvements across all datasets. This performance boost demonstrates that end-to-end RL optimization effectively aligns the behaviors of multiple agents towards the system-level objectives, going beyond the limitations of supervised learning that only optimizes for local agents. IV. CONCLUSIONS In this paper, we proposed a process-supervised multi-agent framework for retrieval-augmented generation. By introducing two lightweight agents, the Decision Maker and the Knowledge Selector, our approach explicitly coordinates the interaction between the retriever and the generator. To address the challenge of sparse and unstable rewards, we employed an LLM-as-Judge to provide process-level supervision for each intermediate action, enabling more accurate credit assignment. Combined with a tree-structured rollout strategy and end-to-end optimization using PPO, our framework achieves stable training, interpretable reasoning trajectories, and improved accuracy on both singlehop and multi-hop QA tasks. ACKNOWLEDGMENT This work was supported by the International Science and Technology Cooperation Project of Guangzhou Economic and Technological Development District (No.2023GH16). Figure 2 Ablation Study\n\nREFERENCES [1] Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., et al. Sirenâ€™s song in the ai ocean: a survey on hallucination in large language models. ArXiv abs/:2309.01219 [2] Sahoo, S. S., Plasek, J. M., Xu, H., Uzuner, ÌˆO., Cohen, T., Yetisgen, M., Liu, H., Meystre, S., and Wang, Y. Large language models for biomedicine: foundations, opportunities, challenges, and best practices. Journal of the American Medical Informatics Association, pp. ocae074, 2024. [3] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1â€“38, 2023. [4] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Ku Ìˆttler, H., Lewis, M., Yih, W.-t., Rockta Ìˆschel, T., et al. Retrievalaugmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459â€“9474, 2020. [5] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023. [6] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan. Query rewriting in retrieval-augmented large language models. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 5303â€“5315. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.322. URL https://doi.org/10.18653/v1/2023. emnlp-main.322. [7] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W. Yih. REPLUG: retrieval-augmented black-box language models. In K. Duh, H. GÃ³mez-Adorno, and S. Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 8371â€“8384. [8] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [9] Z. Wei, W. Chen, and Y. Meng. Instructrag: Instructing retrievalaugmented generation with explicit denoising. CoRR, abs/2406.13629, 2024. doi: 10.48550/ARXIV.2406.13629. [10] T. Yu, S. Zhang, and Y. Feng. Auto-rag: Autonomous retrievalaugmented generation for large language models. CoRR, abs/2411.19443, 2024. doi: 10.48550/ARXIV.2411.19443. [11] Y. Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. Catanzaro. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. CoRR, abs/2407.02485, 2024. doi: 10.48550/ARXIV.2407.02485. [12] E. Schmidt. How google works. Hachette UK, 2014. [13] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021. [14] Y. Zhou, Y. Liu, X. Li, J. Jin, H. Qian, Z. Liu, C. Li, Z. Dou, T. Ho, and P. S. Yu. Trustworthiness in retrieval-augmented generation systems: A survey. CoRR, abs/2409.10102, 2024. doi: 10.48550/ARXIV.2409.10102. [15] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. Highdimensional continuous control using generalized advantage estimation. In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. [16] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. [17] L. Yuan, Z. Zhang, L. Li, C. Guan, and Y. Yu. A survey of progress on cooperative multi-agent reinforcement learning in open environment. CoRR, abs/2312.01058, 2023. doi: 10.48550/ ARXIV.2312.01058. [18] C. Zhu, M. Dastani, and S. Wang. A survey of multi-agent deep reinforcement learning with communication. Auton. Agents Multi Agent Syst., 38(1):4, 2024. doi: 10.1007/S10458-023-09633-6. [19] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang. Towards general text embeddings with multi-stage contrastive learning. CoRR, abs/2308.03281, 2023. doi: 10.48550/ARXIV. [20] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.",
  "pages": [
    {
      "page": 1,
      "text_raw": "XXX-X-XXXX-XXXX-X/XX/$XX.00 Â©20XX IEEE \nSIRAG: Towards Stable and Interpretable RAG with \nA Process-Supervised Multi-Agent Framework  \nJunlin Wangâ€   \nHeyuan Tobacco Monopoly Administration \nXinyuan Road, Yuancheng District \nHeyuan, China \n953620519@qq.com \n \nZehao Wuâ€  \nSchool of Automation Science and Engineering \nSouth China University of Technology \nGuangzhou, China \nauzhwu@mail.scut.edu.cn \n \nShaowei Lu  \nHeyuan Tobacco Monopoly Administration \nXinyuan Road, Yuancheng District \nHeyuan, China \n416149366@qq.com \n \nYanlan Li  \nHeyuan Tobacco Monopoly Administration \nXinyuan Road, Yuancheng District \nHeyuan, China \n191732947@qq.com \nXinghao Huang*  \nHeyuan Tobacco Monopoly Administration \nXinyuan Road, Yuancheng District \nHeyuan, China \n*Corresponding author: ouxiangpoul@126.com  \nâ€ : These authors contributed equally to this work \nAbstractâ€”Retrieval-Augmented Generation (RAG) enables \nlarge language models (LLMs) to access external knowledge \nsources, but the effectiveness of RAG relies on the coordination \nbetween the retriever and the generator. Since these components \nare developed independently, their interaction is often suboptimal: \nthe retriever may return irrelevant or redundant documents, \nwhile the generator may fail to fully leverage retrieved evidence. \nIn this work, we propose a process-supervised multi-agent \nframework to bridge the gap between retriever and generator. The \nframework introduces two lightweight agents: a Decision Maker, \nwhich determines when to continue retrieval or stop for answer \ngeneration, and a Knowledge Selector, which filters retrieved \ndocuments to retain only the most useful evidence. To provide fine-\ngrained supervision, we employ an LLM-as-a-Judge that evaluates \neach intermediate action with process-level rewards, ensuring \nmore accurate credit assignment than relying solely on final \nanswer correctness. We further adopt a tree-structured rollout \nstrategy to explore diverse reasoning paths, and train both agents \nwith Proximal Policy Optimization (PPO) in an end-to-end \nmanner. Experiments on single-hop and multi-hop question \nanswering benchmarks show that our approach achieves higher \naccuracy, more stable convergence, and produces more \ninterpretable reasoning trajectories compared with standard \nRAG baselines. Importantly, the proposed framework is modular \nand plug-and-play, requiring no modification to the retriever or \ngenerator, making it practical for real-world RAG applications. \nKeywordsâ€”Retrieval-Augmented Generation (RAG), Multi-\nAgent Cooperation, Proximal Policy Optimization (PPO) \nI. INTRODUCTION \nLarge language models (LLMs) have demonstrated \nremarkable capabilities in knowledge-intensive tasks such as \nopen-domain question answering and reasoning. However, they \nremain limited by their static parametric knowledge and are \nprone to hallucinations.[1]-[3] To mitigate these issues, \nRetrieval-Augmented Generation (RAG) has become a \nwidely adopted paradigm [4], where an external retriever fetches \npotentially relevant documents to ground the generation \nprocesss[5]. Despite its success, the effectiveness of RAG still \nhinges on the tight coordination between the retriever and the \ngenerator. Since these components are typically developed \nindependently, they often suffer from semantic and functional \nmisalignment: the retriever may provide documents that are \nirrelevant or redundant, while the generator may fail to \nformulate effective queries or fully leverage the retrieved \nevidence. \nExisting approaches attempt to bridge this gap through \nretriever fine-tuning, generator adaptation, or introducing \nintermediate modules such as rerankers or query rewriters[6]-\n[11]. While these methods offer partial improvements, they also \nface notable limitations: retriever fine-tuning requires curated \ndata and cannot be easily applied to commercial search \nengines[12][13], generator fine-tuning is computationally \nexpensive and risks degrading pretrained capabilities[14], and \ntask-specific intermediate modules usually optimize only a \nsingle stage, leading to suboptimal coordination across the entire \npipeline[6][14]. Thus, there is a strong need for a lightweight \nyet general mechanism that can flexibly coordinate retriever \nand generator behaviors without retraining them.\n",
      "text_clean": "XXX-X-XXXX-XXXX-X/XX/$XX.00 Â©20XX IEEE SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework Junlin Wangâ€  Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China 953620519@qq.com Zehao Wuâ€  School of Automation Science and Engineering South China University of Technology Guangzhou, China auzhwu@mail.scut.edu.cn Shaowei Lu Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China 416149366@qq.com Yanlan Li Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China 191732947@qq.com Xinghao Huang* Heyuan Tobacco Monopoly Administration Xinyuan Road, Yuancheng District Heyuan, China *Corresponding author: ouxiangpoul@126.com â€ : These authors contributed equally to this work Abstractâ€”Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access external knowledge sources, but the effectiveness of RAG relies on the coordination between the retriever and the generator. Since these components are developed independently, their interaction is often suboptimal: the retriever may return irrelevant or redundant documents, while the generator may fail to fully leverage retrieved evidence. In this work, we propose a process-supervised multi-agent framework to bridge the gap between retriever and generator. The framework introduces two lightweight agents: a Decision Maker, which determines when to continue retrieval or stop for answer generation, and a Knowledge Selector, which filters retrieved documents to retain only the most useful evidence. To provide finegrained supervision, we employ an LLM-as-a-Judge that evaluates each intermediate action with process-level rewards, ensuring more accurate credit assignment than relying solely on final answer correctness. We further adopt a tree-structured rollout strategy to explore diverse reasoning paths, and train both agents with Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on single-hop and multi-hop question answering benchmarks show that our approach achieves higher accuracy, more stable convergence, and produces more interpretable reasoning trajectories compared with standard RAG baselines. Importantly, the proposed framework is modular and plug-and-play, requiring no modification to the retriever or generator, making it practical for real-world RAG applications. Keywordsâ€”Retrieval-Augmented Generation (RAG), MultiAgent Cooperation, Proximal Policy Optimization (PPO) I. INTRODUCTION Large language models (LLMs) have demonstrated remarkable capabilities in knowledge-intensive tasks such as open-domain question answering and reasoning. However, they remain limited by their static parametric knowledge and are prone to hallucinations.[1]-[3] To mitigate these issues, Retrieval-Augmented Generation (RAG) has become a widely adopted paradigm [4], where an external retriever fetches potentially relevant documents to ground the generation processs[5]. Despite its success, the effectiveness of RAG still hinges on the tight coordination between the retriever and the generator. Since these components are typically developed independently, they often suffer from semantic and functional misalignment: the retriever may provide documents that are irrelevant or redundant, while the generator may fail to formulate effective queries or fully leverage the retrieved evidence. Existing approaches attempt to bridge this gap through retriever fine-tuning, generator adaptation, or introducing intermediate modules such as rerankers or query rewriters[6]- [11]. While these methods offer partial improvements, they also face notable limitations: retriever fine-tuning requires curated data and cannot be easily applied to commercial search engines[12][13], generator fine-tuning is computationally expensive and risks degrading pretrained capabilities[14], and task-specific intermediate modules usually optimize only a single stage, leading to suboptimal coordination across the entire pipeline[6][14]. Thus, there is a strong need for a lightweight yet general mechanism that can flexibly coordinate retriever and generator behaviors without retraining them.",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 2,
      "text_raw": " \nFigure 1: Overall framework of SIRAG. Left: The proposed lightweight multi-agent system with collaborative strategies.  \nRight: The end-to-end optimization pipeline for our multi-agent system. \n \nTo address this challenge, we propose a process-supervised \nmulti-agent framework that explicitly models and optimizes \nthe decision process connecting retrieval and generation. Our \nframework introduces two lightweight agents: a Decision \nMaker, which decides whether to continue retrieval, \nreformulate queries, or hand over to the generator, and a \nKnowledge Selector, which filters retrieved documents to \nensure that only the most relevant and useful evidence is passed \nto the generator. These agents interact cooperatively to form \nreasoning trajectories that bridge the gap between retriever and \ngenerator. \nA key novelty of our approach is the use of LLM-as-Judge \nto provide process-level supervision. Instead of relying solely \non the correctness of the final generated answer as a sparse \nreward signal, we employ a strong LLM to evaluate each \nintermediate action, assigning fine-grained credit to decisions \nsuch as query formulation and evidence selection. This not only \nmitigates the credit assignment problem but also improves \ntraining stability and interpretability. To further enhance \nlearning efficiency, we adopt a tree-structured rollout \nstrategy to explore multiple reasoning paths, and optimize agent \npolicies end-to-end with Proximal Policy Optimization (PPO). \nWe validate our method on multiple single-hop and multi-\nhop question answering benchmarks. Experimental results \ndemonstrate that our process-supervised proxy framework \nachieves superior answer accuracy compared to strong baselines, \nwhile also yielding more interpretable reasoning trajectories. \nOur key contributions are as follows: \nï¬ We propose a multi-agent framework with Decision Maker \nand Knowledge Selector agents to coordinate retrieverâ€“\ngenerator interactions. \nï¬ We introduce LLM-as-a-Judge process supervision, \nproviding fine-grained rewards for intermediate actions to \nimprove stability and interpretability. \nï¬ We develop a tree-structured rollout with PPO optimization \nto enable effective end-to-end training of cooperative \nagents. \n \n \nï¬ Experiments on diverse QA benchmarks show improved \naccuracy and reasoning quality, while maintaining plug-\nand-play modularity. \nII. METHODS \nIn this section, we provide a detailed explanation of the \nimplementation of SIRAG, with the overall architecture \nillustrated in Figure 1. \nA. Problem Formulation \nWe \nconsider \nthe \ntask \nof \nretrieval-augmented \ngeneration(RAG), where the goal is to answer a question ğ‘ğ‘ by \nleveraging both an external retriever and a generator. Let â„›(ğ‘ğ‘) \ndenote the retriever returning a set of documents, and ğ’¢ğ’¢(â‹…) \ndenote the generator producing the final answer. The key \nchallenge is to ensure that the retrieval process and the generator \nare well aligned so that the retrieved documents are useful for \nthe generation step. \nInstead of directly optimizing the retriever or the generator, \nwe introduce a multi-agent proxy system to control the \nintermediate decision-making process. Formally, we design two \ncooperative agents: \nï¬ \nDecision Maker (DM): decides whether to issue a new \nquery, terminate retrieval, or hand over to the generator. \nï¬ \nKnowledge Selector (KS): filters the retrieved \ndocuments to select the most relevant subset for the \ngenerator. \nEach agent operate in a partially observable environment and \ninteracts sequentially, forming an action trajectory ğœğœ=\n{(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)}ğ‘¡ğ‘¡=1\nğ‘‡ğ‘‡\n. The objective is to learn optimal policies ğœ‹ğœ‹ğœƒğœƒ\nğ·ğ·ğ·ğ· and \nğœ‹ğœ‹ğœƒğœƒ\nğ¾ğ¾ğ¾ğ¾ that maximize expected task performance. \nB. Multi-agent interaction \nAt each step, the Decision Maker observes the current \nquestion, accumulated evidence, and reasoning state ğ‘ ğ‘ ğ‘¡ğ‘¡\nğ·ğ·ğ·ğ· , and \nchooses one of the following actions: \nğ‘ğ‘ğ‘¡ğ‘¡\nğ·ğ·ğ·ğ·âˆˆ{ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘), ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†&ğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğº} \nIf a retrieval action is chosen, a query is sent to the retriever, \nand the retrieved documents are passed to the Knowledge \n",
      "text_clean": "Figure 1: Overall framework of SIRAG. Left: The proposed lightweight multi-agent system with collaborative strategies. Right: The end-to-end optimization pipeline for our multi-agent system. To address this challenge, we propose a process-supervised multi-agent framework that explicitly models and optimizes the decision process connecting retrieval and generation. Our framework introduces two lightweight agents: a Decision Maker, which decides whether to continue retrieval, reformulate queries, or hand over to the generator, and a Knowledge Selector, which filters retrieved documents to ensure that only the most relevant and useful evidence is passed to the generator. These agents interact cooperatively to form reasoning trajectories that bridge the gap between retriever and generator. A key novelty of our approach is the use of LLM-as-Judge to provide process-level supervision. Instead of relying solely on the correctness of the final generated answer as a sparse reward signal, we employ a strong LLM to evaluate each intermediate action, assigning fine-grained credit to decisions such as query formulation and evidence selection. This not only mitigates the credit assignment problem but also improves training stability and interpretability. To further enhance learning efficiency, we adopt a tree-structured rollout strategy to explore multiple reasoning paths, and optimize agent policies end-to-end with Proximal Policy Optimization (PPO). We validate our method on multiple single-hop and multihop question answering benchmarks. Experimental results demonstrate that our process-supervised proxy framework achieves superior answer accuracy compared to strong baselines, while also yielding more interpretable reasoning trajectories. Our key contributions are as follows: ï¬ We propose a multi-agent framework with Decision Maker and Knowledge Selector agents to coordinate retrieverâ€“ generator interactions. ï¬ We introduce LLM-as-a-Judge process supervision, providing fine-grained rewards for intermediate actions to improve stability and interpretability. ï¬ We develop a tree-structured rollout with PPO optimization to enable effective end-to-end training of cooperative agents. ï¬ Experiments on diverse QA benchmarks show improved accuracy and reasoning quality, while maintaining plugand-play modularity. II. METHODS In this section, we provide a detailed explanation of the implementation of SIRAG, with the overall architecture illustrated in Figure 1. A. Problem Formulation We consider the task of retrieval-augmented generation(RAG), where the goal is to answer a question ğ‘ğ‘ by leveraging both an external retriever and a generator. Let â„›(ğ‘ğ‘) denote the retriever returning a set of documents, and ğ’¢ğ’¢(â‹…) denote the generator producing the final answer. The key challenge is to ensure that the retrieval process and the generator are well aligned so that the retrieved documents are useful for the generation step. Instead of directly optimizing the retriever or the generator, we introduce a multi-agent proxy system to control the intermediate decision-making process. Formally, we design two cooperative agents: ï¬ Decision Maker (DM): decides whether to issue a new query, terminate retrieval, or hand over to the generator. ï¬ Knowledge Selector (KS): filters the retrieved documents to select the most relevant subset for the generator. Each agent operate in a partially observable environment and interacts sequentially, forming an action trajectory ğœğœ= {(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)}ğ‘¡ğ‘¡=1 ğ‘‡ğ‘‡ . The objective is to learn optimal policies ğœ‹ğœ‹ğœƒğœƒ ğ·ğ·ğ·ğ· and ğœ‹ğœ‹ğœƒğœƒ ğ¾ğ¾ğ¾ğ¾ that maximize expected task performance. B. Multi-agent interaction At each step, the Decision Maker observes the current question, accumulated evidence, and reasoning state ğ‘ ğ‘ ğ‘¡ğ‘¡ ğ·ğ·ğ·ğ· , and chooses one of the following actions: ğ‘ğ‘ğ‘¡ğ‘¡ ğ·ğ·ğ·ğ·âˆˆ{ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘), ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†&ğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğºğº} If a retrieval action is chosen, a query is sent to the retriever, and the retrieved documents are passed to the Knowledge",
      "figure_captions": [
        {
          "bbox": [
            63.536399841308594,
            213.44052124023438,
            553.4141845703125,
            226.93746948242188
          ],
          "text": "Figure 1: Overall framework of SIRAG. Left: The proposed lightweight multi-agent system with collaborative strategies."
        }
      ],
      "images": [
        {
          "image_id": "p2_cluster_001",
          "page": 2,
          "file_path": "images/page_0002_cluster_001.png",
          "bbox": [
            43.69991683959961,
            66.89990234375,
            566.5023193359375,
            214.505615234375
          ],
          "caption": "a diagram showing the process of creating a business model",
          "detailed_caption": "The image shows a diagram of a business process, with a flowchart depicting the different stages of the process. The flowchart is composed of several boxes connected by arrows, each box representing a step in the process, such as \"Identify the Problem,\" \"Develop a Plan,\" and \"Analyze the Results.\" The arrows indicate the direction of the flow, from the initial step to the final step. The boxes contain text that explains the purpose of each step and how they interact with each other.",
          "ocr_text": "Agents LLMS1-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46-47-48-50-51-52-53-54-55-56-57-58-59-60-61-62-63-64-65-66-67-68-69-70-71-72-73-74-75-76-77-78-79-80-81-82-83-84-85-86-87-88-89-90-91-92-93-94-95-96-97-98-99-100-102-103-104-105-106-107-108-109-110-112-113-114-115-116-117-118-119-120-122-130-131-132-133-134-135-136-137-138-139-140-150-170-172-173-174-175-176-176.177-172.173-173.172-174.174-173.-173-172.-173.173.174.175-173-.173.175.176.176-177.172.175, 176.172, 173.177.173, 173, 174, 176, 172.174, 173-176, 176-173, 172, 174.176, 177.176.-172.177, 172-175, 173.-172-172, 175, 174-174, 175-175.177.-172.-172",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>AgentsLLMS1-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46-47-48-50-51-52-53-54-55-56-57-58-59-60-61-62-63-64-65-66-67-68-69-70-71-72-73-74-75-76-77-78-79-80-81-82-83-84-85-86-87-88-89-90-91-92-93-94-95-96-97-98-99-100-102-103-104-105-106-107-108-109-110-112-113-114-115-116-117-118-119-120-122-130-131-132-133-134-135-136-137-138-139-140-150-170-172-173-174-175-176-176.177-172.173-173.172-174.174-173.-173-172.-173.173.174.175-173-.173.175.176.176-177.172.175,176.172,173.177.173,173,174,176,172.174,173-176,176-173,172,174.176,177.176.-172.177,172-175,173.-172-172,175,174-174,175-175.177.-172.-172</s>",
            "parsed": {
              "<OCR>": "AgentsLLMS1-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46-47-48-50-51-52-53-54-55-56-57-58-59-60-61-62-63-64-65-66-67-68-69-70-71-72-73-74-75-76-77-78-79-80-81-82-83-84-85-86-87-88-89-90-91-92-93-94-95-96-97-98-99-100-102-103-104-105-106-107-108-109-110-112-113-114-115-116-117-118-119-120-122-130-131-132-133-134-135-136-137-138-139-140-150-170-172-173-174-175-176-176.177-172.173-173.172-174.174-173.-173-172.-173.173.174.175-173-.173.175.176.176-177.172.175,176.172,173.177.173,173,174,176,172.174,173-176,176-173,172,174.176,177.176.-172.177,172-175,173.-172-172,175,174-174,175-175.177.-172.-172"
            }
          },
          "paper_caption": "Figure 1: Overall framework of SIRAG. Left: The proposed lightweight multi-agent system with collaborative strategies.",
          "paper_caption_bbox": [
            63.536399841308594,
            213.44052124023438,
            553.4141845703125,
            226.93746948242188
          ]
        }
      ]
    },
    {
      "page": 3,
      "text_raw": "Selector. \nThe \nKnowledge \nSelector \nthen \nobserves \n(ğ‘ğ‘, â„›(ğ‘ğ‘), ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) and selects a subset of documents: \nğ‘ğ‘ğ‘¡ğ‘¡\nğ¾ğ¾ğ¾ğ¾= ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘‘ğ‘‘1, ğ‘‘ğ‘‘2, â€¦ , ğ‘‘ğ‘‘ğ‘›ğ‘›) âŠ†â„›(ğ‘ğ‘) \nThese filtered documents are appended to the evidence pool. \nThe process repeats until the Decision Maker outputs Stop & \nGenerate, in which case all accumulated evidence is passed to \nthe generator ğ’¢ğ’¢ for answer synthesis. \nC. LLM-as-a-Judge: Process-Level Reward Modeling \nTo collect diverse trajectories for training, we adopt a tree-\nstructured rollout strategy. For each input question, the \nDecision Maker is forced to explore multiple reasoning \nstrategies at the top level (e.g., retrieving vs. stopping early), \nwhile deeper levels are expanded stochastically. This results in \na decision tree where each path corresponds to a possible \nreasoning trajectory: \nğœğœ= {(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)}ğ‘¡ğ‘¡=1\nğ‘‡ğ‘‡\n, ğ‘…ğ‘…(ğœğœ) âˆˆàµ›0,1àµŸ\nàµ«1àµ¯ \nWhere ğ‘…ğ‘…(ğœğœ)  is the system-level reward (final answer  \ncorrectness). This rollout ensures that both simple and complex \nstrategies are evaluated, providing a richer training signal than \nsingle-path exploration. \n \nSystem-level rewards alone are sparse and insufficient for \ncredit assignment. To address this, we propose using a strong \nLLM (e.g., GPT-4, Qwen2-72B) as a process supervisor to \nevaluate the quality of each intermediate action. For each node \n(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) in the rollout tree, the LLM judge provides a score: \nğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) âˆˆàµ£0,1àµ§\nàµ«2àµ¯ \nreflecting whether the action is reasonable, informative, and \nconsistent with the question intent. \nThe final credit for each action combines system- and \nprocess-level signals: \nğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) = ğ›¼ğ›¼âˆ™ğ‘…ğ‘…(ğœğœ) + ğ›½ğ›½âˆ™ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)\nàµ«3àµ¯ \nWhere ğ›¼ğ›¼, ğ›½ğ›½ are trade-off coefficients. \nD. Policy Optimization with PPO \n We train both agents with Proximal Policy Optimization \n(PPO), a widely used reinforcement learning algorithm. \nHaving obtained the credit rewards that reflect each agentâ€™s \ncontribution, we develop an optimization framework to guide \nend-to-end training across all agents. The key idea is to use these \ncredit signals for optimizing the collaborative behavior of the \nentire system. The optimization objective for our multi-agent \nsystem can be formulated as maximizing the expected credit \nrewards: \nğ’¥ğ’¥(ğœƒğœƒ) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡\nğ‘–ğ‘–, ğ‘ğ‘ğ‘¡ğ‘¡\nğ‘–ğ‘–àµ¯\nğ‘¡ğ‘¡\nğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–\nàµ©\nàµ«4àµ¯ \nSince each agentâ€™s action is a sequence of tokens, we \ndecompose \nthis \noptimization \nusing \nProximal \nPolicy \nOptimization (PPO) [16][17][18] as follows: \nâ„’ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†= à·â„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ\nğ‘–ğ‘–\n(ğœƒğœƒ, ğœ‘ğœ‘)\nğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–\nàµ«5àµ¯ \nSpecifically, for each agent ğ‘–ğ‘–, we define: \nâ„’ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶\nğ‘–ğ‘–\n(ğœƒğœƒ) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·min (ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–(ğœƒğœƒ)ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–(ğœƒğœƒ),\nğ‘šğ‘š\nğ‘¡ğ‘¡\n1 âˆ’ğœ€ğœ€, 1 + ğœ€ğœ€)ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n)àµ©                                      (6) \nWhere ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–(ğœƒğœƒ) =\nğœ‹ğœ‹ğœƒğœƒ(ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n|ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n)\nğœ‹ğœ‹ğœƒğœƒğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ(ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n|ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n) is the probability ratio, ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n \nrepresents the concatenation of current state and the first ğ‘šğ‘šâˆ’1 \ntokens in the action sequence for agent ğ‘–ğ‘– at time step ğ‘¡ğ‘¡, and ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n \ndenotes its ğ‘šğ‘š-ğ‘¡ğ‘¡â„ token. We compute the advantage estimate \nusing GAE[15]:ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n= âˆ‘\n(ğ›¾ğ›¾ğ›¾ğ›¾)ğ‘™ğ‘™ğ›¿ğ›¿ğ‘¡ğ‘¡,ğ‘šğ‘š+ğ‘™ğ‘™\nğ‘–ğ‘–\nğ‘€ğ‘€âˆ’ğ‘šğ‘šâˆ’1\nğ‘™ğ‘™=0\n, where ğ‘€ğ‘€ is the \ntoken length of the action sequence. \nTo estimate state values across the multi-agent system, we \nemploy a centralized state-value function ğ‘‰ğ‘‰ğœ™ğœ™ that takes each \nagentâ€™s state ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n as input. The value function is optimized to \nminimize the mean squared error:  \nâ„’ğ‘‰ğ‘‰\nğ‘–ğ‘–(ğœ™ğœ™) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·àµ«ğ‘‰ğ‘‰ğœ™ğœ™àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\nàµ¯âˆ’ğºğºà· ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\nàµ¯\n2\nğ‘šğ‘š\nğ‘¡ğ‘¡\nàµ©\nàµ«7àµ¯ \n \nWhere ğºğºà· ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n= ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\n+ ğ‘‰ğ‘‰ğœ™ğœ™àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š\nğ‘–ğ‘–\nàµ¯ is the empirical return. The \nfinal optimization objective combines the policy and value \nlosses: \nâ„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ\nğ‘–ğ‘–\n(ğœƒğœƒ, ğœ™ğœ™) = â„’ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶\nğ‘–ğ‘–\n(ğœƒğœƒ) + ğ‘ğ‘ğ‘£ğ‘£â„’ğ‘‰ğ‘‰\nğ‘–ğ‘–(ğœ™ğœ™)\nàµ«8àµ¯ \nWhere ğ‘ğ‘ğ‘£ğ‘£ controls the weight of the value loss. This joint \nobjective enables end-to-end training of both policy and value \nnetworks across all agents. \nIII. RESULTS&DISCUSSION \nIn this section, we first present the datasets and \nimplementation details, followed by a comparison of our \nSIRAG with the latest RAG methods with analysis. \nA. Datasets \nTo comprehensively evaluate our SIRAG, we experiment on \nboth single-hop datasets including Natural Questions (NQ) and \nPopQA, \nas \nwell \nas \nmulti-hop \ndatasets \nincluding \n2WikiMultiHopQA (2Wiki), and HotpotQA (HQA). For each \ndataset, we only use 100 randomly sampled questions instead of \nthe full training set. \nB. Experimental Details \nFollowing Asai et al.[8], we construct our retrieval system \nusing the 2018 Wikipedia dump as the knowledge source and \nuse contriever-msmarco as our dense retriever. We utilize \nQwen2.5-7B-Instruct as fixed LLM server, while Qwen2-0.5B \nis trained as candidate lightweight agent for efficient edge \ndeployment. In the warm-up phase, we collect 4 solutions for \neach question with Qwen2.5-7B-Instruct. We use a learning rate \nof 4e-5, with 2 epochs and a batch size of 4. For the RL phase, \nwe set learning rate of 5e-7 for policy model and 5e-6 for value \nmodel with a batch size of 2 and maximal depth of 5. \n",
      "text_clean": "Selector. The Knowledge Selector then observes (ğ‘ğ‘, â„›(ğ‘ğ‘), ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) and selects a subset of documents: ğ‘ğ‘ğ‘¡ğ‘¡ ğ¾ğ¾ğ¾ğ¾= ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘‘ğ‘‘1, ğ‘‘ğ‘‘2, â€¦ , ğ‘‘ğ‘‘ğ‘›ğ‘›) âŠ†â„›(ğ‘ğ‘) These filtered documents are appended to the evidence pool. The process repeats until the Decision Maker outputs Stop & Generate, in which case all accumulated evidence is passed to the generator ğ’¢ğ’¢ for answer synthesis. C. LLM-as-a-Judge: Process-Level Reward Modeling To collect diverse trajectories for training, we adopt a treestructured rollout strategy. For each input question, the Decision Maker is forced to explore multiple reasoning strategies at the top level (e.g., retrieving vs. stopping early), while deeper levels are expanded stochastically. This results in a decision tree where each path corresponds to a possible reasoning trajectory: ğœğœ= {(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡)}ğ‘¡ğ‘¡=1 ğ‘‡ğ‘‡ , ğ‘…ğ‘…(ğœğœ) âˆˆàµ›0,1àµŸ àµ«1àµ¯ Where ğ‘…ğ‘…(ğœğœ) is the system-level reward (final answer correctness). This rollout ensures that both simple and complex strategies are evaluated, providing a richer training signal than single-path exploration. System-level rewards alone are sparse and insufficient for credit assignment. To address this, we propose using a strong LLM (e.g., GPT-4, Qwen2-72B) as a process supervisor to evaluate the quality of each intermediate action. For each node (ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) in the rollout tree, the LLM judge provides a score: ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) âˆˆàµ£0,1àµ§ àµ«2àµ¯ reflecting whether the action is reasonable, informative, and consistent with the question intent. The final credit for each action combines system- and process-level signals: ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) = ğ›¼ğ›¼âˆ™ğ‘…ğ‘…(ğœğœ) + ğ›½ğ›½âˆ™ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘¡ğ‘¡, ğ‘ğ‘ğ‘¡ğ‘¡) àµ«3àµ¯ Where ğ›¼ğ›¼, ğ›½ğ›½ are trade-off coefficients. D. Policy Optimization with PPO We train both agents with Proximal Policy Optimization (PPO), a widely used reinforcement learning algorithm. Having obtained the credit rewards that reflect each agentâ€™s contribution, we develop an optimization framework to guide end-to-end training across all agents. The key idea is to use these credit signals for optimizing the collaborative behavior of the entire system. The optimization objective for our multi-agent system can be formulated as maximizing the expected credit rewards: ğ’¥ğ’¥(ğœƒğœƒ) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡ ğ‘–ğ‘–, ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘–ğ‘–àµ¯ ğ‘¡ğ‘¡ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– àµ© àµ«4àµ¯ Since each agentâ€™s action is a sequence of tokens, we decompose this optimization using Proximal Policy Optimization (PPO) [16][17][18] as follows: â„’ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†= à·â„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ ğ‘–ğ‘– (ğœƒğœƒ, ğœ‘ğœ‘) ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– àµ«5àµ¯ Specifically, for each agent ğ‘–ğ‘–, we define: â„’ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ ğ‘–ğ‘– (ğœƒğœƒ) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·min (ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘–(ğœƒğœƒ)ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– , ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘–(ğœƒğœƒ), ğ‘šğ‘š ğ‘¡ğ‘¡ 1 âˆ’ğœ€ğœ€, 1 + ğœ€ğœ€)ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– )àµ© (6) Where ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘–(ğœƒğœƒ) = ğœ‹ğœ‹ğœƒğœƒ(ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– |ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– ) ğœ‹ğœ‹ğœƒğœƒğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ(ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– |ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– ) is the probability ratio, ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– represents the concatenation of current state and the first ğ‘šğ‘šâˆ’1 tokens in the action sequence for agent ğ‘–ğ‘– at time step ğ‘¡ğ‘¡, and ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– denotes its ğ‘šğ‘š-ğ‘¡ğ‘¡â„ token. We compute the advantage estimate using GAE[15]:ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– = âˆ‘ (ğ›¾ğ›¾ğ›¾ğ›¾)ğ‘™ğ‘™ğ›¿ğ›¿ğ‘¡ğ‘¡,ğ‘šğ‘š+ğ‘™ğ‘™ ğ‘–ğ‘– ğ‘€ğ‘€âˆ’ğ‘šğ‘šâˆ’1 ğ‘™ğ‘™=0 , where ğ‘€ğ‘€ is the token length of the action sequence. To estimate state values across the multi-agent system, we employ a centralized state-value function ğ‘‰ğ‘‰ğœ™ğœ™ that takes each agentâ€™s state ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– as input. The value function is optimized to minimize the mean squared error: â„’ğ‘‰ğ‘‰ ğ‘–ğ‘–(ğœ™ğœ™) = ğ”¼ğ”¼ğœğœğœğœğœ‹ğœ‹ğœƒğœƒàµ¥à·à·àµ«ğ‘‰ğ‘‰ğœ™ğœ™àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– àµ¯âˆ’ğºğºà· ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– àµ¯ 2 ğ‘šğ‘š ğ‘¡ğ‘¡ àµ© àµ«7àµ¯ Where ğºğºà· ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– = ğ´ğ´Ì‚ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– + ğ‘‰ğ‘‰ğœ™ğœ™àµ«ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘šğ‘š ğ‘–ğ‘– àµ¯ is the empirical return. The final optimization objective combines the policy and value losses: â„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ ğ‘–ğ‘– (ğœƒğœƒ, ğœ™ğœ™) = â„’ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ ğ‘–ğ‘– (ğœƒğœƒ) + ğ‘ğ‘ğ‘£ğ‘£â„’ğ‘‰ğ‘‰ ğ‘–ğ‘–(ğœ™ğœ™) àµ«8àµ¯ Where ğ‘ğ‘ğ‘£ğ‘£ controls the weight of the value loss. This joint objective enables end-to-end training of both policy and value networks across all agents. III. RESULTS&DISCUSSION In this section, we first present the datasets and implementation details, followed by a comparison of our SIRAG with the latest RAG methods with analysis. A. Datasets To comprehensively evaluate our SIRAG, we experiment on both single-hop datasets including Natural Questions (NQ) and PopQA, as well as multi-hop datasets including 2WikiMultiHopQA (2Wiki), and HotpotQA (HQA). For each dataset, we only use 100 randomly sampled questions instead of the full training set. B. Experimental Details Following Asai et al.[8], we construct our retrieval system using the 2018 Wikipedia dump as the knowledge source and use contriever-msmarco as our dense retriever. We utilize Qwen2.5-7B-Instruct as fixed LLM server, while Qwen2-0.5B is trained as candidate lightweight agent for efficient edge deployment. In the warm-up phase, we collect 4 solutions for each question with Qwen2.5-7B-Instruct. We use a learning rate of 4e-5, with 2 epochs and a batch size of 4. For the RL phase, we set learning rate of 5e-7 for policy model and 5e-6 for value model with a batch size of 2 and maximal depth of 5.",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 4,
      "text_raw": "C. Performance Comparison \nWe report the performance on both single-hop and multi-hop \ndatasets in Table 1. First, our SIRAG consistently outperforms \nvarious baselines across different datasets, achieving superior \naverage performance of 48.23% with lightweight agents of only \n0.5B parameters. This demonstrates the effectiveness of our \nagent-centric alignment approach in bridging the gap between \nthe retriever and the LLM. Second, compared to single-hop \ndatasets, our method yields particularly notable gains in \nchallenging multi-hop reasoning tasks. Specifically, SIRAG \nachieves significant improvements on multi-hop datasets (2Wiki \n+9.3%, HQA +9.2%), while maintaining strong performance on \nsingle-hop tasks (PopQA +1.8%). This significant performance \ngain suggests that, even without training original RAG system, \nour SIRAG effectively enhances the coordination between the \nretriever and LLM, which is particularly crucial for addressing \ncomplex multi-hop tasks. Third, although retriever fine-tuning \nmethod requires fewer tuned parameters, it does not overcome \nthe limitations of standard RAG systems in handling complex \ncognitive and multi-hop reasoning tasks. Both LLM fine-tuning \nand intermediate module methods show promising results, but \nare constrained by either large tuning parameters (7B/72B) or \ninconsistent performance across different reasoning datasets. In \ncontrast, our SIRAG achieves consistent improvements across \nalmost all datasets with only 0.5B additional parameters, \ndemonstrating both efficiency and effectiveness in enhancing \nRAG systems. \n \nTable 1 Main results. Comparison of EM(%) on four datasets. \nmethod \nAgent \ntuned \nparams \n2Wiki \nHQA \nNQ \nPopQA \nAverage \ndirect \n/ \n- \n31.6 \n35.4 \n43.3 \n24.3 \n33.65 \nstandard \n/ \n- \n27.3 \n42.1 \n50.8 \n30.0 \n37.55 \nReranker[19] \n7B \n7B \n26.4 \n37.2 \n47.6 \n20.9 \n33.02 \nQuery-Writer[6] \n1.5B \n1.5B \n32.3 \n36.8 \n53.1 \n30.5 \n38.18 \nselfRAG[8] \n/ \n7B \n35.9 \n45.5 \n50.4 \n31.5 \n40.83 \nSIRAG(ours) \n0.5b \n0.5b \n45.2 \n54.7 \n51.7 \n33.3 \n46.23 \n \nD. Ablation Study \nTo thoroughly evaluate the effectiveness of different \ncomponents in our training process, we conduct comprehensive \nablation studies across four in-domain datasets. Specifically, we \nexamine the following variants: (1) â€œw/o LLM judgeâ€: A variant \nwithout the tree-structured rollout and LLM-as-a-judge credit \nassignment, meaning that we directly optimize each agent using \nthe system-level reward (a single trajectory). (2) â€œw/o RLâ€: The \nperformance in the supervised warm-up phase.  \n \nThe experimental results reveal several key findings. First, \nremoving the tree-structured rollout (and LLM-as-a-judge credit \nassignment) leads to unstable performance during the RL phase, \noccasionally degrading below the supervised warm-up model. \nThis degradation can be attributed to the direct use of system-\nlevel rewards as supervised signals for all agents, which fails to \naccurately assess individual agent contributions and may mask \ndetrimental actions within successful trajectories. In contrast, \nour LLM-as-a-judge credit assignment mechanism enables \nreward allocation in probabilistic expectation through tree-\nstructured exploration, ensuring that each agent receives \nappropriate feedback for its specific actions. Second, comparing \nwith the supervised warm-up model (â€œw/o RLâ€), our SIRAG \nachieves substantial improvements across all datasets. This \nperformance \nboost \ndemonstrates \nthat \nend-to-end \nRL \noptimization effectively aligns the behaviors of multiple agents \ntowards the system-level objectives, going beyond the \nlimitations of supervised learning that only optimizes for local \nagents. \nIV. CONCLUSIONS \nIn this paper, we proposed a process-supervised multi-agent \nframework for retrieval-augmented generation. By introducing \ntwo lightweight agents, the Decision Maker and the Knowledge \nSelector, our approach explicitly coordinates the interaction \nbetween the retriever and the generator. To address the challenge \nof sparse and unstable rewards, we employed an LLM-as-Judge \nto provide process-level supervision for each intermediate action, \nenabling more accurate credit assignment. Combined with a \ntree-structured rollout strategy and end-to-end optimization \nusing PPO, our framework achieves stable training, interpretable \nreasoning trajectories, and improved accuracy on both single-\nhop and multi-hop QA tasks. \nACKNOWLEDGMENT \nThis work was supported by the International Science and \nTechnology Cooperation Project of Guangzhou Economic and \nTechnological Development District (No.2023GH16). \n  \nFigure 2 Ablation Study \n",
      "text_clean": "C. Performance Comparison We report the performance on both single-hop and multi-hop datasets in Table 1. First, our SIRAG consistently outperforms various baselines across different datasets, achieving superior average performance of 48.23% with lightweight agents of only 0.5B parameters. This demonstrates the effectiveness of our agent-centric alignment approach in bridging the gap between the retriever and the LLM. Second, compared to single-hop datasets, our method yields particularly notable gains in challenging multi-hop reasoning tasks. Specifically, SIRAG achieves significant improvements on multi-hop datasets (2Wiki +9.3%, HQA +9.2%), while maintaining strong performance on single-hop tasks (PopQA +1.8%). This significant performance gain suggests that, even without training original RAG system, our SIRAG effectively enhances the coordination between the retriever and LLM, which is particularly crucial for addressing complex multi-hop tasks. Third, although retriever fine-tuning method requires fewer tuned parameters, it does not overcome the limitations of standard RAG systems in handling complex cognitive and multi-hop reasoning tasks. Both LLM fine-tuning and intermediate module methods show promising results, but are constrained by either large tuning parameters (7B/72B) or inconsistent performance across different reasoning datasets. In contrast, our SIRAG achieves consistent improvements across almost all datasets with only 0.5B additional parameters, demonstrating both efficiency and effectiveness in enhancing RAG systems. Table 1 Main results. Comparison of EM(%) on four datasets. method Agent tuned params 2Wiki HQA NQ PopQA Average direct / - 31.6 35.4 43.3 24.3 33.65 standard / - 27.3 42.1 50.8 30.0 37.55 Reranker[19] 7B 7B 26.4 37.2 47.6 20.9 33.02 Query-Writer[6] 1.5B 1.5B 32.3 36.8 53.1 30.5 38.18 selfRAG[8] / 7B 35.9 45.5 50.4 31.5 40.83 SIRAG(ours) 0.5b 0.5b 45.2 54.7 51.7 33.3 46.23 D. Ablation Study To thoroughly evaluate the effectiveness of different components in our training process, we conduct comprehensive ablation studies across four in-domain datasets. Specifically, we examine the following variants: (1) â€œw/o LLM judgeâ€: A variant without the tree-structured rollout and LLM-as-a-judge credit assignment, meaning that we directly optimize each agent using the system-level reward (a single trajectory). (2) â€œw/o RLâ€: The performance in the supervised warm-up phase. The experimental results reveal several key findings. First, removing the tree-structured rollout (and LLM-as-a-judge credit assignment) leads to unstable performance during the RL phase, occasionally degrading below the supervised warm-up model. This degradation can be attributed to the direct use of systemlevel rewards as supervised signals for all agents, which fails to accurately assess individual agent contributions and may mask detrimental actions within successful trajectories. In contrast, our LLM-as-a-judge credit assignment mechanism enables reward allocation in probabilistic expectation through treestructured exploration, ensuring that each agent receives appropriate feedback for its specific actions. Second, comparing with the supervised warm-up model (â€œw/o RLâ€), our SIRAG achieves substantial improvements across all datasets. This performance boost demonstrates that end-to-end RL optimization effectively aligns the behaviors of multiple agents towards the system-level objectives, going beyond the limitations of supervised learning that only optimizes for local agents. IV. CONCLUSIONS In this paper, we proposed a process-supervised multi-agent framework for retrieval-augmented generation. By introducing two lightweight agents, the Decision Maker and the Knowledge Selector, our approach explicitly coordinates the interaction between the retriever and the generator. To address the challenge of sparse and unstable rewards, we employed an LLM-as-Judge to provide process-level supervision for each intermediate action, enabling more accurate credit assignment. Combined with a tree-structured rollout strategy and end-to-end optimization using PPO, our framework achieves stable training, interpretable reasoning trajectories, and improved accuracy on both singlehop and multi-hop QA tasks. ACKNOWLEDGMENT This work was supported by the International Science and Technology Cooperation Project of Guangzhou Economic and Technological Development District (No.2023GH16). Figure 2 Ablation Study",
      "figure_captions": [
        {
          "bbox": [
            120.05999755859375,
            622.0992431640625,
            219.4650115966797,
            635.5961303710938
          ],
          "text": "Figure 2 Ablation Study"
        }
      ],
      "images": [
        {
          "image_id": "p4_cluster_001",
          "page": 4,
          "file_path": "images/page_0004_cluster_001.png",
          "bbox": [
            60.94991683959961,
            494.7504577636719,
            281.84991455078125,
            618.7916259765625
          ],
          "caption": "a bar chart showing the number of people who have been diagnosed with cancer",
          "detailed_caption": "The image shows a bar chart depicting the number of people who have been diagnosed with cancer in the United States. The chart is composed of different colors and text, providing a visual representation of the data.",
          "ocr_text": "SIRAG60w/o LLM judgew/0 RL50-403020-10-02Wiki HOANOPoo OA",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>SIRAG60w/o LLM judgew/0 RL50-403020-10-02WikiHOANOPooOA</s>",
            "parsed": {
              "<OCR>": "SIRAG60w/o LLM judgew/0 RL50-403020-10-02WikiHOANOPooOA"
            }
          },
          "paper_caption": "Figure 2 Ablation Study",
          "paper_caption_bbox": [
            120.05999755859375,
            622.0992431640625,
            219.4650115966797,
            635.5961303710938
          ]
        }
      ]
    },
    {
      "page": 5,
      "text_raw": "REFERENCES \n[1] Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., \nZhang, Y., Chen, Y., et al. Sirenâ€™s song in the ai ocean: a survey on \nhallucination in large language models. ArXiv abs/:2309.01219 \n[2] Sahoo, S. S., Plasek, J. M., Xu, H., Uzuner,  ÌˆO., Cohen, T., Yetisgen, M., \nLiu, H., Meystre, S., and Wang, Y. Large language models for \nbiomedicine: foundations, opportunities, challenges, and best practices. \nJournal of the American Medical Informatics Association, pp. ocae074, \n2024. \n[3] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., \nMadotto, A., and Fung, P. Survey of hallucination in natural language \ngeneration. ACM Computing Surveys, 55(12):1â€“38, 2023. \n[4] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., \nKu Ìˆttler, H., Lewis, M., Yih, W.-t., Rockta Ìˆschel, T., et al. Retrieval-\naugmented generation for knowledgeintensive nlp tasks. Advances in \nNeural Information Processing Systems, 33:9459â€“9474, 2020. \n[5] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and \nWang, H. Retrieval-augmented generation for large language models: A \nsurvey. arXiv preprint arXiv:2312.10997, 2023. \n[6] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan. Query rewriting in \nretrieval-augmented large language models. In H. Bouamor, J. Pino, and \nK. Bali, editors, Proceedings of the 2023 Conference on Empirical \nMethods in Natural Language Processing, EMNLP 2023, Singapore, \nDecember 6-10, 2023, pages 5303â€“5315. Association for Computational \nLinguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.322. URL \nhttps://doi.org/10.18653/v1/2023. emnlp-main.322. \n[7] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. \nZettlemoyer, and W. Yih. REPLUG: retrieval-augmented black-box \nlanguage models. In K. Duh, H. GÃ³mez-Adorno, and S. Bethard, editors, \nProceedings of the 2024 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies (Volume 1: Long Papers), NAACL 2024, Mexico City, \nMexico, June 16-21, 2024, pages 8371â€“8384. \n[8] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to \nretrieve, generate, and critique through self-reflection. In The Twelfth \nInternational Conference on Learning Representations, ICLR 2024, \nVienna, Austria, May 7-11, 2024. OpenReview.net, 2024. \n[9] Z. Wei, W. Chen, and Y. Meng. Instructrag: Instructing retrieval-\naugmented generation with explicit denoising. CoRR, abs/2406.13629, \n2024. doi: 10.48550/ARXIV.2406.13629. \n[10] T. Yu, S. Zhang, and Y. Feng. Auto-rag: Autonomous retrieval-\naugmented generation for large language models. CoRR, abs/2411.19443, \n2024. doi: 10.48550/ARXIV.2411.19443. \n[11] Y. Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. \nCatanzaro. Rankrag: Unifying context ranking with retrieval-augmented \ngeneration \nin \nllms. \nCoRR, \nabs/2407.02485, \n2024. \ndoi: \n10.48550/ARXIV.2407.02485. \n[12] E. Schmidt. How google works. Hachette UK, 2014. \n[13] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. \nJain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. \nKrueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: \nBrowser-assisted question-answering with human feedback. CoRR, \nabs/2112.09332, 2021. \n[14] Y. Zhou, Y. Liu, X. Li, J. Jin, H. Qian, Z. Liu, C. Li, Z. Dou, T. Ho, and \nP. S. Yu. Trustworthiness in retrieval-augmented generation systems: A \nsurvey. CoRR, abs/2409.10102, 2024. doi: 10.48550/ARXIV.2409.10102. \n[15] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-\ndimensional continuous control using generalized advantage estimation. \nIn Y. Bengio and Y. LeCun, editors, 4th International Conference on \nLearning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, \n2016, Conference Track Proceedings, 2016. \n[16] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. \nProximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. \n[17] L. Yuan, Z. Zhang, L. Li, C. Guan, and Y. Yu. A survey of progress on \ncooperative multi-agent reinforcement learning in open environment. \nCoRR, abs/2312.01058, 2023. doi: 10.48550/ ARXIV.2312.01058. \n[18] C. Zhu, M. Dastani, and S. Wang. A survey of multi-agent deep \nreinforcement learning with communication. Auton. Agents Multi Agent \nSyst., 38(1):4, 2024. doi: 10.1007/S10458-023-09633-6. \n[19] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang. Towards \ngeneral text embeddings with multi-stage contrastive learning. CoRR, \nabs/2308.03281, 2023. doi: 10.48550/ARXIV. \n[20] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to \nretrieve, generate, and critique through self-reflection. In The Twelfth \nInternational Conference on Learning Representations, ICLR 2024, \nVienna, Austria, May 7-11, 2024. OpenReview.net, 2024. \n \n",
      "text_clean": "REFERENCES [1] Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., et al. Sirenâ€™s song in the ai ocean: a survey on hallucination in large language models. ArXiv abs/:2309.01219 [2] Sahoo, S. S., Plasek, J. M., Xu, H., Uzuner, ÌˆO., Cohen, T., Yetisgen, M., Liu, H., Meystre, S., and Wang, Y. Large language models for biomedicine: foundations, opportunities, challenges, and best practices. Journal of the American Medical Informatics Association, pp. ocae074, 2024. [3] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1â€“38, 2023. [4] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Ku Ìˆttler, H., Lewis, M., Yih, W.-t., Rockta Ìˆschel, T., et al. Retrievalaugmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459â€“9474, 2020. [5] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023. [6] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan. Query rewriting in retrieval-augmented large language models. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 5303â€“5315. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.322. URL https://doi.org/10.18653/v1/2023. emnlp-main.322. [7] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W. Yih. REPLUG: retrieval-augmented black-box language models. In K. Duh, H. GÃ³mez-Adorno, and S. Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 8371â€“8384. [8] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [9] Z. Wei, W. Chen, and Y. Meng. Instructrag: Instructing retrievalaugmented generation with explicit denoising. CoRR, abs/2406.13629, 2024. doi: 10.48550/ARXIV.2406.13629. [10] T. Yu, S. Zhang, and Y. Feng. Auto-rag: Autonomous retrievalaugmented generation for large language models. CoRR, abs/2411.19443, 2024. doi: 10.48550/ARXIV.2411.19443. [11] Y. Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. Catanzaro. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. CoRR, abs/2407.02485, 2024. doi: 10.48550/ARXIV.2407.02485. [12] E. Schmidt. How google works. Hachette UK, 2014. [13] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021. [14] Y. Zhou, Y. Liu, X. Li, J. Jin, H. Qian, Z. Liu, C. Li, Z. Dou, T. Ho, and P. S. Yu. Trustworthiness in retrieval-augmented generation systems: A survey. CoRR, abs/2409.10102, 2024. doi: 10.48550/ARXIV.2409.10102. [15] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. Highdimensional continuous control using generalized advantage estimation. In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. [16] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. [17] L. Yuan, Z. Zhang, L. Li, C. Guan, and Y. Yu. A survey of progress on cooperative multi-agent reinforcement learning in open environment. CoRR, abs/2312.01058, 2023. doi: 10.48550/ ARXIV.2312.01058. [18] C. Zhu, M. Dastani, and S. Wang. A survey of multi-agent deep reinforcement learning with communication. Auton. Agents Multi Agent Syst., 38(1):4, 2024. doi: 10.1007/S10458-023-09633-6. [19] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang. Towards general text embeddings with multi-stage contrastive learning. CoRR, abs/2308.03281, 2023. doi: 10.48550/ARXIV. [20] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.",
      "figure_captions": [],
      "images": []
    }
  ]
}