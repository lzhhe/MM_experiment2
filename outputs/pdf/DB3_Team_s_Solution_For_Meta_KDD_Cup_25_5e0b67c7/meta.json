{
  "pdf_path_rel": "../../../papers/NLP/DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25.pdf",
  "pdf_path_abs": "E:\\Desktop\\M502083B Â§öÊ®°ÊÄÅÊú∫Âô®Â≠¶‰π†\\MM_experiment2\\papers\\NLP\\DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25.pdf",
  "num_pages": 7,
  "full_text_clean": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† 1 Key Laboratory of High Confidence Software Technologies, CS, Peking University, China wfl00014@pku.edu.cn, {chenjiazun, zhanyirui, suifengzhao}@stu.pku.edu.cn, gaojun@pku.edu.cn, China 2 Theory Lab, Central Research Institute, 2012 Labs, Huawei Technologies Co., Ltd, Shenzhen, China {jiangweipeng, zhang.chaorui, harvey.hanwei, baibo8}@huawei.com, China Abstract This paper presents the db3 team‚Äôs winning solution for the Meta CRAG-MM Challenge 2025 at KDD Cup‚Äô25. Addressing the challenge‚Äôs unique multi-modal, multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive framework that integrates tailored retrieval pipelines for different tasks with a unified LLM-tuning approach for hallucination control. Our solution features (1) domain-specific retrieval pipelines handling imageindexed knowledge graphs, web sources, and multi-turn conversations; and (2) advanced refusal training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd place in Task 2, and 1st place in Task 3, securing the grand prize for excellence in ego-centric queries through superior handling of first-person perspective challenges. CCS Concepts ‚Ä¢ Computing methodologies ‚ÜíNatural language generation. Keywords Visual Language Models, Multimodal RAG 1 Introduction Recent advancements in artificial intelligence have highlighted the importance of multi-modal reasoning, especially in scenarios where systems must interpret and respond to both visual and textual information. The Meta CRAG-MM Challenge 2025, hosted on AIcrowd, aims to accelerate progress in this area by introducing a novel benchmark, called CRAG-MM, that encompasses both single-turn and multi-turn visual question answering tasks. Unlike traditional VQA datasets, CRAG-MM is designed to evaluate factual question answering in multi-modal contexts, integrating images with complex, multi-turn conversational flows. Notably, ego-centric images are a major characteristic of this contest. In the context of the Meta CRAG-MM Challenge 2025, ego-centric images refer to images captured from a first-person perspective, typically using wearable cameras mounted on glasses. Unlike traditional third-person or static camera views, ego-centric images provide a direct visual record of the user‚Äôs personal experience and interactions within their environment. This perspective enables the capture of rich contextual information, including handobject interactions, dynamic activities, and real-time environmental changes. However, ego-centric images also present unique challenges, such as frequent occlusions, rapid viewpoint shifts, and *These authors contributed equally to this research. ‚Ä†Corresponding Authors . complex backgrounds. Addressing these challenges is crucial for advancing machine perception and understanding of daily human activities from a truly immersive viewpoint, making ego-centric image analysis a key focus of this competition. The Meta CRAG-MM Challenge 2025 is composed of the following three tasks: (1) Task 1: Participants are required to answer single-turn questions using both image-indexed knowledge graphs as context. (2) Task 2: This task also involves single-turn questions but additionally introduces a web search knowledge source. (3) Task 3: A multi-turn retrieval-augmented generation (RAG) task, where systems must engage in dialogue-like interactions, retrieving and integrating information from multiple sources to generate coherent responses. The author‚Äôs team, db3, participates in the contest and achieves second place, second place, and first place in the three tasks, respectively, resulting in winning the grand prize in ego image total score. This paper describes the author‚Äôs solution to the three tasks. The two main challenges of this contest are 1. How to deal with the retrieval information, both the multimodal part and the multiturn conversation part. 2. How to control hallucinations. As the metrics of the contest involve punishing answering incorrect answers, we have to train the model to output \"I don‚Äôt know\" on queries hard for them to answer. Therefore, we describe our solutions in two parts: the retrieval part and the hallucination control part. In this paper, in addition to presenting our final submitted solution, we also describe the various approaches and experiments we attempted during the development process. Our code is available on GitLab 1. In the remainder of the paper, we first introduce some basic constructions we make for this contest . We discuss the retrieval part in Sec. 3, and we discuss the hallucination control part in Sec. 4. We discuss the checkpoint selection and ensemble tricks we use in this contest in Sec. 5. We conclude our work and look into future works in Sec. 6. 2 Basic Constructions. We construct some basic modules for this contest that are useful for all the following procedures. LLM judge. Since the online judge and prompt for this contest are not available, we construct a local LLM judge for this contest. We observe that using different prompts and LLM judges can have a huge difference in the results, and some of our training process relies on the LLM judge‚Äôs results. We enumerate some combinations 1https://gitlab.aicrowd.com/jiazunchen/db3-team-s-solution-for-meta-kdd-cup-25 arXiv:2509.09681v1 [cs.IR] 12 Aug 2025\n\n1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† Image Domain adapter Context Math Math adapter Answer Plants Entity Chunks ‚Ä¶ Web Chunks Rerank er Rewrite Query Resize Tool Process Entity Process Web Process Adapters M E R G E Answer_0 Answer_1 Answer_2 Answer Figure 1: Pipeline for Different Tasks: A domain adapter is first applied to classify the query domain for different pipelines, e.g., a math tool for math problems, a retrieval through entity chunks module for plants, and a rewriting and retrieving from web chunks module for all other domains. After a reranker module for all retrieved text chunks, we obtain multiple answers using trained hallucination control adapters, and these answers are ensembled to obtain a final output answer. of the prompts and LLM APIs we use locally and reconcile the local scores with the online scores we get. The following prompt applied with GPT-4o-mini achieves the relatively most similar results with online evaluation results. The prompt used is as follows: 1 You are an expert evaluator for question answering systems. 2 Your task is to determine if a prediction correctly answers a ‚Ü©‚Üíquestion based on the ground truth. 3 4 Rules: 5 1. The prediction is correct if it captures all the key ‚Ü©‚Üíinformation from the ground truth. 6 2. The prediction is correct even if phrased differently as ‚Ü©‚Üílong as the meaning is the same. 7 3. The prediction is incorrect if it contains incorrect ‚Ü©‚Üíinformation or is missing essential details. 8 9 10 Question: {query} 11 Ground truth: {ground_truth} 12 Prediction: {prediction} 13 14 Output only 'true' or 'false' to indicate if the prediction is ‚Ü©‚Üícorrect. Noting that this prompt is not the best one that follows human evaluation. Through our observation, this prompt and online evaluation is much stricter than human evaluation, resulting in many of the results that should be correct being assigned as wrong during online evaluation. However, we have to choose this prompt to pursue a better score during the online evaluation. Train/Validation Split. To perform checkpoint selection and some other techniques, we have to split the training and validation sets. As the query types and domains vary, we also split the train/validation set proportionally to make sure each question type and domain appears equally in the train and validation splits. Domain Prediction. We rely on the domain splits to apply different pipelines for different domains. Since some of the domains have blurred semantics, we combine some of the originally split domains and have these final domains for classification: vehicle, plant, local, math, science, food, animal, and other. We re-annotate the queries using the new classes, and we tune Llama 3.2-VL for domain classification. The performance of this module is acceptable, as the classification accuracy of this model is 91.17% on the Animal Food Local Math Other Plants Science Vehicle Animal Food Local Math Other Plants Science Vehicle 96% 1% 0% 0% 3% 0% 0% 0% 1% 90% 2% 0% 7% 1% 0% 0% 0% 0% 93% 0% 7% 0% 0% 0% 0% 0% 0% 73% 18% 0% 9% 0% 1% 3% 4% 0% 88% 0% 1% 3% 1% 2% 0% 0% 1% 97% 0% 0% 2% 0% 0% 0% 27% 0% 70% 0% 0% 0% 1% 0% 2% 0% 0% 97% Overall Accuracy: 91.17% Figure 2: Confusion Matrix of Domain Prediction validation set, and many of the wrongly assigned queries actually can fit in many classes. The confusion matrix is presented in Fig. 2. 3 The Retrieval Component of the Solution In this section, we will propose the retrieval component of our solution. In this contest, the query is always associated with a query image. Therefore, we need to deal with both the query image and the text query. Specifically, we will present pipelines involving retrieving from the image-indexed knowledge graph in Task #1, the text-indexed web source in Task #2, and the multi-turn context in Task #3. Additionally, two techniques that are useful for all three tasks are presented in this section: the OCR extraction and the tool solution for math problems. 3.1 Retrieval Pipeline for Image-indexed Knowledge Graph. In Task 1, an image-indexed knowledge graph is provided as the information source. In Task 1, an image-indexed knowledge graph is provided as the information source. Specifically, an image is linked with a CLIP [4] index and is associated with a structured segment of text, which serves as a description of the entity represented by the image. The information useful for answering the question is mostly based on the information in the structured text; therefore,\n\nDB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 the task involves identifying the correct entity the query is about in the knowledge graph. Retrieving and Reranking through Image. The most direct solution is to retrieve and rerank through the query image and the index images. The most related images are first retrieved through the CLIP index, and the related text information is used as the retrieved information. The key challenge here is the quality of the retrieval results. Through experiment, we observe that directly applying the top 1/3/5 results of this source through the image index harms the final QA accuracy. We applied the following techniques to improve the retrieval quality. Grounding In the Query Image. Ego-centric images consist of a large portion of the query images. The distribution of ego-centric images differs a lot from the index images. The ego-centric images are mostly taken in everyday life and contain much irrelevant background, while the index image is often a Wikipedia-style image concentrating on the entity. Therefore, the CLIP embeddings of the query image and correct index image are very likely to differ a lot. For example, if the query is about a car parked in front of a building, the CLIP embedding of the query image will be more similar to a building‚Äôs CLIP embedding. We have to clip the car part of the query image to retrieve the correct car entity in the knowledge graph. Therefore, we introduce grounding in the query image to resolve this problem. We leverage the open-source Grounding-DINO [3] model to deal with this task. The Grounding-DINO model takes in a short text and an image as input. The short text serves as a prompt describing the object or region of interest (for example, ‚Äúthe plant in the middle‚Äù or ‚Äúa black car‚Äù). The model then processes both the image and the text to identify and localize the region in the image that best matches the given description. Through our annotation, we find that with the appropriate text input, the Grounding-DINO model can localize the correct entity in most cases, and through statistics, the correct entities appear more frequently in the retrieved top entities. However, the text input is quite hard to obtain. The ideal input is related to the query and the query image, and it‚Äôs hard to prompt the VLM to output the ideal describing text having the same quality as the human-annotated ones. In practice, we prompt VLMs to generate the describing text. The Dino prompt we used is as follows: 1 Image: {} 2 Given this image, a query, your task is to simply describe the ‚Ü©‚Üíobject in the image. 3 Query: {} 4 Output only simple object names in phrases, do not output a ‚Ü©‚Üísentence. 5 Do not answer the query, just output the object name appearing ‚Ü©‚Üíin the image, not the answer or answer entity. Alternatively, we can use a certain phrase for each domain, e.g., Car, Plant, Building,..., as the DINO phrase to minimize the LLM inference cost. Reranking through Image. In the contest retrieval scenario, retrieving through CLIP embeddings is highly inaccurate. Following human practice, checking whether two images refer to the same entity is essential. Therefore, we propose reranking by comparing the query and index images. We use the following prompt to let VLMs judge whether the two images belong to the same entity. We use the image rerank prompt to do this task: What is the horsepower of this truck? A white truck Dino prompt Grounding DINO Image API Image rerank prompt No Llama Llama Figure 3: Grounding in the Query Image + Rerank through Image 1 Image: {query image},{index image} 2 Given two images, the first one is a query image, the second ‚Ü©‚Üíone is an image about an entity, a query about the first ‚Ü©‚Üíimage, descriptions about the second image, your task is to ‚Ü©‚Üí determine whether the query about the first image is about ‚Ü©‚Üí the entity in the second image. 3 Query: {} 4 Description: {} 5 If the entity in the second image appears in the first image, ‚Ü©‚Üíoutput Yes, otherwise, output No. We only preserve the related items (with judge output yes) in the top index. Through experiment, we observe that using this prompt with a powerful VLM, e.g., GPT-4o, can achieve considerable boosts in performance. Since only a relatively small VLM can be used in the contest, we aim to distill Llama 3.2-VL to achieve this ability. Specifically, we sample the top 5 candidates for each query using the grounding and retrieving through image pipeline. We verify each candidate by using them to retrieve content to answer the query and judging through a strong VLM, e.g., GPT-4o. A candidate is viewed as a related item if the retrieved item helps the base VLM answer the query correctly, and the strong VLM judges the entity in the index image to be the same as the entity in the grounded query image. Such a candidate is labeled as relevant, and the base VLM is trained to classify the relevance. In our experiment, we observe that reranking with a powerful VLM can largely boost the final QA results (by 10%), and using a distilled Llama 3.2-VL can boost the QA performance by 2%. Since the boost is tiny, and we try this technique in the early stage of the contest, we don‚Äôt have time to add it to the final pipeline. Retrieving and Reranking through Text. Since we observe severe difficulty in utilizing the top-k candidates through CLIP image embedding, we propose retrieving and reranking through text. In many cases, the VLM itself can identify the correct entity name the query is about. We can retrieve the relevant attributes about this entity from the image-indexed knowledge graph. We made two attempts at retrieving through text: the entity name approach and the merged text query approach. The entity name approach. The entity name approach means first extracting the desired entity name from the query and query image, and then identifying the same entity from the knowledge graph. For example, the query asks about when this plant blooms; the entity name will be the plant‚Äôs botanical name, which is expected to exist in the knowledge graph. In the first step, we prompt the VLM to extract the query entity‚Äôs name. We use the entity prompt to conduct this task. 1 Given an image and a query about it, your task is to extract ‚Ü©‚Üíthe entity's name the query is about.\n\n1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† Ram 2500 Entity prompt Image API Llama Entity match Ram 2500 {Kia Telluride,‚Ä¶} {Kia Telluride,‚Ä¶} {GMC Terrain,‚Ä¶} Candidate entity information What is the horsepower of this truck? Figure 4: Retrieving and Reranking through Text. 2 If the entity is about a plant, you should output the full ‚Ü©‚Üíbotanical name. 3 .... 4 If the enityt is about a vehicle, you should output the vehicle ‚Ü©‚Üí's brand and model. 5 Query: {} 6 You should output the entity's name directly. The prompt consists of detailed entity name instructions that match the entities‚Äô names in the knowledge graph. Similarly, prompting strong VLMs, e.g., GPT-4o, can extract more correct entity names compared to prompting Llama 3.2-VL. On one hand, GPT-4o possesses more internal knowledge. On the other hand, GPT-4o can follow the instructions better. We also tune Llama 3.2-VL using the ground truth label name to learn the format and follow the instructions better. (The ground truth label is obtained by similar prompts with the ground truth answer. As the ground truth label usually appears in the ground truth answer, most of the GPT-4o extraction results are identical to the ground truth entity name.) In the second step, we use the retrieving-through-image method to extract a large number of candidates (e.g., 1000), and the generated entity name is compared with the entity names of the candidates. The entity names are tokenized and stemmed, and only if the processed token set is identical or contains the other are the two entity names considered related. Through experiment, we find that using GPT-4o, 40% of the entity name can be extracted correctly. The ratio is 20% for Llama 3.2-VL. The potential of the QA system can be boosted by 10% using the GPT-4o result, and the result is 5% for the Llama 3.2-VL results. Unfortunately, we didn‚Äôt have enough time to train a model that performed well in all domains. As a result, we could only apply this solution in the field of plants. Notably, even powerful VLMs like GPT-4o can only identify 40% of the entity‚Äôs desired names. This indicates that for the hard cases like vehicles, plants, and food, state-of-the-art VLMs still can‚Äôt recall their names, as different models of cars or different plants in the same class look quite similar. During the contest, we try to search for open-source models to identify a car‚Äôs brand and model or plants‚Äô botanical names. However, it turns out that there are no such open-source models, and a system that can deal with these needs requires a highly sophisticated database building and model training. Query using merged text query. A merged text query is a pure text query that contains all the query information in the original text query and the query image. We will propose the full merged text query rewrite process in Sec. 3.2 in detail. Suppose we have the merged query here. We follow the same steps of retrieving a large number of candidates (e.g., 1000), and we convert the structured What is the horsepower of this truck? What is the horsepower of Ram 2500? Rewrite prompt Web API Llama The New Ram 2500 And Its Incredible The 2021 Ram 2500 has a 6.4-liter V-8 engine that generates 410 horsepower‚Ä¶ Candidate entity information What is the horsepower of Ram 2500? Web retrieval chunks What is the horsepower of Ram 2500? TOP K chunks Reranker Figure 5: Retrieval Pipeline for Text-indexed Web Source. text information of the candidates to a text embedding retrieval base. Specifically, the attribute-value pair in the entity‚Äôs structured text is converted into separate text outputs. For example, the Volkswagen Beetle entity has an attribute, end of production year: 2019. This attribute value pair will be converted to the end of the production year of Volkswagen Beetles, which is 2019. These sentences, each representing one attribute of an entity, are further selected using rerank models, e.g., BGE-reranker-v2-m3. Through experiment, we find that the potential of the QA system can be boosted by 20% using this technique. We successfully integrated this technique into our hallucination control module, and the overall performance is slightly boosted for the plant category. 3.2 Retrieval Pipeline for Text-indexed Web Source. In Task 2, a text-indexed web page source is provided as an information source. Specifically, the chunked web page content is linked with a text embedding index. The difficulty in this task lies in how to rewrite the query into a merged text query that has the same modality as the web page text, which can be retrieved through the text embedding. For example, for the query, When does this car stop production?, and the query image containing a Volkswagen Beetle, we have to rewrite the original query to When does the Volkswagen Beetle stop production? We have to substitute the pronouns in the original query with information from the query image and rewrite a merged query that can be answered individually without the query image. SFT tuning for Merge Query Rewrite. We can prompt VLMs to generate a merged query. We use the rewrite prompt to rewrite a merged query: 1 ###Task You are an expert at converting visual questions into ‚Ü©‚Üíeffective search queries. 2 Your goal is to create a comprehensive search query that will ‚Ü©‚Üíhelp find the most relevant information. 3 For each image-based question, you must create a search query ‚Ü©‚Üíthat combines: 4 1. Key visual elements from the image (objects, text, logos, ‚Ü©‚Üíscenes, actions, etc.) 5 2. The core question being asked 6 3. Potential answer terms or relevant context 7 For example: 8 - If asking about a logo: include company name, industry, and ‚Ü©‚Üívisual description 9 - If asking about an object: include its appearance, category, ‚Ü©‚Üíand possible brands/models 10 - If asking about an event/scene: include location hints, ‚Ü©‚Üíactivities, and time period clues 11 ''' 12 Image:{}\n\nDB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 13 Query: {} We first prompt the powerful GPT-4o to generate the merged query rewrite. As we have mentioned before, even GPT-4o can only identify half of the entities in this contest. Moreover, the instructions we make in the prompt are still not 100% followed by GPT-4o. This is worse while using Llama 3.2-VL for rewriting. Here we list the potentially correct answering case in Task 2 using the rewrite system. As we can see, the GPT-4o rewrite system can boost answer potential by Àú13%, while the Llama 3.2-VL rewrite can only boost it by 7%. Table 1: Comparison of Different Rewriting Methods Method Score (%) Ori ‚àº33 Llama Rewrite ‚àº40 GPT-4o Rewrite ‚àº46 Llama-distill Rewrite ‚àº44 GPT-4o Cheat Rewrite ‚àº60 Llama-distill-cheat Rewrite ‚àº44 To solve the difficulty of entity extraction, we introduce a cheated version of rewrite. In the prompt we listed above, we additionally add the ground truth answer. As the ground truth entity mostly appears in the ground truth answer, providing the ground truth answer can guide the VLM to include the ground truth entity in the rewrite query. As we can see, the cheated version achieves 60% of the potential correct score. We tune the Llama 3.2-VL base model for better rewrite performance. We select both the cheated and non-cheated rewrite queries as the training sample for Llama 3.2-VL. The results are also presented in Table 1. As we can see, regardless of using the cheated or non-cheated version, the boost of potential performance is Àú4%. The performance is close to the GPT-4o rewrite performance ( 3% down); therefore, the result is rather satisfying. We choose one of the checkpoints in the Llama-distill checkpoints as the final rewrite module we use in the submitted version. RL Tuning for Merge Query Rewrite. During the contest, we also try using RL to tune this rewrite module. We try two different approaches, the DPO approach and the RL (GRPO) approach. DPO approach. [5] We follow the normal procedure of DPO training. We construct the training data as follows. Following the best SFT checkpoint we obtain in the last subsection, we sample 5 rewrite merge queries using high temperature. The better pairs in DPO are the rewrite queries that can produce the context, making the QA result correct, and the worse pairs in DPO are the rewrite queries that produce the context, making the QA result incorrect. RL (GRPO) approach. [6] We use the GRPO algorithm in RL training for query rewrite. Similarly, the merge query is rewarded if the context helps answer correctly. Conversely, the merge query is punished if the context makes the answer wrong. Outcome. As we have mentioned, due to the lack of sufficient internal knowledge, the space for further improvement through RL is limited. The gap between tuned Llama and GPT-4o is only 3%. In our experiment, we fail to obtain a better rewrite checkpoint Q1: what is the name of the radio station playing on the radio? Q2: what kind of a station is that? Q3: where do they broadcast from? A1: the station playing on the car radio is nj 101.5. A2: nj 101.5 is an fm station that plays talk radio on weekdays and classic hits on weekends. A3: nj 101.5 broadcasts from trenton. Context for Q2 Context for Q3 Figure 6: One-step Context for Multi-Round QA. than the original SFT-tuned checkpoint. Therefore, our submitted version is based on SFT-tuning. Retrieval through Text. For simplicity, we directly use the preprocessed web content instead of the original HTML content. After obtaining the rewritten merged query, we retrieve the text chunks using the bge-large-en-v1.5 index. Conventionally, we further use BGE-reranker-v2-m3 to sort the candidate chunks more accurately [1]. 3.3 Retrieval Pipeline for Multi-turn Conversation. In Task 3, the information source is the same as the source in Task 2. The difference is the introduction of a multi-turn conversation. In this subsection, we present our retrieval pipeline for multi-turn conversation. One-step Context. Though some queries may require longer context, we observe in our experiment that, provided with the groundtruth QA, the QA performance of using one-step context is almost the same as the result of using full context. Therefore, for the simplicity of sampling and training, we only use the last-step context in the contest. Merge Query Rewrite with Context. Since many of the queries require context QAs to specify the entity the query is about, we add the query QAs into the merge query rewrite prompt. The prompt we use is as follows: 1 ###Task You are an expert at converting visual questions into ‚Ü©‚Üíeffective search queries. 2 The current query is a part of multi-turn conversation. You ‚Ü©‚Üíshould use the history conversation to make sure what the ‚Ü©‚Üícurrent query is about. 3 Your goal is to create a comprehensive search query that will ‚Ü©‚Üíhelp find the most relevant information for the currecnt ‚Ü©‚Üíquery. 4 For each image-based question, you must create a search query ‚Ü©‚Üíthat combines: 5 1. Key visual elements from the image (objects, text, logos, ‚Ü©‚Üíscenes, actions, etc.) 6 2. The core current question being asked 7 3. Potential answer terms or relevant context 8 For example: 9 - If asking about a logo: include company name, industry, and ‚Ü©‚Üívisual description 10 - If asking about an object: include its appearance, category, ‚Ü©‚Üíand possible brands/models 11 - If asking about an event/scene: include location hints, ‚Ü©‚Üíactivities, and time period clues 12 Query: {} 13 Context: {}\n\n1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† What‚Äôs the answer? Solve_math(11*7) Llama Math tool prompt Calculate expression 11 * 7 = 77 Solve_math(11*7) Response prompt 11 * 7 = 77 Llama 11 times 7 is 77 Figure 7: Solve Math Problems Using Tools. The tuning process is the same as the process without a multiturn conversation. Sampling Strategy. To achieve the best final score, a hallucination control component has to be employed. However, in merge query rewrite training, we need to deploy the control component, as sufficient QA information has to be included in context. 3.4 OCR from the Query Image. Through observation, we find that many of the queries are directly about the text in the image. Therefore, we suspect employing a separate OCR module can boost the performance for queries of this kind. Through experiment, we find that many more queries can be answered correctly using OCR results from GPT-4o. However, we find that using VLM models to extract text has a severe drawback. A large number of queries involve a large amount of text from a book. Conducting OCR using VLMs requires generating many tokens, making this approach unavailable in this contest. We also try using OCR tools like PaddleOCR [2]. However, we find that the extraction results are not as satisfying as using VLMs, particularly in the ego-centric images. Considering all the difficulties, we do not include an OCR component in our pipeline. However, we believe this component can be useful, as the base VLM normally can‚Äôt focus on too much text in the image. 3.5 Solve Math Problems Using Tools. Math problems are always tricky for LLMs/VLMs, and using tools is a standard solution for this. Therefore, we employ using tools to solve the math problems. Through observation of the contest data, we conclude there are three types of major problems in math: calculation and simplification of numbers and variables, base conversion, and chemical formula balancing. We construct tools for these three types of queries, and we provide APIs for these tools. We prompt the VLM to follow the provided tool API. To solve the instruction-following problem, we construct Àú50 tool examples and tune the base VLM. The final version of our math module can solve most of the math problems correctly in the training data of the contest. 4 Hallucination Control Component of the Solution LLMs/VLMs inevitably encounter queries that fall outside their reliable knowledge scope. During the contest, we are awarded 1 point for answering correctly and penalized 1 point for answering incorrectly; therefore, we have to control hallucinations to achieve higher scores. The hallucination control component therefore, has two equally important goals: (1) Maximize the proportion of correct answers. (2) Minimize the proportion of wrong answers by refusing when necessary. 4.1 Answerability Estimation Whether a query is answerable by the current retrieval and answering system can be determined by the correctness of its generated answers. We denote the queries that are hard for the current system to produce a correct answer as unanswerable. 4.2 Refusal Training Pipeline Supervised Fine-Tuning (SFT) ‚Ä¢ Unanswerable queries: label ‚ÜíI don‚Äôt know. ‚Ä¢ Answerable queries: ground-truth reference answers Direct Preference Optimisation (DPO) We build pairwise preferences (better, worse): ‚Ä¢ Answerable queries: better = correct answer; worse = I don‚Äôt know. ‚Ä¢ Unanswerable queries: better = I don‚Äôt know; worse = hallucinated answer. Reinforcement Learning (GRPO) We adopt GRPO because it eliminates the value-function critic, reducing instability, yet the reward definition applies equally to PPO: ùëü= Ô£±Ô£¥Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£¥Ô£≥ +ùëò, correct answer; 0, I don‚Äôt know; ‚àí1, incorrect answer. With ùëò=1, the expected return is identical to the Refusal Score. We obtain different checkpoints using different queries for further selection and ensemble. 5 Checkpoint Ensemble Trick Since the hallucination control process is tricky and highly unstable, selecting and combining the best checkpoints became quite important. In this section, we conclude the tricks we employ in this contest. Checkpoint Candidate Pool. We collect the checkpoint results in many training trials under different settings to form a checkpoint candidate pool. The checkpoints with poorer results are neglected in the pool. All the results on the validation set are recorded so that further processes in the candidate pool do not require re-evaluation. Checkpoint Ensemble and Selection. There are many ways of ensembling checkpoints. We list some of them: Ensemble according to Domain. Since the difficulty of different domains varies a lot, we can control the QA checkpoints or even block answering according to the domain information. The strategy here can be selecting the best checkpoints on each domain, and if no checkpoint can achieve positive scores on this domain, we block the answers on this domain directly. Ensemble according to Equivalence. We conduct equivalence clustering on all the answers of different checkpoints, and we select the answers supported by the most checkpoints. We can enumerate\n\nDB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 the checkpoints in the candidate pool to get the optimal checkpoint subset. Mixed Ensemble of Domain and Equivalence. We can apply a mixed strategy, which means enumerating checkpoint combinations on each domain and selecting different optimal checkpoint combinations for each domain. Pros and Cons. Pros: The ensemble process helps us boost plenty of performance on the final score. Cons: Since many checkpoints are involved, it requires multiple rounds of inference during the test, making it hard to control the time limit. Unfortunately, many strategies that work well locally fail to work online, even if we manage to control the time limit carefully. This potentially harms the final score of the solution. Another major drawback is that conducting such a large-scale selection in the checkpoints creates a large gap between the local evaluation results and the online evaluation results. Our enumeration results show huge improvements locally, while it turns out to be heavily overfitting. Our final version also differs from the complex combinations for each domain, because the less selection is made, the less overfitting is observed. 6 Conclusion This paper presented db3‚Äôs comprehensive solution for the Meta CRAG-MM Challenge 2025, which secured top rankings across all tasks and won the grand prize for ego-centric queries. Our key innovations include: 1. Domain-Adaptive Retrieval: We developed specialized pipelines for different data modalities, most importantly, merged query rewriting for text-based retrieval, significantly improving context relevance. 2. Hallucination Control: Through multi-stage training (SFT, DPO, RL) with refusal optimization, we created models that reliably output \"I don‚Äôt know\" for unanswerable queries while maximizing correct responses. Despite our success, limitations remain in fine-grained entity recognition and OCR integration for text-heavy images. Future work should explore dedicated recognition models and optimized OCR-VLM pipelines. Our solution demonstrates that combining task-specific retrieval with rigorous hallucination control is essential for reliable multi-modal QA systems, particularly for challenging ego-centric scenarios. 7 Acknowledgement We extend our sincere gratitude to the experts from Huawei‚Äôs Theory Lab at the Central Research Institute, 2012 Labs, for their invaluable technical discussions and suggestions throughout this project. This project is funded by NSFC (No. 62272008). References [1] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2402.03216 [cs.CL] [2] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. 2025. PaddleOCR 3.0 Technical Report. arXiv:2507.05595 [cs.CV] https://arxiv.org/abs/2507.05595 [3] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. 2024. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision. Springer, 38‚Äì55. [4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 8748‚Äì8763. [5] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. Advances in neural information processing systems 36 (2023), 53728‚Äì53741. [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). A LoRA and FineTuning Hyperparameters. The LoRA and Finetuning hyperparameters used in tuning the base model and API generation module is listed in Tab. 2: Table 2: LoRA and FineTuning Hyperparameters. Name Value LoRA_alpha 16 LoRA_dropout 0.1 LoRA_r 8 target_modules [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"] bias \"none\" 4-bit True max_seq_length 2048/4096 per_device_train_batch_size 1 gradient_accumulation_steps 4 optim \"adamw_hf\" learning_rate 2e-4 max_grad_norm 0.3 scheduler \"cosine\", warm_up_ratio=0.1",
  "pages": [
    {
      "page": 1,
      "text_raw": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25\n1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao\n2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä†\n1 Key Laboratory of High Confidence Software Technologies, CS, Peking University, China\nwfl00014@pku.edu.cn, {chenjiazun, zhanyirui, suifengzhao}@stu.pku.edu.cn, gaojun@pku.edu.cn, China\n2 Theory Lab, Central Research Institute, 2012 Labs, Huawei Technologies Co., Ltd, Shenzhen, China\n{jiangweipeng, zhang.chaorui, harvey.hanwei, baibo8}@huawei.com, China\nAbstract\nThis paper presents the db3 team‚Äôs winning solution for the Meta\nCRAG-MM Challenge 2025 at KDD Cup‚Äô25. Addressing the chal-\nlenge‚Äôs unique multi-modal, multi-turn question answering bench-\nmark (CRAG-MM), we developed a comprehensive framework that\nintegrates tailored retrieval pipelines for different tasks with a uni-\nfied LLM-tuning approach for hallucination control. Our solution\nfeatures (1) domain-specific retrieval pipelines handling image-\nindexed knowledge graphs, web sources, and multi-turn conver-\nsations; and (2) advanced refusal training using SFT, DPO, and\nRL. The system achieved 2nd place in Task 1, 2nd place in Task\n2, and 1st place in Task 3, securing the grand prize for excellence\nin ego-centric queries through superior handling of first-person\nperspective challenges.\nCCS Concepts\n‚Ä¢ Computing methodologies ‚ÜíNatural language generation.\nKeywords\nVisual Language Models, Multimodal RAG\n1\nIntroduction\nRecent advancements in artificial intelligence have highlighted the\nimportance of multi-modal reasoning, especially in scenarios where\nsystems must interpret and respond to both visual and textual infor-\nmation. The Meta CRAG-MM Challenge 2025, hosted on AIcrowd,\naims to accelerate progress in this area by introducing a novel\nbenchmark, called CRAG-MM, that encompasses both single-turn\nand multi-turn visual question answering tasks. Unlike traditional\nVQA datasets, CRAG-MM is designed to evaluate factual question\nanswering in multi-modal contexts, integrating images with com-\nplex, multi-turn conversational flows.\nNotably, ego-centric images are a major characteristic of this\ncontest. In the context of the Meta CRAG-MM Challenge 2025,\nego-centric images refer to images captured from a first-person\nperspective, typically using wearable cameras mounted on glasses.\nUnlike traditional third-person or static camera views, ego-centric\nimages provide a direct visual record of the user‚Äôs personal experi-\nence and interactions within their environment. This perspective\nenables the capture of rich contextual information, including hand-\nobject interactions, dynamic activities, and real-time environmental\nchanges. However, ego-centric images also present unique chal-\nlenges, such as frequent occlusions, rapid viewpoint shifts, and\n*These authors contributed equally to this research.\n‚Ä†Corresponding Authors .\ncomplex backgrounds. Addressing these challenges is crucial for\nadvancing machine perception and understanding of daily human\nactivities from a truly immersive viewpoint, making ego-centric\nimage analysis a key focus of this competition.\nThe Meta CRAG-MM Challenge 2025 is composed of the follow-\ning three tasks:\n(1) Task 1: Participants are required to answer single-turn\nquestions using both image-indexed knowledge graphs as\ncontext.\n(2) Task 2: This task also involves single-turn questions but\nadditionally introduces a web search knowledge source.\n(3) Task 3: A multi-turn retrieval-augmented generation (RAG)\ntask, where systems must engage in dialogue-like interac-\ntions, retrieving and integrating information from multiple\nsources to generate coherent responses.\nThe author‚Äôs team, db3, participates in the contest and achieves\nsecond place, second place, and first place in the three tasks, respec-\ntively, resulting in winning the grand prize in ego image total score.\nThis paper describes the author‚Äôs solution to the three tasks.\nThe two main challenges of this contest are 1. How to deal with\nthe retrieval information, both the multimodal part and the multi-\nturn conversation part. 2. How to control hallucinations. As the\nmetrics of the contest involve punishing answering incorrect an-\nswers, we have to train the model to output \"I don‚Äôt know\" on\nqueries hard for them to answer. Therefore, we describe our solu-\ntions in two parts: the retrieval part and the hallucination control\npart. In this paper, in addition to presenting our final submitted\nsolution, we also describe the various approaches and experiments\nwe attempted during the development process. Our code is available\non GitLab 1.\nIn the remainder of the paper, we first introduce some basic\nconstructions we make for this contest . We discuss the retrieval\npart in Sec. 3, and we discuss the hallucination control part in Sec. 4.\nWe discuss the checkpoint selection and ensemble tricks we use in\nthis contest in Sec. 5. We conclude our work and look into future\nworks in Sec. 6.\n2\nBasic Constructions.\nWe construct some basic modules for this contest that are useful\nfor all the following procedures.\nLLM judge. Since the online judge and prompt for this contest are\nnot available, we construct a local LLM judge for this contest. We\nobserve that using different prompts and LLM judges can have a\nhuge difference in the results, and some of our training process\nrelies on the LLM judge‚Äôs results. We enumerate some combinations\n1https://gitlab.aicrowd.com/jiazunchen/db3-team-s-solution-for-meta-kdd-cup-25\narXiv:2509.09681v1  [cs.IR]  12 Aug 2025\n",
      "text_clean": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† 1 Key Laboratory of High Confidence Software Technologies, CS, Peking University, China wfl00014@pku.edu.cn, {chenjiazun, zhanyirui, suifengzhao}@stu.pku.edu.cn, gaojun@pku.edu.cn, China 2 Theory Lab, Central Research Institute, 2012 Labs, Huawei Technologies Co., Ltd, Shenzhen, China {jiangweipeng, zhang.chaorui, harvey.hanwei, baibo8}@huawei.com, China Abstract This paper presents the db3 team‚Äôs winning solution for the Meta CRAG-MM Challenge 2025 at KDD Cup‚Äô25. Addressing the challenge‚Äôs unique multi-modal, multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive framework that integrates tailored retrieval pipelines for different tasks with a unified LLM-tuning approach for hallucination control. Our solution features (1) domain-specific retrieval pipelines handling imageindexed knowledge graphs, web sources, and multi-turn conversations; and (2) advanced refusal training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd place in Task 2, and 1st place in Task 3, securing the grand prize for excellence in ego-centric queries through superior handling of first-person perspective challenges. CCS Concepts ‚Ä¢ Computing methodologies ‚ÜíNatural language generation. Keywords Visual Language Models, Multimodal RAG 1 Introduction Recent advancements in artificial intelligence have highlighted the importance of multi-modal reasoning, especially in scenarios where systems must interpret and respond to both visual and textual information. The Meta CRAG-MM Challenge 2025, hosted on AIcrowd, aims to accelerate progress in this area by introducing a novel benchmark, called CRAG-MM, that encompasses both single-turn and multi-turn visual question answering tasks. Unlike traditional VQA datasets, CRAG-MM is designed to evaluate factual question answering in multi-modal contexts, integrating images with complex, multi-turn conversational flows. Notably, ego-centric images are a major characteristic of this contest. In the context of the Meta CRAG-MM Challenge 2025, ego-centric images refer to images captured from a first-person perspective, typically using wearable cameras mounted on glasses. Unlike traditional third-person or static camera views, ego-centric images provide a direct visual record of the user‚Äôs personal experience and interactions within their environment. This perspective enables the capture of rich contextual information, including handobject interactions, dynamic activities, and real-time environmental changes. However, ego-centric images also present unique challenges, such as frequent occlusions, rapid viewpoint shifts, and *These authors contributed equally to this research. ‚Ä†Corresponding Authors . complex backgrounds. Addressing these challenges is crucial for advancing machine perception and understanding of daily human activities from a truly immersive viewpoint, making ego-centric image analysis a key focus of this competition. The Meta CRAG-MM Challenge 2025 is composed of the following three tasks: (1) Task 1: Participants are required to answer single-turn questions using both image-indexed knowledge graphs as context. (2) Task 2: This task also involves single-turn questions but additionally introduces a web search knowledge source. (3) Task 3: A multi-turn retrieval-augmented generation (RAG) task, where systems must engage in dialogue-like interactions, retrieving and integrating information from multiple sources to generate coherent responses. The author‚Äôs team, db3, participates in the contest and achieves second place, second place, and first place in the three tasks, respectively, resulting in winning the grand prize in ego image total score. This paper describes the author‚Äôs solution to the three tasks. The two main challenges of this contest are 1. How to deal with the retrieval information, both the multimodal part and the multiturn conversation part. 2. How to control hallucinations. As the metrics of the contest involve punishing answering incorrect answers, we have to train the model to output \"I don‚Äôt know\" on queries hard for them to answer. Therefore, we describe our solutions in two parts: the retrieval part and the hallucination control part. In this paper, in addition to presenting our final submitted solution, we also describe the various approaches and experiments we attempted during the development process. Our code is available on GitLab 1. In the remainder of the paper, we first introduce some basic constructions we make for this contest . We discuss the retrieval part in Sec. 3, and we discuss the hallucination control part in Sec. 4. We discuss the checkpoint selection and ensemble tricks we use in this contest in Sec. 5. We conclude our work and look into future works in Sec. 6. 2 Basic Constructions. We construct some basic modules for this contest that are useful for all the following procedures. LLM judge. Since the online judge and prompt for this contest are not available, we construct a local LLM judge for this contest. We observe that using different prompts and LLM judges can have a huge difference in the results, and some of our training process relies on the LLM judge‚Äôs results. We enumerate some combinations 1https://gitlab.aicrowd.com/jiazunchen/db3-team-s-solution-for-meta-kdd-cup-25 arXiv:2509.09681v1 [cs.IR] 12 Aug 2025",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 2,
      "text_raw": "1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä†\nImage\nDomain \nadapter\nContext\nMath\nMath \nadapter\nAnswer\nPlants\nEntity Chunks\n‚Ä¶\nWeb Chunks\nRerank\ner\nRewrite Query\nResize\nTool \nProcess\nEntity\nProcess\nWeb\nProcess\nAdapters\nM\nE\nR\nG\nE\nAnswer_0\nAnswer_1\nAnswer_2\nAnswer\nFigure 1: Pipeline for Different Tasks: A domain adapter is first applied to classify the query domain for different pipelines,\ne.g., a math tool for math problems, a retrieval through entity chunks module for plants, and a rewriting and retrieving from\nweb chunks module for all other domains. After a reranker module for all retrieved text chunks, we obtain multiple answers\nusing trained hallucination control adapters, and these answers are ensembled to obtain a final output answer.\nof the prompts and LLM APIs we use locally and reconcile the local\nscores with the online scores we get. The following prompt applied\nwith GPT-4o-mini achieves the relatively most similar results with\nonline evaluation results. The prompt used is as follows:\n1 You are an expert evaluator for question answering systems.\n2 Your task is to determine if a prediction correctly answers a\n‚Ü©‚Üíquestion based on the ground truth.\n3\n4 Rules:\n5 1. The prediction is correct if it captures all the key\n‚Ü©‚Üíinformation from the ground truth.\n6 2. The prediction is correct even if phrased differently as\n‚Ü©‚Üílong as the meaning is the same.\n7 3. The prediction is incorrect if it contains incorrect\n‚Ü©‚Üíinformation or is missing essential details.\n8\n9\n10 Question: {query}\n11 Ground truth: {ground_truth}\n12 Prediction: {prediction}\n13\n14 Output only 'true' or 'false' to indicate if the prediction is\n‚Ü©‚Üícorrect.\nNoting that this prompt is not the best one that follows human\nevaluation. Through our observation, this prompt and online eval-\nuation is much stricter than human evaluation, resulting in many\nof the results that should be correct being assigned as wrong dur-\ning online evaluation. However, we have to choose this prompt to\npursue a better score during the online evaluation.\nTrain/Validation Split. To perform checkpoint selection and some\nother techniques, we have to split the training and validation sets.\nAs the query types and domains vary, we also split the train/valida-\ntion set proportionally to make sure each question type and domain\nappears equally in the train and validation splits.\nDomain Prediction. We rely on the domain splits to apply dif-\nferent pipelines for different domains. Since some of the domains\nhave blurred semantics, we combine some of the originally split\ndomains and have these final domains for classification: vehicle,\nplant, local, math, science, food, animal, and other. We re-annotate\nthe queries using the new classes, and we tune Llama 3.2-VL for\ndomain classification. The performance of this module is accept-\nable, as the classification accuracy of this model is 91.17% on the\nAnimal\nFood\nLocal\nMath\nOther\nPlants Science Vehicle\nAnimal\nFood\nLocal\nMath\nOther\nPlants\nScience\nVehicle\n96%\n1%\n0%\n0%\n3%\n0%\n0%\n0%\n1%\n90%\n2%\n0%\n7%\n1%\n0%\n0%\n0%\n0%\n93%\n0%\n7%\n0%\n0%\n0%\n0%\n0%\n0%\n73% 18%\n0%\n9%\n0%\n1%\n3%\n4%\n0%\n88%\n0%\n1%\n3%\n1%\n2%\n0%\n0%\n1%\n97%\n0%\n0%\n2%\n0%\n0%\n0%\n27%\n0%\n70%\n0%\n0%\n0%\n1%\n0%\n2%\n0%\n0%\n97%\nOverall Accuracy: 91.17%\nFigure 2: Confusion Matrix of Domain Prediction\nvalidation set, and many of the wrongly assigned queries actually\ncan fit in many classes. The confusion matrix is presented in Fig. 2.\n3\nThe Retrieval Component of the Solution\nIn this section, we will propose the retrieval component of our\nsolution. In this contest, the query is always associated with a\nquery image. Therefore, we need to deal with both the query image\nand the text query. Specifically, we will present pipelines involving\nretrieving from the image-indexed knowledge graph in Task #1, the\ntext-indexed web source in Task #2, and the multi-turn context in\nTask #3. Additionally, two techniques that are useful for all three\ntasks are presented in this section: the OCR extraction and the tool\nsolution for math problems.\n3.1\nRetrieval Pipeline for Image-indexed\nKnowledge Graph.\nIn Task 1, an image-indexed knowledge graph is provided as the\ninformation source. In Task 1, an image-indexed knowledge graph is\nprovided as the information source. Specifically, an image is linked\nwith a CLIP [4] index and is associated with a structured segment\nof text, which serves as a description of the entity represented by\nthe image. The information useful for answering the question is\nmostly based on the information in the structured text; therefore,\n",
      "text_clean": "1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† Image Domain adapter Context Math Math adapter Answer Plants Entity Chunks ‚Ä¶ Web Chunks Rerank er Rewrite Query Resize Tool Process Entity Process Web Process Adapters M E R G E Answer_0 Answer_1 Answer_2 Answer Figure 1: Pipeline for Different Tasks: A domain adapter is first applied to classify the query domain for different pipelines, e.g., a math tool for math problems, a retrieval through entity chunks module for plants, and a rewriting and retrieving from web chunks module for all other domains. After a reranker module for all retrieved text chunks, we obtain multiple answers using trained hallucination control adapters, and these answers are ensembled to obtain a final output answer. of the prompts and LLM APIs we use locally and reconcile the local scores with the online scores we get. The following prompt applied with GPT-4o-mini achieves the relatively most similar results with online evaluation results. The prompt used is as follows: 1 You are an expert evaluator for question answering systems. 2 Your task is to determine if a prediction correctly answers a ‚Ü©‚Üíquestion based on the ground truth. 3 4 Rules: 5 1. The prediction is correct if it captures all the key ‚Ü©‚Üíinformation from the ground truth. 6 2. The prediction is correct even if phrased differently as ‚Ü©‚Üílong as the meaning is the same. 7 3. The prediction is incorrect if it contains incorrect ‚Ü©‚Üíinformation or is missing essential details. 8 9 10 Question: {query} 11 Ground truth: {ground_truth} 12 Prediction: {prediction} 13 14 Output only 'true' or 'false' to indicate if the prediction is ‚Ü©‚Üícorrect. Noting that this prompt is not the best one that follows human evaluation. Through our observation, this prompt and online evaluation is much stricter than human evaluation, resulting in many of the results that should be correct being assigned as wrong during online evaluation. However, we have to choose this prompt to pursue a better score during the online evaluation. Train/Validation Split. To perform checkpoint selection and some other techniques, we have to split the training and validation sets. As the query types and domains vary, we also split the train/validation set proportionally to make sure each question type and domain appears equally in the train and validation splits. Domain Prediction. We rely on the domain splits to apply different pipelines for different domains. Since some of the domains have blurred semantics, we combine some of the originally split domains and have these final domains for classification: vehicle, plant, local, math, science, food, animal, and other. We re-annotate the queries using the new classes, and we tune Llama 3.2-VL for domain classification. The performance of this module is acceptable, as the classification accuracy of this model is 91.17% on the Animal Food Local Math Other Plants Science Vehicle Animal Food Local Math Other Plants Science Vehicle 96% 1% 0% 0% 3% 0% 0% 0% 1% 90% 2% 0% 7% 1% 0% 0% 0% 0% 93% 0% 7% 0% 0% 0% 0% 0% 0% 73% 18% 0% 9% 0% 1% 3% 4% 0% 88% 0% 1% 3% 1% 2% 0% 0% 1% 97% 0% 0% 2% 0% 0% 0% 27% 0% 70% 0% 0% 0% 1% 0% 2% 0% 0% 97% Overall Accuracy: 91.17% Figure 2: Confusion Matrix of Domain Prediction validation set, and many of the wrongly assigned queries actually can fit in many classes. The confusion matrix is presented in Fig. 2. 3 The Retrieval Component of the Solution In this section, we will propose the retrieval component of our solution. In this contest, the query is always associated with a query image. Therefore, we need to deal with both the query image and the text query. Specifically, we will present pipelines involving retrieving from the image-indexed knowledge graph in Task #1, the text-indexed web source in Task #2, and the multi-turn context in Task #3. Additionally, two techniques that are useful for all three tasks are presented in this section: the OCR extraction and the tool solution for math problems. 3.1 Retrieval Pipeline for Image-indexed Knowledge Graph. In Task 1, an image-indexed knowledge graph is provided as the information source. In Task 1, an image-indexed knowledge graph is provided as the information source. Specifically, an image is linked with a CLIP [4] index and is associated with a structured segment of text, which serves as a description of the entity represented by the image. The information useful for answering the question is mostly based on the information in the structured text; therefore,",
      "figure_captions": [
        {
          "bbox": [
            53.44900131225586,
            216.97789001464844,
            559.2922973632812,
            258.86077880859375
          ],
          "text": "Figure 1: Pipeline for Different Tasks: A domain adapter is first applied to classify the query domain for different pipelines, e.g., a math tool for math problems, a retrieval through entity chunks module for plants, and a rewriting and retrieving from web chunks module for all other domains. After a reranker module for all retrieved text chunks, we obtain multiple answers using trained hallucination control adapters, and these answers are ensembled to obtain a final output answer."
        },
        {
          "bbox": [
            338.06298828125,
            415.203369140625,
            538.0946044921875,
            424.1697692871094
          ],
          "text": "Figure 2: Confusion Matrix of Domain Prediction"
        }
      ],
      "images": [
        {
          "image_id": "p2_cluster_001",
          "page": 2,
          "file_path": "images/page_0002_cluster_001.png",
          "bbox": [
            121.00655364990234,
            134.95948791503906,
            154.4251708984375,
            155.0157928466797
          ],
          "caption": "a camera with a smiley face on it's face",
          "detailed_caption": "The image shows a cartoon camera with a smiley face on it against a white background. The camera is in the center of the image, with its arms outstretched and its head tilted slightly to the side. Its eyes are wide and its mouth is slightly open, giving it a cheerful expression. Its body is round and its antennae are long and thin.",
          "ocr_text": "C",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>C</s>",
            "parsed": {
              "<OCR>": "C"
            }
          },
          "paper_caption": "Figure 1: Pipeline for Different Tasks: A domain adapter is first applied to classify the query domain for different pipelines, e.g., a math tool for math problems, a retrieval through entity chunks module for plants, and a rewriting and retrieving from web chunks module for all other domains. After a reranker module for all retrieved text chunks, we obtain multiple answers using trained hallucination control adapters, and these answers are ensembled to obtain a final output answer.",
          "paper_caption_bbox": [
            53.44900131225586,
            216.97789001464844,
            559.2922973632812,
            258.86077880859375
          ]
        }
      ]
    },
    {
      "page": 3,
      "text_raw": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25\nthe task involves identifying the correct entity the query is about\nin the knowledge graph.\nRetrieving and Reranking through Image. The most direct\nsolution is to retrieve and rerank through the query image and the\nindex images. The most related images are first retrieved through\nthe CLIP index, and the related text information is used as the\nretrieved information. The key challenge here is the quality of\nthe retrieval results. Through experiment, we observe that directly\napplying the top 1/3/5 results of this source through the image index\nharms the final QA accuracy. We applied the following techniques\nto improve the retrieval quality.\nGrounding In the Query Image. Ego-centric images consist of\na large portion of the query images. The distribution of ego-centric\nimages differs a lot from the index images. The ego-centric images\nare mostly taken in everyday life and contain much irrelevant\nbackground, while the index image is often a Wikipedia-style image\nconcentrating on the entity. Therefore, the CLIP embeddings of the\nquery image and correct index image are very likely to differ a lot.\nFor example, if the query is about a car parked in front of a building,\nthe CLIP embedding of the query image will be more similar to a\nbuilding‚Äôs CLIP embedding. We have to clip the car part of the query\nimage to retrieve the correct car entity in the knowledge graph.\nTherefore, we introduce grounding in the query image to resolve\nthis problem. We leverage the open-source Grounding-DINO [3]\nmodel to deal with this task. The Grounding-DINO model takes in a\nshort text and an image as input. The short text serves as a prompt\ndescribing the object or region of interest (for example, ‚Äúthe plant\nin the middle‚Äù or ‚Äúa black car‚Äù). The model then processes both the\nimage and the text to identify and localize the region in the image\nthat best matches the given description.\nThrough our annotation, we find that with the appropriate text\ninput, the Grounding-DINO model can localize the correct entity in\nmost cases, and through statistics, the correct entities appear more\nfrequently in the retrieved top entities. However, the text input is\nquite hard to obtain. The ideal input is related to the query and the\nquery image, and it‚Äôs hard to prompt the VLM to output the ideal\ndescribing text having the same quality as the human-annotated\nones. In practice, we prompt VLMs to generate the describing text.\nThe Dino prompt we used is as follows:\n1 Image: {}\n2 Given this image, a query, your task is to simply describe the\n‚Ü©‚Üíobject in the image.\n3 Query: {}\n4 Output only simple object names in phrases, do not output a\n‚Ü©‚Üísentence.\n5 Do not answer the query, just output the object name appearing\n‚Ü©‚Üíin the image, not the answer or answer entity.\nAlternatively, we can use a certain phrase for each domain, e.g.,\nCar, Plant, Building,..., as the DINO phrase to minimize the LLM\ninference cost.\nReranking through Image. In the contest retrieval scenario,\nretrieving through CLIP embeddings is highly inaccurate. Following\nhuman practice, checking whether two images refer to the same\nentity is essential. Therefore, we propose reranking by comparing\nthe query and index images. We use the following prompt to let\nVLMs judge whether the two images belong to the same entity. We\nuse the image rerank prompt to do this task:\nWhat is the horsepower \nof this truck?\nA white truck\nDino prompt\nGrounding DINO\nImage API\nImage rerank prompt\nNo\nLlama\nLlama\nFigure 3: Grounding in the Query Image + Rerank through\nImage\n1 Image: {query image},{index image}\n2 Given two images, the first one is a query image, the second\n‚Ü©‚Üíone is an image about an entity, a query about the first\n‚Ü©‚Üíimage, descriptions about the second image, your task is to\n‚Ü©‚Üí\ndetermine whether the query about the first image is about\n‚Ü©‚Üí\nthe entity in the second image.\n3 Query: {}\n4 Description: {}\n5 If the entity in the second image appears in the first image,\n‚Ü©‚Üíoutput Yes, otherwise, output No.\nWe only preserve the related items (with judge output yes) in the\ntop index. Through experiment, we observe that using this prompt\nwith a powerful VLM, e.g., GPT-4o, can achieve considerable boosts\nin performance. Since only a relatively small VLM can be used in\nthe contest, we aim to distill Llama 3.2-VL to achieve this ability.\nSpecifically, we sample the top 5 candidates for each query using\nthe grounding and retrieving through image pipeline. We verify\neach candidate by using them to retrieve content to answer the\nquery and judging through a strong VLM, e.g., GPT-4o. A candidate\nis viewed as a related item if the retrieved item helps the base VLM\nanswer the query correctly, and the strong VLM judges the entity\nin the index image to be the same as the entity in the grounded\nquery image. Such a candidate is labeled as relevant, and the base\nVLM is trained to classify the relevance.\nIn our experiment, we observe that reranking with a powerful\nVLM can largely boost the final QA results (by 10%), and using a\ndistilled Llama 3.2-VL can boost the QA performance by 2%. Since\nthe boost is tiny, and we try this technique in the early stage of the\ncontest, we don‚Äôt have time to add it to the final pipeline.\nRetrieving and Reranking through Text. Since we observe\nsevere difficulty in utilizing the top-k candidates through CLIP\nimage embedding, we propose retrieving and reranking through\ntext. In many cases, the VLM itself can identify the correct entity\nname the query is about. We can retrieve the relevant attributes\nabout this entity from the image-indexed knowledge graph. We\nmade two attempts at retrieving through text: the entity name\napproach and the merged text query approach.\nThe entity name approach. The entity name approach means\nfirst extracting the desired entity name from the query and query\nimage, and then identifying the same entity from the knowledge\ngraph. For example, the query asks about when this plant blooms;\nthe entity name will be the plant‚Äôs botanical name, which is ex-\npected to exist in the knowledge graph.\nIn the first step, we prompt the VLM to extract the query entity‚Äôs\nname. We use the entity prompt to conduct this task.\n1 Given an image and a query about it, your task is to extract\n‚Ü©‚Üíthe entity's name the query is about.\n",
      "text_clean": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 the task involves identifying the correct entity the query is about in the knowledge graph. Retrieving and Reranking through Image. The most direct solution is to retrieve and rerank through the query image and the index images. The most related images are first retrieved through the CLIP index, and the related text information is used as the retrieved information. The key challenge here is the quality of the retrieval results. Through experiment, we observe that directly applying the top 1/3/5 results of this source through the image index harms the final QA accuracy. We applied the following techniques to improve the retrieval quality. Grounding In the Query Image. Ego-centric images consist of a large portion of the query images. The distribution of ego-centric images differs a lot from the index images. The ego-centric images are mostly taken in everyday life and contain much irrelevant background, while the index image is often a Wikipedia-style image concentrating on the entity. Therefore, the CLIP embeddings of the query image and correct index image are very likely to differ a lot. For example, if the query is about a car parked in front of a building, the CLIP embedding of the query image will be more similar to a building‚Äôs CLIP embedding. We have to clip the car part of the query image to retrieve the correct car entity in the knowledge graph. Therefore, we introduce grounding in the query image to resolve this problem. We leverage the open-source Grounding-DINO [3] model to deal with this task. The Grounding-DINO model takes in a short text and an image as input. The short text serves as a prompt describing the object or region of interest (for example, ‚Äúthe plant in the middle‚Äù or ‚Äúa black car‚Äù). The model then processes both the image and the text to identify and localize the region in the image that best matches the given description. Through our annotation, we find that with the appropriate text input, the Grounding-DINO model can localize the correct entity in most cases, and through statistics, the correct entities appear more frequently in the retrieved top entities. However, the text input is quite hard to obtain. The ideal input is related to the query and the query image, and it‚Äôs hard to prompt the VLM to output the ideal describing text having the same quality as the human-annotated ones. In practice, we prompt VLMs to generate the describing text. The Dino prompt we used is as follows: 1 Image: {} 2 Given this image, a query, your task is to simply describe the ‚Ü©‚Üíobject in the image. 3 Query: {} 4 Output only simple object names in phrases, do not output a ‚Ü©‚Üísentence. 5 Do not answer the query, just output the object name appearing ‚Ü©‚Üíin the image, not the answer or answer entity. Alternatively, we can use a certain phrase for each domain, e.g., Car, Plant, Building,..., as the DINO phrase to minimize the LLM inference cost. Reranking through Image. In the contest retrieval scenario, retrieving through CLIP embeddings is highly inaccurate. Following human practice, checking whether two images refer to the same entity is essential. Therefore, we propose reranking by comparing the query and index images. We use the following prompt to let VLMs judge whether the two images belong to the same entity. We use the image rerank prompt to do this task: What is the horsepower of this truck? A white truck Dino prompt Grounding DINO Image API Image rerank prompt No Llama Llama Figure 3: Grounding in the Query Image + Rerank through Image 1 Image: {query image},{index image} 2 Given two images, the first one is a query image, the second ‚Ü©‚Üíone is an image about an entity, a query about the first ‚Ü©‚Üíimage, descriptions about the second image, your task is to ‚Ü©‚Üí determine whether the query about the first image is about ‚Ü©‚Üí the entity in the second image. 3 Query: {} 4 Description: {} 5 If the entity in the second image appears in the first image, ‚Ü©‚Üíoutput Yes, otherwise, output No. We only preserve the related items (with judge output yes) in the top index. Through experiment, we observe that using this prompt with a powerful VLM, e.g., GPT-4o, can achieve considerable boosts in performance. Since only a relatively small VLM can be used in the contest, we aim to distill Llama 3.2-VL to achieve this ability. Specifically, we sample the top 5 candidates for each query using the grounding and retrieving through image pipeline. We verify each candidate by using them to retrieve content to answer the query and judging through a strong VLM, e.g., GPT-4o. A candidate is viewed as a related item if the retrieved item helps the base VLM answer the query correctly, and the strong VLM judges the entity in the index image to be the same as the entity in the grounded query image. Such a candidate is labeled as relevant, and the base VLM is trained to classify the relevance. In our experiment, we observe that reranking with a powerful VLM can largely boost the final QA results (by 10%), and using a distilled Llama 3.2-VL can boost the QA performance by 2%. Since the boost is tiny, and we try this technique in the early stage of the contest, we don‚Äôt have time to add it to the final pipeline. Retrieving and Reranking through Text. Since we observe severe difficulty in utilizing the top-k candidates through CLIP image embedding, we propose retrieving and reranking through text. In many cases, the VLM itself can identify the correct entity name the query is about. We can retrieve the relevant attributes about this entity from the image-indexed knowledge graph. We made two attempts at retrieving through text: the entity name approach and the merged text query approach. The entity name approach. The entity name approach means first extracting the desired entity name from the query and query image, and then identifying the same entity from the knowledge graph. For example, the query asks about when this plant blooms; the entity name will be the plant‚Äôs botanical name, which is expected to exist in the knowledge graph. In the first step, we prompt the VLM to extract the query entity‚Äôs name. We use the entity prompt to conduct this task. 1 Given an image and a query about it, your task is to extract ‚Ü©‚Üíthe entity's name the query is about.",
      "figure_captions": [
        {
          "bbox": [
            317.9549865722656,
            155.70614624023438,
            558.2002563476562,
            175.6787872314453
          ],
          "text": "Figure 3: Grounding in the Query Image + Rerank through Image"
        }
      ],
      "images": [
        {
          "image_id": "p3_cluster_001",
          "page": 3,
          "file_path": "images/page_0003_cluster_001.png",
          "bbox": [
            325.31939697265625,
            85.88158416748047,
            356.70220947265625,
            126.80449676513672
          ],
          "caption": "A car is parked in a parking lot at night.",
          "detailed_caption": "The image shows two cars parked in a parking lot at night, illuminated by a street light and surrounded by trees. The sky is visible in the background, creating a peaceful atmosphere.",
          "ocr_text": "C",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>C</s>",
            "parsed": {
              "<OCR>": "C"
            }
          },
          "paper_caption": "Figure 3: Grounding in the Query Image + Rerank through Image",
          "paper_caption_bbox": [
            317.9549865722656,
            155.70614624023438,
            558.2002563476562,
            175.6787872314453
          ]
        },
        {
          "image_id": "p3_cluster_002",
          "page": 3,
          "file_path": "images/page_0003_cluster_002.png",
          "bbox": [
            376.071533203125,
            106.40728759765625,
            391.0080871582031,
            115.37150573730469
          ],
          "caption": "a cartoon robot with a pink nose and blue eyes",
          "detailed_caption": "The image shows a small robot with a pink button on its head against a white background. It appears to be an edited photo, giving it a unique and eye-catching look.",
          "ocr_text": "C",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>C</s>",
            "parsed": {
              "<OCR>": "C"
            }
          },
          "paper_caption": "Figure 3: Grounding in the Query Image + Rerank through Image",
          "paper_caption_bbox": [
            317.9549865722656,
            155.70614624023438,
            558.2002563476562,
            175.6787872314453
          ]
        },
        {
          "image_id": "p3_cluster_003",
          "page": 3,
          "file_path": "images/page_0003_cluster_003.png",
          "bbox": [
            407.2615966796875,
            93.41409301757812,
            535.5552368164062,
            137.43673706054688
          ],
          "caption": "A diagram of an image API with a car in the background.",
          "detailed_caption": "The image shows a diagram of a car parked on the side of a road, surrounded by trees, street lights, and a clear blue sky. The car is in the center of the image, with the text \"Image API\" above it.",
          "ocr_text": "Image APILlama No",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>Image APILlamaNo</s>",
            "parsed": {
              "<OCR>": "Image APILlamaNo"
            }
          },
          "paper_caption": "Figure 3: Grounding in the Query Image + Rerank through Image",
          "paper_caption_bbox": [
            317.9549865722656,
            155.70614624023438,
            558.2002563476562,
            175.6787872314453
          ]
        },
        {
          "image_id": "p3_cluster_004",
          "page": 3,
          "file_path": "images/page_0003_cluster_004.png",
          "bbox": [
            453.8861083984375,
            85.48007202148438,
            487.0516662597656,
            100.7860107421875
          ],
          "caption": "a car is seen in this surveillance image",
          "detailed_caption": "The image shows a car parked in a parking lot at night, with a blurred background. The car is in focus, while the background is slightly out of focus, giving the photo a dreamy, ethereal quality.",
          "ocr_text": "C",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>C</s>",
            "parsed": {
              "<OCR>": "C"
            }
          },
          "paper_caption": "Figure 3: Grounding in the Query Image + Rerank through Image",
          "paper_caption_bbox": [
            317.9549865722656,
            155.70614624023438,
            558.2002563476562,
            175.6787872314453
          ]
        },
        {
          "image_id": "p3_cluster_005",
          "page": 3,
          "file_path": "images/page_0003_cluster_005.png",
          "bbox": [
            530.576416015625,
            92.78772735595703,
            556.49853515625,
            108.76821899414062
          ],
          "caption": "a white suv parked in a parking lot",
          "detailed_caption": "The image shows a white SUV parked in a parking lot surrounded by trees. The photo is slightly blurred, giving it a dreamy quality.",
          "ocr_text": "1",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>1</s>",
            "parsed": {
              "<OCR>": "1"
            }
          },
          "paper_caption": "Figure 3: Grounding in the Query Image + Rerank through Image",
          "paper_caption_bbox": [
            317.9549865722656,
            155.70614624023438,
            558.2002563476562,
            175.6787872314453
          ]
        }
      ]
    },
    {
      "page": 4,
      "text_raw": "1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä†\nRam 2500\nEntity prompt\nImage API\nLlama\nEntity match\nRam 2500\n{Kia Telluride,‚Ä¶}\n{Kia Telluride,‚Ä¶}\n{GMC Terrain,‚Ä¶}\nCandidate entity \ninformation\nWhat is the \nhorsepower of this \ntruck?\nFigure 4: Retrieving and Reranking through Text.\n2 If the entity is about a plant, you should output the full\n‚Ü©‚Üíbotanical name.\n3 ....\n4 If the enityt is about a vehicle, you should output the vehicle\n‚Ü©‚Üí's brand and model.\n5 Query: {}\n6 You should output the entity's name directly.\nThe prompt consists of detailed entity name instructions that\nmatch the entities‚Äô names in the knowledge graph. Similarly, prompt-\ning strong VLMs, e.g., GPT-4o, can extract more correct entity\nnames compared to prompting Llama 3.2-VL. On one hand, GPT-4o\npossesses more internal knowledge. On the other hand, GPT-4o\ncan follow the instructions better. We also tune Llama 3.2-VL using\nthe ground truth label name to learn the format and follow the\ninstructions better. (The ground truth label is obtained by similar\nprompts with the ground truth answer. As the ground truth label\nusually appears in the ground truth answer, most of the GPT-4o\nextraction results are identical to the ground truth entity name.)\nIn the second step, we use the retrieving-through-image method\nto extract a large number of candidates (e.g., 1000), and the gener-\nated entity name is compared with the entity names of the candi-\ndates. The entity names are tokenized and stemmed, and only if\nthe processed token set is identical or contains the other are the\ntwo entity names considered related.\nThrough experiment, we find that using GPT-4o, 40% of the\nentity name can be extracted correctly. The ratio is 20% for Llama\n3.2-VL. The potential of the QA system can be boosted by 10% using\nthe GPT-4o result, and the result is 5% for the Llama 3.2-VL results.\nUnfortunately, we didn‚Äôt have enough time to train a model that\nperformed well in all domains. As a result, we could only apply this\nsolution in the field of plants.\nNotably, even powerful VLMs like GPT-4o can only identify\n40% of the entity‚Äôs desired names. This indicates that for the hard\ncases like vehicles, plants, and food, state-of-the-art VLMs still can‚Äôt\nrecall their names, as different models of cars or different plants\nin the same class look quite similar. During the contest, we try to\nsearch for open-source models to identify a car‚Äôs brand and model\nor plants‚Äô botanical names. However, it turns out that there are no\nsuch open-source models, and a system that can deal with these\nneeds requires a highly sophisticated database building and model\ntraining.\nQuery using merged text query. A merged text query is a pure\ntext query that contains all the query information in the original\ntext query and the query image. We will propose the full merged\ntext query rewrite process in Sec. 3.2 in detail. Suppose we have the\nmerged query here. We follow the same steps of retrieving a large\nnumber of candidates (e.g., 1000), and we convert the structured\nWhat is the \nhorsepower of this \ntruck?\nWhat is the \nhorsepower of\nRam 2500?\nRewrite prompt\nWeb API\nLlama\nThe New Ram 2500 \nAnd Its Incredible\nThe 2021 Ram 2500 has \na 6.4-liter V-8 \nengine that generates \n410 horsepower‚Ä¶\nCandidate entity \ninformation\nWhat is the \nhorsepower of\nRam 2500?\nWeb retrieval \nchunks\nWhat is the \nhorsepower of\nRam 2500?\nTOP K \nchunks\nReranker\nFigure 5: Retrieval Pipeline for Text-indexed Web Source.\ntext information of the candidates to a text embedding retrieval\nbase.\nSpecifically, the attribute-value pair in the entity‚Äôs structured\ntext is converted into separate text outputs. For example, the Volk-\nswagen Beetle entity has an attribute, end of production year: 2019.\nThis attribute value pair will be converted to the end of the produc-\ntion year of Volkswagen Beetles, which is 2019. These sentences,\neach representing one attribute of an entity, are further selected\nusing rerank models, e.g., BGE-reranker-v2-m3.\nThrough experiment, we find that the potential of the QA system\ncan be boosted by 20% using this technique. We successfully inte-\ngrated this technique into our hallucination control module, and\nthe overall performance is slightly boosted for the plant category.\n3.2\nRetrieval Pipeline for Text-indexed Web\nSource.\nIn Task 2, a text-indexed web page source is provided as an infor-\nmation source. Specifically, the chunked web page content is linked\nwith a text embedding index. The difficulty in this task lies in how\nto rewrite the query into a merged text query that has the same\nmodality as the web page text, which can be retrieved through the\ntext embedding. For example, for the query, When does this car stop\nproduction?, and the query image containing a Volkswagen Beetle,\nwe have to rewrite the original query to When does the Volkswagen\nBeetle stop production? We have to substitute the pronouns in the\noriginal query with information from the query image and rewrite a\nmerged query that can be answered individually without the query\nimage.\nSFT tuning for Merge Query Rewrite. We can prompt VLMs to\ngenerate a merged query. We use the rewrite prompt to rewrite a\nmerged query:\n1 ###Task You are an expert at converting visual questions into\n‚Ü©‚Üíeffective search queries.\n2 Your goal is to create a comprehensive search query that will\n‚Ü©‚Üíhelp find the most relevant information.\n3 For each image-based question, you must create a search query\n‚Ü©‚Üíthat combines:\n4 1. Key visual elements from the image (objects, text, logos,\n‚Ü©‚Üíscenes, actions, etc.)\n5 2. The core question being asked\n6 3. Potential answer terms or relevant context\n7 For example:\n8 - If asking about a logo: include company name, industry, and\n‚Ü©‚Üívisual description\n9 - If asking about an object: include its appearance, category,\n‚Ü©‚Üíand possible brands/models\n10 - If asking about an event/scene: include location hints,\n‚Ü©‚Üíactivities, and time period clues\n11 '''\n12 Image:{}\n",
      "text_clean": "1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† Ram 2500 Entity prompt Image API Llama Entity match Ram 2500 {Kia Telluride,‚Ä¶} {Kia Telluride,‚Ä¶} {GMC Terrain,‚Ä¶} Candidate entity information What is the horsepower of this truck? Figure 4: Retrieving and Reranking through Text. 2 If the entity is about a plant, you should output the full ‚Ü©‚Üíbotanical name. 3 .... 4 If the enityt is about a vehicle, you should output the vehicle ‚Ü©‚Üí's brand and model. 5 Query: {} 6 You should output the entity's name directly. The prompt consists of detailed entity name instructions that match the entities‚Äô names in the knowledge graph. Similarly, prompting strong VLMs, e.g., GPT-4o, can extract more correct entity names compared to prompting Llama 3.2-VL. On one hand, GPT-4o possesses more internal knowledge. On the other hand, GPT-4o can follow the instructions better. We also tune Llama 3.2-VL using the ground truth label name to learn the format and follow the instructions better. (The ground truth label is obtained by similar prompts with the ground truth answer. As the ground truth label usually appears in the ground truth answer, most of the GPT-4o extraction results are identical to the ground truth entity name.) In the second step, we use the retrieving-through-image method to extract a large number of candidates (e.g., 1000), and the generated entity name is compared with the entity names of the candidates. The entity names are tokenized and stemmed, and only if the processed token set is identical or contains the other are the two entity names considered related. Through experiment, we find that using GPT-4o, 40% of the entity name can be extracted correctly. The ratio is 20% for Llama 3.2-VL. The potential of the QA system can be boosted by 10% using the GPT-4o result, and the result is 5% for the Llama 3.2-VL results. Unfortunately, we didn‚Äôt have enough time to train a model that performed well in all domains. As a result, we could only apply this solution in the field of plants. Notably, even powerful VLMs like GPT-4o can only identify 40% of the entity‚Äôs desired names. This indicates that for the hard cases like vehicles, plants, and food, state-of-the-art VLMs still can‚Äôt recall their names, as different models of cars or different plants in the same class look quite similar. During the contest, we try to search for open-source models to identify a car‚Äôs brand and model or plants‚Äô botanical names. However, it turns out that there are no such open-source models, and a system that can deal with these needs requires a highly sophisticated database building and model training. Query using merged text query. A merged text query is a pure text query that contains all the query information in the original text query and the query image. We will propose the full merged text query rewrite process in Sec. 3.2 in detail. Suppose we have the merged query here. We follow the same steps of retrieving a large number of candidates (e.g., 1000), and we convert the structured What is the horsepower of this truck? What is the horsepower of Ram 2500? Rewrite prompt Web API Llama The New Ram 2500 And Its Incredible The 2021 Ram 2500 has a 6.4-liter V-8 engine that generates 410 horsepower‚Ä¶ Candidate entity information What is the horsepower of Ram 2500? Web retrieval chunks What is the horsepower of Ram 2500? TOP K chunks Reranker Figure 5: Retrieval Pipeline for Text-indexed Web Source. text information of the candidates to a text embedding retrieval base. Specifically, the attribute-value pair in the entity‚Äôs structured text is converted into separate text outputs. For example, the Volkswagen Beetle entity has an attribute, end of production year: 2019. This attribute value pair will be converted to the end of the production year of Volkswagen Beetles, which is 2019. These sentences, each representing one attribute of an entity, are further selected using rerank models, e.g., BGE-reranker-v2-m3. Through experiment, we find that the potential of the QA system can be boosted by 20% using this technique. We successfully integrated this technique into our hallucination control module, and the overall performance is slightly boosted for the plant category. 3.2 Retrieval Pipeline for Text-indexed Web Source. In Task 2, a text-indexed web page source is provided as an information source. Specifically, the chunked web page content is linked with a text embedding index. The difficulty in this task lies in how to rewrite the query into a merged text query that has the same modality as the web page text, which can be retrieved through the text embedding. For example, for the query, When does this car stop production?, and the query image containing a Volkswagen Beetle, we have to rewrite the original query to When does the Volkswagen Beetle stop production? We have to substitute the pronouns in the original query with information from the query image and rewrite a merged query that can be answered individually without the query image. SFT tuning for Merge Query Rewrite. We can prompt VLMs to generate a merged query. We use the rewrite prompt to rewrite a merged query: 1 ###Task You are an expert at converting visual questions into ‚Ü©‚Üíeffective search queries. 2 Your goal is to create a comprehensive search query that will ‚Ü©‚Üíhelp find the most relevant information. 3 For each image-based question, you must create a search query ‚Ü©‚Üíthat combines: 4 1. Key visual elements from the image (objects, text, logos, ‚Ü©‚Üíscenes, actions, etc.) 5 2. The core question being asked 6 3. Potential answer terms or relevant context 7 For example: 8 - If asking about a logo: include company name, industry, and ‚Ü©‚Üívisual description 9 - If asking about an object: include its appearance, category, ‚Ü©‚Üíand possible brands/models 10 - If asking about an event/scene: include location hints, ‚Ü©‚Üíactivities, and time period clues 11 ''' 12 Image:{}",
      "figure_captions": [
        {
          "bbox": [
            73.59700012207031,
            159.9904022216797,
            274.2471008300781,
            168.95680236816406
          ],
          "text": "Figure 4: Retrieving and Reranking through Text."
        },
        {
          "bbox": [
            322.0400085449219,
            159.9794158935547,
            554.1176147460938,
            168.94581604003906
          ],
          "text": "Figure 5: Retrieval Pipeline for Text-indexed Web Source."
        }
      ],
      "images": [
        {
          "image_id": "p4_cluster_001",
          "page": 4,
          "file_path": "images/page_0004_cluster_001.png",
          "bbox": [
            113.67565155029297,
            108.67473602294922,
            131.2359161376953,
            119.21359252929688
          ],
          "caption": "a robot with a smiley face on it's face",
          "detailed_caption": "The image shows a white and pink robot with a smiley face on its face, giving it a cheerful and inviting look. Its eyes are wide open and its mouth is slightly open, as if it is about to say something. Its body is round and its antennae are long and thin. Its hands are clasped together in front of it, and its head is slightly tilted to the side.",
          "ocr_text": "C",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>C</s>",
            "parsed": {
              "<OCR>": "C"
            }
          },
          "paper_caption": "Figure 5: Retrieval Pipeline for Text-indexed Web Source.",
          "paper_caption_bbox": [
            322.0400085449219,
            159.9794158935547,
            554.1176147460938,
            168.94581604003906
          ]
        },
        {
          "image_id": "p4_cluster_002",
          "page": 4,
          "file_path": "images/page_0004_cluster_002.png",
          "bbox": [
            150.117919921875,
            82.95745086669922,
            197.54949951171875,
            144.04071044921875
          ],
          "caption": "A picture of a car with an image API on it.",
          "detailed_caption": "The image shows a white car parked in a parking lot with the words \"Image API\" written above it. The car is surrounded by other cars on the road, giving the impression of a busy city street.",
          "ocr_text": "Image API",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>Image API</s>",
            "parsed": {
              "<OCR>": "Image API"
            }
          },
          "paper_caption": "Figure 5: Retrieval Pipeline for Text-indexed Web Source.",
          "paper_caption_bbox": [
            322.0400085449219,
            159.9794158935547,
            554.1176147460938,
            168.94581604003906
          ]
        },
        {
          "image_id": "p4_cluster_003",
          "page": 4,
          "file_path": "images/page_0004_cluster_003.png",
          "bbox": [
            57.40729522705078,
            94.62651062011719,
            96.39862823486328,
            112.62105560302734
          ],
          "caption": "a car is parked in a parking lot at night",
          "detailed_caption": "The image shows a car parked in a parking lot at night, with the background blurred out. The car is in focus, while the background is slightly blurred, giving the image a dreamy quality.",
          "ocr_text": "1",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>1</s>",
            "parsed": {
              "<OCR>": "1"
            }
          },
          "paper_caption": "Figure 5: Retrieval Pipeline for Text-indexed Web Source.",
          "paper_caption_bbox": [
            322.0400085449219,
            159.9794158935547,
            554.1176147460938,
            168.94581604003906
          ]
        }
      ]
    },
    {
      "page": 5,
      "text_raw": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25\n13 Query: {}\nWe first prompt the powerful GPT-4o to generate the merged\nquery rewrite. As we have mentioned before, even GPT-4o can\nonly identify half of the entities in this contest. Moreover, the\ninstructions we make in the prompt are still not 100% followed by\nGPT-4o. This is worse while using Llama 3.2-VL for rewriting. Here\nwe list the potentially correct answering case in Task 2 using the\nrewrite system. As we can see, the GPT-4o rewrite system can boost\nanswer potential by Àú13%, while the Llama 3.2-VL rewrite can only\nboost it by 7%.\nTable 1: Comparison of Different Rewriting Methods\nMethod\nScore (%)\nOri\n‚àº33\nLlama Rewrite\n‚àº40\nGPT-4o Rewrite\n‚àº46\nLlama-distill Rewrite\n‚àº44\nGPT-4o Cheat Rewrite\n‚àº60\nLlama-distill-cheat Rewrite\n‚àº44\nTo solve the difficulty of entity extraction, we introduce a cheated\nversion of rewrite. In the prompt we listed above, we additionally\nadd the ground truth answer. As the ground truth entity mostly\nappears in the ground truth answer, providing the ground truth\nanswer can guide the VLM to include the ground truth entity in\nthe rewrite query. As we can see, the cheated version achieves 60%\nof the potential correct score.\nWe tune the Llama 3.2-VL base model for better rewrite perfor-\nmance. We select both the cheated and non-cheated rewrite queries\nas the training sample for Llama 3.2-VL. The results are also pre-\nsented in Table 1. As we can see, regardless of using the cheated\nor non-cheated version, the boost of potential performance is Àú4%.\nThe performance is close to the GPT-4o rewrite performance ( 3%\ndown); therefore, the result is rather satisfying. We choose one of\nthe checkpoints in the Llama-distill checkpoints as the final rewrite\nmodule we use in the submitted version.\nRL Tuning for Merge Query Rewrite. During the contest, we\nalso try using RL to tune this rewrite module. We try two different\napproaches, the DPO approach and the RL (GRPO) approach.\nDPO approach. [5] We follow the normal procedure of DPO\ntraining. We construct the training data as follows. Following the\nbest SFT checkpoint we obtain in the last subsection, we sample 5\nrewrite merge queries using high temperature. The better pairs in\nDPO are the rewrite queries that can produce the context, making\nthe QA result correct, and the worse pairs in DPO are the rewrite\nqueries that produce the context, making the QA result incorrect.\nRL (GRPO) approach. [6] We use the GRPO algorithm in RL\ntraining for query rewrite. Similarly, the merge query is rewarded\nif the context helps answer correctly. Conversely, the merge query\nis punished if the context makes the answer wrong.\nOutcome. As we have mentioned, due to the lack of sufficient\ninternal knowledge, the space for further improvement through\nRL is limited. The gap between tuned Llama and GPT-4o is only\n3%. In our experiment, we fail to obtain a better rewrite checkpoint\nQ1: what is the name of the radio station playing on the radio?\nQ2: what kind of a station is that?\nQ3: where do they broadcast from?\nA1: the station playing on the car radio is nj 101.5.\nA2: nj 101.5 is an fm station that plays talk radio on weekdays \nand classic hits on weekends.\nA3: nj 101.5 broadcasts from trenton.\nContext for Q2\nContext for Q3\nFigure 6: One-step Context for Multi-Round QA.\nthan the original SFT-tuned checkpoint. Therefore, our submitted\nversion is based on SFT-tuning.\nRetrieval through Text. For simplicity, we directly use the pre-\nprocessed web content instead of the original HTML content. Af-\nter obtaining the rewritten merged query, we retrieve the text\nchunks using the bge-large-en-v1.5 index. Conventionally, we fur-\nther use BGE-reranker-v2-m3 to sort the candidate chunks more\naccurately [1].\n3.3\nRetrieval Pipeline for Multi-turn\nConversation.\nIn Task 3, the information source is the same as the source in Task\n2. The difference is the introduction of a multi-turn conversation.\nIn this subsection, we present our retrieval pipeline for multi-turn\nconversation.\nOne-step Context. Though some queries may require longer con-\ntext, we observe in our experiment that, provided with the ground-\ntruth QA, the QA performance of using one-step context is almost\nthe same as the result of using full context. Therefore, for the sim-\nplicity of sampling and training, we only use the last-step context\nin the contest.\nMerge Query Rewrite with Context. Since many of the queries\nrequire context QAs to specify the entity the query is about, we add\nthe query QAs into the merge query rewrite prompt. The prompt\nwe use is as follows:\n1 ###Task You are an expert at converting visual questions into\n‚Ü©‚Üíeffective search queries.\n2 The current query is a part of multi-turn conversation. You\n‚Ü©‚Üíshould use the history conversation to make sure what the\n‚Ü©‚Üícurrent query is about.\n3 Your goal is to create a comprehensive search query that will\n‚Ü©‚Üíhelp find the most relevant information for the currecnt\n‚Ü©‚Üíquery.\n4 For each image-based question, you must create a search query\n‚Ü©‚Üíthat combines:\n5 1. Key visual elements from the image (objects, text, logos,\n‚Ü©‚Üíscenes, actions, etc.)\n6 2. The core current question being asked\n7 3. Potential answer terms or relevant context\n8 For example:\n9 - If asking about a logo: include company name, industry, and\n‚Ü©‚Üívisual description\n10 - If asking about an object: include its appearance, category,\n‚Ü©‚Üíand possible brands/models\n11 - If asking about an event/scene: include location hints,\n‚Ü©‚Üíactivities, and time period clues\n12 Query: {}\n13 Context: {}\n",
      "text_clean": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 13 Query: {} We first prompt the powerful GPT-4o to generate the merged query rewrite. As we have mentioned before, even GPT-4o can only identify half of the entities in this contest. Moreover, the instructions we make in the prompt are still not 100% followed by GPT-4o. This is worse while using Llama 3.2-VL for rewriting. Here we list the potentially correct answering case in Task 2 using the rewrite system. As we can see, the GPT-4o rewrite system can boost answer potential by Àú13%, while the Llama 3.2-VL rewrite can only boost it by 7%. Table 1: Comparison of Different Rewriting Methods Method Score (%) Ori ‚àº33 Llama Rewrite ‚àº40 GPT-4o Rewrite ‚àº46 Llama-distill Rewrite ‚àº44 GPT-4o Cheat Rewrite ‚àº60 Llama-distill-cheat Rewrite ‚àº44 To solve the difficulty of entity extraction, we introduce a cheated version of rewrite. In the prompt we listed above, we additionally add the ground truth answer. As the ground truth entity mostly appears in the ground truth answer, providing the ground truth answer can guide the VLM to include the ground truth entity in the rewrite query. As we can see, the cheated version achieves 60% of the potential correct score. We tune the Llama 3.2-VL base model for better rewrite performance. We select both the cheated and non-cheated rewrite queries as the training sample for Llama 3.2-VL. The results are also presented in Table 1. As we can see, regardless of using the cheated or non-cheated version, the boost of potential performance is Àú4%. The performance is close to the GPT-4o rewrite performance ( 3% down); therefore, the result is rather satisfying. We choose one of the checkpoints in the Llama-distill checkpoints as the final rewrite module we use in the submitted version. RL Tuning for Merge Query Rewrite. During the contest, we also try using RL to tune this rewrite module. We try two different approaches, the DPO approach and the RL (GRPO) approach. DPO approach. [5] We follow the normal procedure of DPO training. We construct the training data as follows. Following the best SFT checkpoint we obtain in the last subsection, we sample 5 rewrite merge queries using high temperature. The better pairs in DPO are the rewrite queries that can produce the context, making the QA result correct, and the worse pairs in DPO are the rewrite queries that produce the context, making the QA result incorrect. RL (GRPO) approach. [6] We use the GRPO algorithm in RL training for query rewrite. Similarly, the merge query is rewarded if the context helps answer correctly. Conversely, the merge query is punished if the context makes the answer wrong. Outcome. As we have mentioned, due to the lack of sufficient internal knowledge, the space for further improvement through RL is limited. The gap between tuned Llama and GPT-4o is only 3%. In our experiment, we fail to obtain a better rewrite checkpoint Q1: what is the name of the radio station playing on the radio? Q2: what kind of a station is that? Q3: where do they broadcast from? A1: the station playing on the car radio is nj 101.5. A2: nj 101.5 is an fm station that plays talk radio on weekdays and classic hits on weekends. A3: nj 101.5 broadcasts from trenton. Context for Q2 Context for Q3 Figure 6: One-step Context for Multi-Round QA. than the original SFT-tuned checkpoint. Therefore, our submitted version is based on SFT-tuning. Retrieval through Text. For simplicity, we directly use the preprocessed web content instead of the original HTML content. After obtaining the rewritten merged query, we retrieve the text chunks using the bge-large-en-v1.5 index. Conventionally, we further use BGE-reranker-v2-m3 to sort the candidate chunks more accurately [1]. 3.3 Retrieval Pipeline for Multi-turn Conversation. In Task 3, the information source is the same as the source in Task 2. The difference is the introduction of a multi-turn conversation. In this subsection, we present our retrieval pipeline for multi-turn conversation. One-step Context. Though some queries may require longer context, we observe in our experiment that, provided with the groundtruth QA, the QA performance of using one-step context is almost the same as the result of using full context. Therefore, for the simplicity of sampling and training, we only use the last-step context in the contest. Merge Query Rewrite with Context. Since many of the queries require context QAs to specify the entity the query is about, we add the query QAs into the merge query rewrite prompt. The prompt we use is as follows: 1 ###Task You are an expert at converting visual questions into ‚Ü©‚Üíeffective search queries. 2 The current query is a part of multi-turn conversation. You ‚Ü©‚Üíshould use the history conversation to make sure what the ‚Ü©‚Üícurrent query is about. 3 Your goal is to create a comprehensive search query that will ‚Ü©‚Üíhelp find the most relevant information for the currecnt ‚Ü©‚Üíquery. 4 For each image-based question, you must create a search query ‚Ü©‚Üíthat combines: 5 1. Key visual elements from the image (objects, text, logos, ‚Ü©‚Üíscenes, actions, etc.) 6 2. The core current question being asked 7 3. Potential answer terms or relevant context 8 For example: 9 - If asking about a logo: include company name, industry, and ‚Ü©‚Üívisual description 10 - If asking about an object: include its appearance, category, ‚Ü©‚Üíand possible brands/models 11 - If asking about an event/scene: include location hints, ‚Ü©‚Üíactivities, and time period clues 12 Query: {} 13 Context: {}",
      "figure_captions": [
        {
          "bbox": [
            340.33599853515625,
            188.3193817138672,
            535.821533203125,
            197.28578186035156
          ],
          "text": "Figure 6: One-step Context for Multi-Round QA."
        }
      ],
      "images": [
        {
          "image_id": "p5_cluster_001",
          "page": 5,
          "file_path": "images/page_0005_cluster_001.png",
          "bbox": [
            317.5993957519531,
            85.70596313476562,
            381.3778076171875,
            170.72850036621094
          ],
          "caption": "A picture of a car dashboard with a dog in the background.",
          "detailed_caption": "The image shows the interior of a car with a steering wheel, dashboard, music player, side mirror, and glass windows. Through the windows, we can see trees, grass, a toy, a house, and the sky.",
          "ocr_text": "-",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>-</s>",
            "parsed": {
              "<OCR>": "-"
            }
          },
          "paper_caption": "Figure 6: One-step Context for Multi-Round QA.",
          "paper_caption_bbox": [
            340.33599853515625,
            188.3193817138672,
            535.821533203125,
            197.28578186035156
          ]
        }
      ]
    },
    {
      "page": 6,
      "text_raw": "1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä†\nWhat‚Äôs the answer?\nSolve_math(11*7)\nLlama\nMath tool prompt\nCalculate expression\n11 * 7 = 77\nSolve_math(11*7)\nResponse prompt\n11 * 7 = 77\nLlama\n11 times 7 is 77\nFigure 7: Solve Math Problems Using Tools.\nThe tuning process is the same as the process without a multi-\nturn conversation.\nSampling Strategy. To achieve the best final score, a hallucination\ncontrol component has to be employed. However, in merge query\nrewrite training, we need to deploy the control component, as\nsufficient QA information has to be included in context.\n3.4\nOCR from the Query Image.\nThrough observation, we find that many of the queries are directly\nabout the text in the image. Therefore, we suspect employing a\nseparate OCR module can boost the performance for queries of this\nkind. Through experiment, we find that many more queries can be\nanswered correctly using OCR results from GPT-4o.\nHowever, we find that using VLM models to extract text has a se-\nvere drawback. A large number of queries involve a large amount of\ntext from a book. Conducting OCR using VLMs requires generating\nmany tokens, making this approach unavailable in this contest.\nWe also try using OCR tools like PaddleOCR [2]. However, we\nfind that the extraction results are not as satisfying as using VLMs,\nparticularly in the ego-centric images. Considering all the difficul-\nties, we do not include an OCR component in our pipeline. However,\nwe believe this component can be useful, as the base VLM normally\ncan‚Äôt focus on too much text in the image.\n3.5\nSolve Math Problems Using Tools.\nMath problems are always tricky for LLMs/VLMs, and using tools\nis a standard solution for this. Therefore, we employ using tools\nto solve the math problems. Through observation of the contest\ndata, we conclude there are three types of major problems in math:\ncalculation and simplification of numbers and variables, base con-\nversion, and chemical formula balancing. We construct tools for\nthese three types of queries, and we provide APIs for these tools.\nWe prompt the VLM to follow the provided tool API. To solve the\ninstruction-following problem, we construct Àú50 tool examples and\ntune the base VLM. The final version of our math module can solve\nmost of the math problems correctly in the training data of the\ncontest.\n4\nHallucination Control Component of the\nSolution\nLLMs/VLMs inevitably encounter queries that fall outside their\nreliable knowledge scope. During the contest, we are awarded 1\npoint for answering correctly and penalized 1 point for answering\nincorrectly; therefore, we have to control hallucinations to achieve\nhigher scores.\nThe hallucination control component therefore, has two equally\nimportant goals:\n(1) Maximize the proportion of correct answers.\n(2) Minimize the proportion of wrong answers by refus-\ning when necessary.\n4.1\nAnswerability Estimation\nWhether a query is answerable by the current retrieval and answer-\ning system can be determined by the correctness of its generated\nanswers. We denote the queries that are hard for the current system\nto produce a correct answer as unanswerable.\n4.2\nRefusal Training Pipeline\nSupervised Fine-Tuning (SFT)\n‚Ä¢ Unanswerable queries: label ‚ÜíI don‚Äôt know.\n‚Ä¢ Answerable queries: ground-truth reference answers\nDirect Preference Optimisation (DPO) We build pairwise pref-\nerences (better, worse):\n‚Ä¢ Answerable queries: better = correct answer; worse = I\ndon‚Äôt know.\n‚Ä¢ Unanswerable queries: better = I don‚Äôt know; worse =\nhallucinated answer.\nReinforcement Learning (GRPO) We adopt GRPO because it\neliminates the value-function critic, reducing instability, yet the\nreward definition applies equally to PPO:\nùëü=\nÔ£±Ô£¥Ô£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£≥\n+ùëò,\ncorrect answer;\n0,\nI don‚Äôt know;\n‚àí1,\nincorrect answer.\nWith ùëò=1, the expected return is identical to the Refusal Score.\nWe obtain different checkpoints using different queries for fur-\nther selection and ensemble.\n5\nCheckpoint Ensemble Trick\nSince the hallucination control process is tricky and highly unsta-\nble, selecting and combining the best checkpoints became quite\nimportant. In this section, we conclude the tricks we employ in this\ncontest.\nCheckpoint Candidate Pool. We collect the checkpoint results in\nmany training trials under different settings to form a checkpoint\ncandidate pool. The checkpoints with poorer results are neglected\nin the pool. All the results on the validation set are recorded so that\nfurther processes in the candidate pool do not require re-evaluation.\nCheckpoint Ensemble and Selection. There are many ways of\nensembling checkpoints. We list some of them:\nEnsemble according to Domain. Since the difficulty of dif-\nferent domains varies a lot, we can control the QA checkpoints or\neven block answering according to the domain information. The\nstrategy here can be selecting the best checkpoints on each domain,\nand if no checkpoint can achieve positive scores on this domain,\nwe block the answers on this domain directly.\nEnsemble according to Equivalence. We conduct equivalence\nclustering on all the answers of different checkpoints, and we select\nthe answers supported by the most checkpoints. We can enumerate\n",
      "text_clean": "1Yikuan Xia*, 1Jiazun Chen*, 1Yirui Zhan 1Suifeng Zhao and 2Weipeng Jiang, 2Chaorui Zhang, 2Wei Han, 2Bo Bai‚Ä†, 1Jun Gao‚Ä† What‚Äôs the answer? Solve_math(11*7) Llama Math tool prompt Calculate expression 11 * 7 = 77 Solve_math(11*7) Response prompt 11 * 7 = 77 Llama 11 times 7 is 77 Figure 7: Solve Math Problems Using Tools. The tuning process is the same as the process without a multiturn conversation. Sampling Strategy. To achieve the best final score, a hallucination control component has to be employed. However, in merge query rewrite training, we need to deploy the control component, as sufficient QA information has to be included in context. 3.4 OCR from the Query Image. Through observation, we find that many of the queries are directly about the text in the image. Therefore, we suspect employing a separate OCR module can boost the performance for queries of this kind. Through experiment, we find that many more queries can be answered correctly using OCR results from GPT-4o. However, we find that using VLM models to extract text has a severe drawback. A large number of queries involve a large amount of text from a book. Conducting OCR using VLMs requires generating many tokens, making this approach unavailable in this contest. We also try using OCR tools like PaddleOCR [2]. However, we find that the extraction results are not as satisfying as using VLMs, particularly in the ego-centric images. Considering all the difficulties, we do not include an OCR component in our pipeline. However, we believe this component can be useful, as the base VLM normally can‚Äôt focus on too much text in the image. 3.5 Solve Math Problems Using Tools. Math problems are always tricky for LLMs/VLMs, and using tools is a standard solution for this. Therefore, we employ using tools to solve the math problems. Through observation of the contest data, we conclude there are three types of major problems in math: calculation and simplification of numbers and variables, base conversion, and chemical formula balancing. We construct tools for these three types of queries, and we provide APIs for these tools. We prompt the VLM to follow the provided tool API. To solve the instruction-following problem, we construct Àú50 tool examples and tune the base VLM. The final version of our math module can solve most of the math problems correctly in the training data of the contest. 4 Hallucination Control Component of the Solution LLMs/VLMs inevitably encounter queries that fall outside their reliable knowledge scope. During the contest, we are awarded 1 point for answering correctly and penalized 1 point for answering incorrectly; therefore, we have to control hallucinations to achieve higher scores. The hallucination control component therefore, has two equally important goals: (1) Maximize the proportion of correct answers. (2) Minimize the proportion of wrong answers by refusing when necessary. 4.1 Answerability Estimation Whether a query is answerable by the current retrieval and answering system can be determined by the correctness of its generated answers. We denote the queries that are hard for the current system to produce a correct answer as unanswerable. 4.2 Refusal Training Pipeline Supervised Fine-Tuning (SFT) ‚Ä¢ Unanswerable queries: label ‚ÜíI don‚Äôt know. ‚Ä¢ Answerable queries: ground-truth reference answers Direct Preference Optimisation (DPO) We build pairwise preferences (better, worse): ‚Ä¢ Answerable queries: better = correct answer; worse = I don‚Äôt know. ‚Ä¢ Unanswerable queries: better = I don‚Äôt know; worse = hallucinated answer. Reinforcement Learning (GRPO) We adopt GRPO because it eliminates the value-function critic, reducing instability, yet the reward definition applies equally to PPO: ùëü= Ô£±Ô£¥Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£¥Ô£≥ +ùëò, correct answer; 0, I don‚Äôt know; ‚àí1, incorrect answer. With ùëò=1, the expected return is identical to the Refusal Score. We obtain different checkpoints using different queries for further selection and ensemble. 5 Checkpoint Ensemble Trick Since the hallucination control process is tricky and highly unstable, selecting and combining the best checkpoints became quite important. In this section, we conclude the tricks we employ in this contest. Checkpoint Candidate Pool. We collect the checkpoint results in many training trials under different settings to form a checkpoint candidate pool. The checkpoints with poorer results are neglected in the pool. All the results on the validation set are recorded so that further processes in the candidate pool do not require re-evaluation. Checkpoint Ensemble and Selection. There are many ways of ensembling checkpoints. We list some of them: Ensemble according to Domain. Since the difficulty of different domains varies a lot, we can control the QA checkpoints or even block answering according to the domain information. The strategy here can be selecting the best checkpoints on each domain, and if no checkpoint can achieve positive scores on this domain, we block the answers on this domain directly. Ensemble according to Equivalence. We conduct equivalence clustering on all the answers of different checkpoints, and we select the answers supported by the most checkpoints. We can enumerate",
      "figure_captions": [
        {
          "bbox": [
            86.4010009765625,
            162.27940368652344,
            261.4430847167969,
            171.2458038330078
          ],
          "text": "Figure 7: Solve Math Problems Using Tools."
        }
      ],
      "images": [
        {
          "image_id": "p6_cluster_001",
          "page": 6,
          "file_path": "images/page_0006_cluster_001.png",
          "bbox": [
            114.58206176757812,
            108.62682342529297,
            131.69033813476562,
            118.89442443847656
          ],
          "caption": "a robot with a smiley face on it's face",
          "detailed_caption": "The image shows a white and pink robot with a smiley face on its face, giving it a cheerful and inviting look. Its eyes are wide open and its mouth is slightly open, as if it is about to say something. Its body is round and its antennae are long and thin. Its hands are clasped together in front of it, and its head is slightly tilted to the side.",
          "ocr_text": "C",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>C</s>",
            "parsed": {
              "<OCR>": "C"
            }
          },
          "paper_caption": "Figure 7: Solve Math Problems Using Tools.",
          "paper_caption_bbox": [
            86.4010009765625,
            162.27940368652344,
            261.4430847167969,
            171.2458038330078
          ]
        },
        {
          "image_id": "p6_cluster_002",
          "page": 6,
          "file_path": "images/page_0006_cluster_002.png",
          "bbox": [
            56.06438446044922,
            85.0799331665039,
            95.12016296386719,
            137.09046936035156
          ],
          "caption": "A person's hand is pressing a button on a wall.",
          "detailed_caption": "The image shows a person's hand holding a piece of paper in front of a wall with a switchboard attached to it.",
          "ocr_text": "=",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>=</s>",
            "parsed": {
              "<OCR>": "="
            }
          },
          "paper_caption": "Figure 7: Solve Math Problems Using Tools.",
          "paper_caption_bbox": [
            86.4010009765625,
            162.27940368652344,
            261.4430847167969,
            171.2458038330078
          ]
        }
      ]
    },
    {
      "page": 7,
      "text_raw": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25\nthe checkpoints in the candidate pool to get the optimal checkpoint\nsubset.\nMixed Ensemble of Domain and Equivalence. We can apply\na mixed strategy, which means enumerating checkpoint combina-\ntions on each domain and selecting different optimal checkpoint\ncombinations for each domain.\nPros and Cons. Pros: The ensemble process helps us boost plenty\nof performance on the final score.\nCons: Since many checkpoints are involved, it requires multiple\nrounds of inference during the test, making it hard to control the\ntime limit. Unfortunately, many strategies that work well locally\nfail to work online, even if we manage to control the time limit\ncarefully. This potentially harms the final score of the solution.\nAnother major drawback is that conducting such a large-scale\nselection in the checkpoints creates a large gap between the local\nevaluation results and the online evaluation results. Our enumer-\nation results show huge improvements locally, while it turns out\nto be heavily overfitting. Our final version also differs from the\ncomplex combinations for each domain, because the less selection\nis made, the less overfitting is observed.\n6\nConclusion\nThis paper presented db3‚Äôs comprehensive solution for the Meta\nCRAG-MM Challenge 2025, which secured top rankings across all\ntasks and won the grand prize for ego-centric queries. Our key\ninnovations include:\n1. Domain-Adaptive Retrieval: We developed specialized pipelines\nfor different data modalities, most importantly, merged query rewrit-\ning for text-based retrieval, significantly improving context rele-\nvance.\n2. Hallucination Control: Through multi-stage training (SFT,\nDPO, RL) with refusal optimization, we created models that reliably\noutput \"I don‚Äôt know\" for unanswerable queries while maximizing\ncorrect responses.\nDespite our success, limitations remain in fine-grained entity\nrecognition and OCR integration for text-heavy images. Future\nwork should explore dedicated recognition models and optimized\nOCR-VLM pipelines. Our solution demonstrates that combining\ntask-specific retrieval with rigorous hallucination control is essen-\ntial for reliable multi-modal QA systems, particularly for challeng-\ning ego-centric scenarios.\n7\nAcknowledgement\nWe extend our sincere gratitude to the experts from Huawei‚Äôs The-\nory Lab at the Central Research Institute, 2012 Labs, for their invalu-\nable technical discussions and suggestions throughout this project.\nThis project is funded by NSFC (No. 62272008).\nReferences\n[1] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.\nBGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text\nEmbeddings Through Self-Knowledge Distillation. arXiv:2402.03216 [cs.CL]\n[2] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu,\nXueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu\nLv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and\nYanjun Ma. 2025. PaddleOCR 3.0 Technical Report. arXiv:2507.05595 [cs.CV]\nhttps://arxiv.org/abs/2507.05595\n[3] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang,\nChunyuan Li, Jianwei Yang, Hang Su, et al. 2024. Grounding dino: Marrying dino\nwith grounded pre-training for open-set object detection. In European conference\non computer vision. Springer, 38‚Äì55.\n[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n2021. Learning transferable visual models from natural language supervision. In\nInternational conference on machine learning. PmLR, 8748‚Äì8763.\n[5] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. 2023. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in neural information processing\nsystems 36 (2023), 53728‚Äì53741.\n[6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei\nZhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing\nthe limits of mathematical reasoning in open language models. arXiv preprint\narXiv:2402.03300 (2024).\nA\nLoRA and FineTuning Hyperparameters.\nThe LoRA and Finetuning hyperparameters used in tuning the base\nmodel and API generation module is listed in Tab. 2:\nTable 2: LoRA and FineTuning Hyperparameters.\nName\nValue\nLoRA_alpha\n16\nLoRA_dropout\n0.1\nLoRA_r\n8\ntarget_modules\n[\"k_proj\", \"q_proj\", \"v_proj\",\n\"up_proj\", \"down_proj\", \"gate_proj\"]\nbias\n\"none\"\n4-bit\nTrue\nmax_seq_length\n2048/4096\nper_device_train_batch_size\n1\ngradient_accumulation_steps\n4\noptim\n\"adamw_hf\"\nlearning_rate\n2e-4\nmax_grad_norm\n0.3\nscheduler\n\"cosine\", warm_up_ratio=0.1\n",
      "text_clean": "DB3 Team‚Äôs Solution For Meta KDD Cup‚Äô 25 the checkpoints in the candidate pool to get the optimal checkpoint subset. Mixed Ensemble of Domain and Equivalence. We can apply a mixed strategy, which means enumerating checkpoint combinations on each domain and selecting different optimal checkpoint combinations for each domain. Pros and Cons. Pros: The ensemble process helps us boost plenty of performance on the final score. Cons: Since many checkpoints are involved, it requires multiple rounds of inference during the test, making it hard to control the time limit. Unfortunately, many strategies that work well locally fail to work online, even if we manage to control the time limit carefully. This potentially harms the final score of the solution. Another major drawback is that conducting such a large-scale selection in the checkpoints creates a large gap between the local evaluation results and the online evaluation results. Our enumeration results show huge improvements locally, while it turns out to be heavily overfitting. Our final version also differs from the complex combinations for each domain, because the less selection is made, the less overfitting is observed. 6 Conclusion This paper presented db3‚Äôs comprehensive solution for the Meta CRAG-MM Challenge 2025, which secured top rankings across all tasks and won the grand prize for ego-centric queries. Our key innovations include: 1. Domain-Adaptive Retrieval: We developed specialized pipelines for different data modalities, most importantly, merged query rewriting for text-based retrieval, significantly improving context relevance. 2. Hallucination Control: Through multi-stage training (SFT, DPO, RL) with refusal optimization, we created models that reliably output \"I don‚Äôt know\" for unanswerable queries while maximizing correct responses. Despite our success, limitations remain in fine-grained entity recognition and OCR integration for text-heavy images. Future work should explore dedicated recognition models and optimized OCR-VLM pipelines. Our solution demonstrates that combining task-specific retrieval with rigorous hallucination control is essential for reliable multi-modal QA systems, particularly for challenging ego-centric scenarios. 7 Acknowledgement We extend our sincere gratitude to the experts from Huawei‚Äôs Theory Lab at the Central Research Institute, 2012 Labs, for their invaluable technical discussions and suggestions throughout this project. This project is funded by NSFC (No. 62272008). References [1] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2402.03216 [cs.CL] [2] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. 2025. PaddleOCR 3.0 Technical Report. arXiv:2507.05595 [cs.CV] https://arxiv.org/abs/2507.05595 [3] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. 2024. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision. Springer, 38‚Äì55. [4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 8748‚Äì8763. [5] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. Advances in neural information processing systems 36 (2023), 53728‚Äì53741. [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). A LoRA and FineTuning Hyperparameters. The LoRA and Finetuning hyperparameters used in tuning the base model and API generation module is listed in Tab. 2: Table 2: LoRA and FineTuning Hyperparameters. Name Value LoRA_alpha 16 LoRA_dropout 0.1 LoRA_r 8 target_modules [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"] bias \"none\" 4-bit True max_seq_length 2048/4096 per_device_train_batch_size 1 gradient_accumulation_steps 4 optim \"adamw_hf\" learning_rate 2e-4 max_grad_norm 0.3 scheduler \"cosine\", warm_up_ratio=0.1",
      "figure_captions": [],
      "images": []
    }
  ]
}