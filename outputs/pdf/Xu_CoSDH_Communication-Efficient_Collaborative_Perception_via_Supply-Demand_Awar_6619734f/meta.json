{
  "pdf_path_rel": "../../../papers/CV/Xu_CoSDH_Communication-Efficient_Collaborative_Perception_via_Supply-Demand_Awareness_and_Intermediate-Late_Hybridization_CVPR_2025_paper.pdf",
  "pdf_path_abs": "E:\\Desktop\\M502083B Â§öÊ®°ÊÄÅÊú∫Âô®Â≠¶‰π†\\MM_experiment2\\papers\\CV\\Xu_CoSDH_Communication-Efficient_Collaborative_Perception_via_Supply-Demand_Awareness_and_Intermediate-Late_Hybridization_CVPR_2025_paper.pdf",
  "num_pages": 10,
  "full_text_clean": "CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization Junhao Xu1*, Yanan Zhang2*, Zhi Cai1, Di Huang1‚Ä† 1State Key Laboratory of Complex and Critical Software Environment, Beihang University, Beijing, China 2School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China junhaoxu@buaa.edu.cn, yananzhang@hfut.edu.cn, {caizhi97, dhuang}@buaa.edu.cn Abstract Multi-agent collaborative perception enhances perceptual capabilities by utilizing information from multiple agents and is considered a fundamental solution to the problem of weak single-vehicle perception in autonomous driving. However, existing collaborative perception methods face a dilemma between communication efficiency and perception accuracy. To address this issue, we propose a novel communication-efficient collaborative perception framework based on supply-demand awareness and intermediate-late hybridization, dubbed as CoSDH. By modeling the supply-demand relationship between agents, the framework refines the selection of collaboration regions, reducing unnecessary communication cost while maintaining accuracy. In addition, we innovatively introduce the intermediate-late hybrid collaboration mode, where late-stage collaboration compensates for the performance degradation in collaborative perception under low communication bandwidth. Extensive experiments on multiple datasets, including both simulated and real-world scenarios, demonstrate that CoSDH achieves state-ofthe-art detection accuracy and optimal bandwidth tradeoffs, delivering superior detection precision under real communication bandwidths, thus proving its effectiveness and practical applicability. The code will be released at https://github.com/Xu2729/CoSDH. 1. Introduction Collaborative perception allows multiple agents to exchange complementary perception information. It fundamentally addresses the issues of limited perception range, sensor blind spots, and occlusion from obstacles inherent in single-agent perception, thereby improving both the range and accuracy of perception. Recent studies have demonstrated that collaborative perception can be applied to var- *Equal Contribution. ‚Ä†Corresponding Author. ious autonomous driving tasks, including 3D object detection [3, 35, 36], semantic segmentation [20, 37], 3D occupancy prediction [31], and trajectory prediction [43], exhibiting enhanced performance and improved robustness to occlusions, thus making it a crucial approach for advancing autonomous driving systems. Communication efficiency is a key issue in collaborative perception. An early approach to improving communication efficiency was to use autoencoders to compress the intermediate features that need to be transmitted [36]. Later, communication-efficient methods such as Where2comm [11] were proposed, which select important and sparse foreground regions for collaboration. However, these methods still lack sufficient precision in area selection and require more bandwidth than the real-world constraints1. As shown in Fig. 1a, when the bandwidth is limited to 27 Mbps, Where2comm [11] experiences a decrease of about 1.5% in average precision (AP@0.7) on the OPV2V [36] dataset with an intersection-over-union (IoU) threshold of 0.7. When the bandwidth is further limited to 5 Mbps and 1 Mbps, AP@0.7 drops by approximately 7.5% and 14.5%, respectively, even falling below the accuracy of late fusion methods at the result level. This makes it difficult for existing collaborative perception methods to meet the demands of practical applications. In this paper, we first analyze the problem of selecting collaboration areas and refine this selection based on the supply-demand relationship between agents. As shown in Fig. 1b, for Ego, low-foreground confidence areas can be divided into three types: Area 1 (low conficence due to absence of foreground), Area 2 (low conficence due to occlusion), and Area 3 (low conficence due to sparsity of the point cloud). Although all three areas appear as background from the Ego‚Äôs perspective, Area 1 is well-observed and does not require collaboration, while Areas 2 and 3, 1To the best of our knowledge, the common V2X communication approach uses IEEE 802.11p-based DSRC (Dedicated Short-Range Communications) technology [1, 24], which provides an information transmission rate of approximately 27 Mbps. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6834\n\nLate Fusion (~0.7 Mbps) (a) The 3D detection accuracy of Where2comm [11] and CoSDH on OPV2V dataset [36] with different bandwidth limit. Assume the number of collaborative agents is 4 and detection frequency is 10Hz. Area 1: Low confidence due to the absence of foreground Area 2: Low confidence due to occlusion Area 3: Low confidence due to sparsity of the point cloud Ego (b) Point cloud map and classification of areas with low foreground confidence. Although Area 1 has low confidence due to the absence of foreground, it allows for good observation without collaboration. In contrast, Areas 2 and 3 exhibit poor observation and require collaboration, with Area 2 having low confidence due to occlusion and Area 3 due to sparsity of the point cloud. Figure 1. Issues of existing communication-efficient collaborative perception methods. with poorer observation, need collaboration. Additionally, to address the issue of significant accuracy degradation under real bandwidth constraints, we propose that compensating intermediate collaboration results with late fusion is a feasible solution. As shown in Fig. 1a, late fusion achieves much higher accuracy than Where2comm [11] even with a bandwidth of approximately 0.7 Mbps, compared to Where2comm‚Äôs accuracy with bandwidth less than 5 Mbps. So the use of intermediate-late hybrid collaboration can greatly improve the accuracy lower bound under low-bandwidth conditions. Based on these insights, we propose CoSDH, a novel communication-efficient collaborative perception framework for 3D object detection. As shown in Fig. 2, CoSDH consists of three key components: i) Supplydemand-aware information selection, which chooses sparse yet crucial regions for collaboration; ii) Intermediate feature transmission and fusion, which transmits and effectively aggregates information from multiple agents in a communication-efficient manner; iii) Confidence-aware late fusion, which compensates for the intermediate fusion results at a minimal communication cost to improve accuracy. To evaluate CoSDH, we conducted extensive experiments on the simulation datasets OPV2V [36], V2XSim [20], and the real-world dataset DAIR-V2X [42]. The experimental results show that i) In the baseline case, CoSDH uses less bandwidth while achieving satisfactory accuracy, demonstrating a better accuracy-bandwidth tradeoff; ii) Under the simulated real-world communication condition with a total bandwidth limit of 27 Mbps, CoSDH experiences less accuracy degradation and outperforms other methods while using less bandwidth. In summary, our contributions are as follows: ‚Ä¢ We present CoSDH, an innovative communicationefficient collaborative perception framework with better accuracy-bandwidth trade-offs for 3D object detection. ‚Ä¢ We propose a novel supply-demand-aware information selection module, further refining the collaboration area selection to achieve more efficient communication. ‚Ä¢ We design a novel intermediate-late hybrid collaborative perception paradigm, where confidence-aware late fusion compensates for the intermediate fusion results to maintain high accuracy under low-bandwidth conditions. ‚Ä¢ We conduct extensive experiments across multiple datasets with bandwidth constraints set closer to realworld conditions, and CoSDH achieves state-of-the-art detection accuracy along with optimal bandwidth tradeoffs, demonstrating its feasibility in real-world collaborative perception scenarios. 2. Related Work 2.1. 3D Object Detection 3D object detection, a key technology in autonomous driving, identifies and localizes objects in 3D scenes using environmental data from onboard sensors and can be categorized into image-based, point-cloud-based, and multimodalfusion-based methods [26]. Image-based methods can be further classified into monocular [16, 33], stereo [6, 18], and multi-view [13, 44, 45] approaches based on the number of onboard cameras. These methods leverage the rich color and texture information of image, along with its dense data representation, to achieve cost-effective 3D perception for autonomous driving. However, image-based methods are inherently limited by the lack of depth information, which restricts their performance. In contrast, point-cloudbased methods, which adopt voxel-based [7, 39, 48], raw point cloud [27, 28, 46], or point-voxel hybrid [5, 29, 30] representations, can fully leverage the precise 3D geometric information provided by LiDAR, significantly improving perception capabilities. Considering that point-cloudbased methods suffer from sparse characteristic and lack rich semantic information, multimodal-fusion-based meth- 6835\n\nods [23, 41, 47] further enhance performance by leveraging the complementary advantages of different modalities. However, these single-agent-based 3D object detection methods are limited by sensor range and susceptible to occlusion, making them unable to detect objects that are further away or completely occluded. This paper focuses on collaborative-perception-based 3D object detection methods, which improve detection performance by supplementing the limitations of single-agent detection with information from other agents. 2.2. Collaborative Perception Collaborative perception can be categorized into early collaboration, intermediate collaboration, and late collaboration based on the collaboration timing. Early collaboration [2, 4, 42] shares raw perception data, providing good perception accuracy but with high bandwidth. Late collaboration [20, 36] shares perception results, significantly reducing bandwidth, but leads to a decline in perception accuracy. Intermediate collaboration operates at the feature level and can achieve a better trade-off between accuracy and bandwidth by adjusting the intermediate features transmitted [3, 10, 11, 19, 21, 22, 32, 35‚Äì 37], which is why it has been widely studied. Some of this research has focused on improving perception accuracy. FCooper [3] and CoFF [10] used manual modeling to fuse multi-agent features. Who2com [22] and When2com [21] perform selective communication and use attention-based fusion. V2VNet [32] and DiscoNet [19] employ communication graph-based fusion methods. However, these methods typically transmit complete, uncompressed intermediate BEV features, leading to enormous bandwidth requirements, which makes them challenging to apply in practice. To address the large bandwidth demand in collaborative perception, AttFuse [36] was the first to use autoencoders to compress intermediate features along the channel dimension, which was later adopted by V2X-ViT [35], CoBEVT [37] and others. However, this method leads to significant accuracy degradation at high compression rates. Where2comm [11] reduces bandwidth requirements by selecting sparse but important foreground regions for collaboration while maintaining perception accuracy. However, as bandwidth is further limited, perception accruacy still rapidly degrades, potentially even falling below the accuracy of simple late collaboration methods. To address this issue, this paper proposes a hybrid collaborative method based on both intermediate and late collaboration, which efficiently compensates for the intermediate collaborative results using late collaboration under bandwidth constraints. Furthermore, we also adopt and improve methods based on auto-encoders and information selection to save bandwidth while maintaining accuracy. 3. Method 3.1. Problem Definition In this paper, we consider the problem of collaborative perception with N agents. Let Xi and yi represent the raw observation and the corresponding ground truth supervision of the i-th agent, respectively, and let Pj‚Üíi be the message sent from agent j to agent i. In the collaborative perception, agent i aggregates its own observations and the messages {Pj‚Üíi}N j=1 sent from other agents to perform 3D object detection task. Our goal is to maximize the collaborative perception 3D object detection accuracy while ensuring that each agent has a communication budget B: ŒæŒ¶(B) = arg max Œ∏ N ‚àë i=1 g(Œ¶Œ∏(Xi,{Pj‚Üíi}N j=1),yi), (1) s.t. N ‚àë j=1 |Pj‚Üíi| ‚â§B (2) where Œ¶Œ∏ represents the collaborative perception 3D object detection model, Œ∏ denotes the model parameters, |Pj‚Üíi| represents the communication volume of the message sent from agent j to agent i, and g(¬∑,¬∑) denotes the 3D object detection evaluation metric. 3.2. Overall Architecture The overall architecture of the proposed CoSDH is shown in Fig. 2. Each agent first processes its locally observed point cloud Xi through a backbone network based on PointPillar [17] and a demand generator to obtain multi-scale BEV features {F(l) i }l=1,2,...,L and a demand mask Di, respectively. Considering the collaboration between Ego agent i and collaborating agent j, agent j generates a supply mask Sj from its multi-scale features {F(l) j }l=1,2,..,L via a supply generator, and multiplies it element-wise with the received demand matrix to obtain the supply-demand mask Mj‚Üíi. Agent j then performs supply-demand-aware information selection by multiplying {F(l) j }l=1,2,..,L with Mj‚Üíi element-wise to obtain sparse spatial features {Z(l) j }l=1,2,..,L. Subsequently, agent j compresses the features through an autoencoder, sending the non-zero parts of the features along with their corresponding coordinates as the message Pj‚Üíi to agent i. Upon receiving Pj‚Üíi, agent i first decodes the features to restore their dimensions and then fuses them with its local features {F(l) i }l=1,2,...,L across multiple scales to obtain the fused features { Àú F(l) i }l=1,2,...,L, which are then passed to the detection head for intermediate collaborative detection results Àúyi. Afterward, we apply confidence-aware late fusion: agent j filters and suppresses its own detection results yj based on confidence and sends them to agent i for late fusion, yielding the final hybrid collaborative perception detection results ÀÜyi. 6836\n\nùëøùíä {ùë≠ùíã (ùíç)} {ùíÅùíã‚Üíùíä (ùíç) } {‡∑´ ùë≠ùíä (ùíç)} {ùë≠ùíä (ùíç)} Supply-Demand-Aware Information Selection Message Compression Decoder Encoder ùë∑ùíã‚Üíùíä ‡∑•ùíöùíä ‡∑ùùíöùíä Confidence-Aware Late Fusion Backbone Demand Generator Supply Generator ùë´ùíä ùë∫ùíã Multi-scale Fusion Detector Detector ùíöùíã Late Fusion ùë¥ùíã‚Üíùíä Observation Multi-scale features Supply mask Demand mask Selection mask ùëøùíä ùë≠ùíä (ùíç) ùë∫ùíã ùë´ùíä ùë¥ùíã‚Üíùíä Spare features Message Fused features Single detection results Fused detection results ùíÅùíä (ùíç) ùë∑ùíã‚Üíùíä ùíöùíã ‡∑•ùíöùíä, ‡∑ùùíöùíä ‡∑™ ùë≠ùíä (ùíç) Ego agent Filter and Suppress by Confidence Figure 2. The overall architecture of CoSDH. The Supply-Demand-Aware Information Selection module selects sparse but important information, which is then further compressed by the Message Compression module to achieve efficient communication. ConfidenceAware Late Fusion compensates for the intermediate fusion detection results to improve accuracy. 3.3. Supply-Demand-Aware Information Selection Previous methods such as Where2comm [11] and How2comm [40] generally use symmetric supply-demand relationships to select sparse features for collaboration, believing that areas with high foreground confidence in the collaborating agent‚Äôs view should be provided, and conversely, areas with low foreground confidence in the Ego agent‚Äôs view need collaboration. However, we believe that areas with low foreground confidence from the Ego‚Äôs perspective can be divided into areas that are hard to observe and those that can be observed but belong to the background. The latter do not require collaboration. Based on this insight, we propose a novel supply-demand-aware information selection method. The demand mask Di indicates where agent i needs information from collaborating agents. Intuitively, the agent requires information from areas that are distant or occluded, which have the common characteristic of having low point cloud density or no point cloud at all. For agent i, we consider using the number of point clouds in each pillar to represent point cloud density, and we map it to the range [0,1], i.e., Ai ‚àà[0,1]H√óW, where H and W represent the number of Pillars along the height and width dimensions. We then select areas where the point cloud density is below a threshold Œµa to obtain the demand mask for agent i, Di = Ai < Œµa ‚àà{0,1}H√óW. The demand mask indicates where agent i has poor perception and needs collaborative information from other agents. Filtering information from other agents using the demand mask not only helps save bandwidth but also avoids interference from other agents‚Äô information in well-perceived areas. For the object detection task, foreground information is more valuable. Providing sparse foreground features can effectively assist other agents in supplementing undetected Concat PFE Larger communication volume Det a. Single-scale Fusion Feature before fusion Feature after fusion Det Insufficient fusion PFE b. Multi-scale Fusion but Single-scale Compression Fuse Compress Conv Deconv c. Multi-scale Compression and Fusion (Ours) ùëØ√ó ùëæ√ó ùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùë™ ùëØ ùüí√ó ùëæ ùüí√ó ùüêùë™ ùëØ ùüñ√ó ùëæ ùüñ√ó ùüíùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüêùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüîùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüíùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüíùë™ ùëØ√ó ùëæ√ó ùë™ Figure 3. Comparison of our multi-scale compression and fusion with other methods. It can achieve thorough fusion with smaller communication volume. and incomplete targets while using less bandwidth. Following previous work [11], we use the spatial confidence map Ci ‚àà[0,1]H√óW output by the detection head to select potential foreground areas. We use a supply threshold Œµc to obtain the supply mask S(l) i = Ci > Œµc ‚àà{0,1}H√óW. By adjusting the threshold, we can dynamically adjust the bandwidth used for collaborative perception to adapt to varying communication conditions. During collaboration, agent j generates a binary supplydemand selection mask Mj‚Üíi = Di ‚äôSj ‚àà{0,1}H√óW based on its supply mask Sj and agent i‚Äôs demand mask Di, and samples it to multiply element-wise with multi-scale BEV features {F(l) j }l=1,2,...,L, obtaining sparse features {Z(l) j‚Üíi}l=1,2,...,L. During communication, only the non-zero parts and their corresponding coordinates need to be transmitted. 6837\n\n3.4. Message Compression and Fusion To further reduce communication bandwidth, we also use autoencoders to compress intermediate features along the channel dimension before communication, while making further improvements to existing compression and fusion schemes. As shown in Fig. 3, traditional single-scale fusion schemes [35, 36] only compress and fuse a single-layer BEV feature map before passing it to the detection head, resulting in insufficient fusion. CoAlign [25] proposed a solution that performs layer-wise fusion on multi-scale features, addressing the issue of insufficient fusion. However, it compresses large-scale single-layer features extracted after Pillar Feature Extraction (PFE), which leads to higher bandwidth. Therefore, we propose a multi-scale compression and fusion approach, where separate autoencoders are designed for each scale of features to perform compression, followed by layer-wise fusion. This approach enables more thorough fusion while using less bandwidth. For message fusion, we use Max fusion, which has two main advantages: i) it is computationally simple and efficient, with the complexity increasing linearly with the number of agents; ii) max fusion selects the maximum value of features from multiple agents, achieving information complementarity. Considering the collaboration between Ego agent i and collaborating agent j, the process of message compression and fusion can be expressed as: Z ‚Ä≤(l) j‚Üíi = f (l) encode(Z(l) j‚Üíi) ‚ààR Cl c0 √óHl√óWl (3) F(l) j‚Üíi = f (l) decode(Z ‚Ä≤(l) j‚Üíi) ‚ààRCl√óHl√óWl (4) F ‚Ä≤(l) j‚Üíi = ftrans form(F(l) j‚Üíi,Œæ j‚Üíi) ‚ààRCl√óHl√óWl (5) Àú F(l) i = max(F(l) i ,{F ‚Ä≤(l) j‚Üíi}jÃ∏=i) ‚ààRCl√óHl√óWl (6) where Z(l) j‚Üíi ‚ààRCl√óHl√óWl represents the sparse features selected based on supply-demand relationships from the previous stage, and Cl,Hl,Wl denote the channel, height, and width dimensions of the l-th layer feature map. f (l) encode, f (l) decode represent the encoder and decoder of the l-th layer autoencoder, c0 is the compression ratio in the channel dimension, and ftrans form represents coordinate transformation. During communication, {Z ‚Ä≤(l) j‚Üíi}l=1,2,..,L is first converted from float32 to float16, and then only the non-zero parts and corresponding coordinates are transmitted to save bandwidth. Upon receiving the message, agent i decodes the feature dimensions, then aligns the features to its own coordinate system using the coordinate transformation matrix Œæj‚Üíi to obtain F ‚Ä≤(l) j‚Üíi, and finally fuses its own features with the collaborating agent‚Äôs features using max fusion to obtain the fused features Àú F(l) i . Figure 4. Our confidence-aware late fusion. It filters detection results based on confidence and suppresses suboptimal results from collaborative agents, improving overall detection accuracy. 3.5. Confidence-Aware Late Fusion Existing collaborative perception methods focus on the singular intermediate collaboration architecture and neglect the advantages of late collaboration. As shown in Table 1, the experimental results indicate that late fusion can demonstrate acceptable perception accuracy with extremely low bandwidth on the OPV2V [36] and V2XSim [20] datasets. Therefore, late fusion can be used to compensate for the results of intermediate fusion, achieving a better accuracybandwidth trade-off. Naive late collaboration methods [20, 36] directly merge results from other agents and then apply NMS (NonMaximum Suppression) to deduplicate and obtain final results. This approach can effectively improve recall for collaborative perception object detection; however, during the merging process, it may introduce some low-confidence false positives, and suboptimal detection results from collaborating agents may override the Ego agent‚Äôs detection results, lowering precision and consequently decreasing final AP. The lower AP in the late fusion method in Table 1 on the DAIR-V2X [42] dataset is due to this issue. To address this problem, we propose a new confidenceaware late fusion method. As shown in Fig. 4, during late fusion, we first filter based on the confidence of the target boxes, discarding target boxes from other agents with confidence lower than Œµl. Furthermore, considering that if both the Ego agent and other agents detect the same target, the detection result from the Ego agent, which has undergone intermediate fusion, is of higher quality. Therefore, we suppress the detection results from other agents to prevent their lower-quality results from degrading the Ego agent‚Äôs better detection results. Specifically, before merging the detection results, we multiply the confidence of target boxes from other agents by a coefficient Œ≤ ‚àà(0,1). 6838\n\nSetting Method OPV2V [36] V2XSim [20] DAIR-V2X [42] AP@0.5‚Üë AP@0.7‚Üë BD‚Üì AP@0.5‚Üë AP@0.7‚Üë BD‚Üì AP@0.5‚Üë AP@0.7‚Üë BD‚Üì Basic No Fusion 79.78% 67.16% 0.0 Mbps 70.31% 58.55% 0.0 Mbps 66.51% 55.46% 0.0 Mbps Early Fusion 95.05% 88.98% 83.1 Mbps 95.68% 88.03% 55.5 Mbps 74.52% 59.22% 50.2 Mbps Late Fusion 94.70% 87.18% 0.2 Mbps 86.88% 78.13% 0.1 Mbps 67.86% 50.47% 0.2 Mbps No Limit When2com [21] 91.75% 81.77% 1,320.0 Mbps 72.65% 62.92% 250.0 Mbps 64.08% 49.14% 984.4 Mbps FCooper [3] 90.06% 74.03% 2,640.0 Mbps 72.96% 57.61% 500.0 Mbps 74.58% 56.47% 1,968.8 Mbps AttFuse [36] 94.31% 82.03% 2,640.0 Mbps 78.06% 64.84% 500.0 Mbps 73.80% 56.86% 1,968.8 Mbps V2VNet [32] 96.66% 92.44% 5,280.0 Mbps 88.97% 85.18% 1,000.0 Mbps 66.63% 47.39% 3,937.5 Mbps DiscoNet [19] 90.93% 78.90% 2,640.0 Mbps 77.34% 68.77% 500.0 Mbps 73.58% 58.45% 1,968.8 Mbps V2XViT [35] 95.87% 89.88% 2,640.0 Mbps 89.01% 80.26% 500.0 Mbps 76.68% 57.57% 1,920.0 Mbps Where2comm [11] 95.59% 91.39% 48.7 Mbps 88.18% 83.66% 27.5 Mbps 76.13% 60.16% 172.3 Mbps CoAlign [25] 96.63% 92.63% 2,640.0 Mbps 88.87% 85.23% 500.0 Mbps 78.06% 63.09% 1,968.8 Mbps CoSDH 96.83% 92.99% 13.4 Mbps 89.23% 86.31% 1.1 Mbps 76.75% 63.85% 7.1 Mbps BD ‚â§6.75 Mbps When2com [21] 79.97% 50.88% 5.2 Mbps 62.02% 43.37% 4.0 Mbps 62.37% 44.01% 3.8 Mbps FCooper [3] 90.37% 72.92% 5.2 Mbps 76.52% 61.83% 4.0 Mbps 67.71% 47.65% 3.8 Mbps AttFuse [36] 93.45% 80.02% 5.2 Mbps 84.58% 71.36% 4.0 Mbps 71.06% 49.57% 3.8 Mbps V2VNet [32] 95.71% 87.16% 5.2 Mbps 85.66% 73.72% 4.0 Mbps 66.49% 45.61% 3.8 Mbps DiscoNet [19] 90.00% 76.72% 5.2 Mbps 78.04% 68.18% 4.0 Mbps 70.83% 53.40% 3.8 Mbps V2XViT [35] 95.85% 87.13% 5.2 Mbps 88.94% 81.47% 4.0 Mbps 71.00% 52.78% 3.8 Mbps Where2comm [11] 94.91% 89.86% 5.4 Mbps 87.51% 82.12% 4.7 Mbps 74.98% 59.51% 5.3 Mbps CoAlign [25] 94.10% 85.99% 5.2 Mbps 88.01% 83.97% 4.0 Mbps 75.26% 60.19% 3.8 Mbps CoSDH 96.75% 92.92% 2.0 Mbps 89.23% 86.31% 1.1 Mbps 76.47% 63.76% 1.4 Mbps Table 1. Comparison of detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20], and DAIR-V2X [42] datasets. ‚ÄúBD‚Äù represents the bandwidth required for each collaborative agent, assuming Ego agent collaborates with up to 4 agents and the detection frequency is 10Hz. ‚ÄúBD‚â§6.75 Mbps‚Äù is used to simulate real-world communication limits, assuming a total communication bandwidth of 27 Mbps, with each collaborating agent‚Äôs bandwidth consumption limited to less than 6.75 Mbps. For intermediate collaboration methods without information selection, we provide a compressed version using autoencoders to meet the bandwidth constraints. For intermediate collaboration methods with information selection, the selection ratio is adjusted to meet the bandwidth constraints. 4. Experiments 4.1. Datasets and Experimental Settings Datasets. We evaluate the proposed CoSDH against other methods on three different collaborative perception datasets (OPV2V [36], V2XSim [20], and DAIR-V2X [42]) for LiDAR-based 3D object detection. The datasets include both simulated and real-world scenarios, and cover two types of collaboration: V2V (Vehicle to Vehicle) and V2I (Vehicle to Infrastructure). Evaluation Metrics We use the average precision (AP) with intersection-over-union (IoU) thresholds of 0.5 and 0.7 to evaluate the performance of different methods on 3D object detection. We assume the target detection frequency is 10Hz and calculate the communication bandwidth based on the average data transmitted by each collaborative agent to the Ego agent, in order to evaluate the communication cost of different methods. Specifically, we consider the bandwidth limitations in real-world collaborative perception scenarios, setting the vehicle‚Äôs communication rate to 27 Mbps [1, 24]. Considering the typical case where the Ego agent collaborates with up to 4 other agents [36], the bandwidth limit for each collaborative agent is 27/4 = 6.75 Mbps. Implementation Our experiments are based on the OpenCOOD [36] framework, using PointPillar [17] as the encoder with a grid size of (0.4m,0.4m), and a maximum of 32 points per Pillar. For our method, we set the number of intermediate feature layers to L = 3, the demand threshold Œµa = 4/32 = 0.125, and the supply threshold Œµc = 0.01. For the OPV2V [36] and DAIR-V2X [42] datasets, the compression rate is c0 = 16, and for the V2XSim [20] dataset, the compression rate is c0 = 8 due to its smaller perception range and lower inherent bandwidth requirement. The late fusion threshold is set to Œµl = 0.3, with a suppression coefficient Œ≤ = 0.9 for OPV2V and V2XSim dataset and Œ≤ = 0.8 for DAIR-V2X dataset because of its difficulty. In late fusion, the dense prediction results before NMS are transmitted, as this reduces the computational burden on the collaborating agents. The experiments use the Adam [14] optimizer, with an initial learning rate set between 0.0001 and 0.002 based on the model‚Äôs testing complexity to ensure proper training. The maximum number of collaborating agents is set to 5. The number of training epochs is set to 40 to ensure model convergence. Other experimental parameters are kept consistent with the OpenCOOD framework. All methods are trained on four NVIDIA GeForce RTX 3090 GPUs. 4.2. Quantitative Evaluation Benchmark Comparison. Table 1 presents the collaborative 3D object detection accuracy and required bandwidth of the proposed CoSDH compared to previous methods across different datasets. Experimental results show that, with the default uncompressed settings, CoSDH achieves the highest accuracy on the OPV2V [36] and V2XSim [20] datasets while requiring only about 1/100 to 1/1000 of the bandwidth compared to other non-communication-efficient 6839\n\nCoSDH Late Fusion Where2comm DiscoNet V2VNet V2X-ViT CoAlign Early Fusion DiscoNet CoAlign V2X-ViT V2VNet Early Fusion Where2comm Late Fusion CoSDH No Fusion No Fusion No Fusion No Fusion No Fusion No Fusion V2VNet V2VNet DiscoNet DiscoNet DiscoNet Early Fusion Early Fusion Early Fusion V2X-ViT V2VNet CoAlign CoSDH Late Fusion Where2comm Where2comm Where2comm Where2comm Late Fusion Late Fusion CoSDH CoSDH CoAlign CoAlign V2X-ViT V2X-ViT DiscoNet CoSDH CoAlign V2VNet Early Fusion Late Fusion Real-world limitation Real-world limitation Real-world limitation Real-world limitation Real-world limitation Real-world limitation V2X-ViT Figure 5. Comparison of the trade-off between detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20] and DAIR-V2X [42] datasets, CoSDH achieves the best accuracy-bandwidth trade-off. The real-world limitation refers to the total bandwidth limit of 27 Mbps, which means that each collaborative agent does not exceed 6.75 Mbps. methods. Although the AP@0.5 of CoSDH on the DAIRV2X [42] dataset is slightly lower than that of CoAlign [25], it achieves a higher AP@0.7 with only about 1/300 of the bandwidth. Both our CoSDH and Where2comm [11] are communication-efficient methods based on information selection, which enable dynamic accuracy-bandwidth tradeoffs by adjusting the selection ratio. The table shows accuracy at a specific bandwidth, and a more detailed comparison of the accuracy-bandwidth curves is provided in the ‚ÄúAccuracy-Bandwidth Trade-Off Comparison‚Äù part. Furthermore, we simulate real-world communication rate limitations. Result shows that CoSDH achieves improvements in AP@0.7 of 3.06%/2.34%/3.75% on OPV2V/V2XSim/DAIR-V2X compared to the previous best methods, while using less bandwidth. When comparing the scenarios with and without bandwidth limitations on the OPV2V and DAIR-V2X datasets, we found that CoSDH achieves less than a 0.3% decrease in AP under conditions where bandwidth is reduced by 80% to 85%, exhibiting less accuracy degradation than Where2comm. Most other methods also show varying degrees of accuracy degradation under bandwidth constraints. Interestingly, under bandwidth limitations, some methods show an improvement in accuracy on certain datasets after using autoencoders to compress intermediate features. For example, FCooper [3] shows improved accuracy on the V2XSim dataset, which may be due to the model‚Äôs initially poor performance. The compressed version, with the added autoencoder, increases the model‚Äôs parameter count, thereby enriching its expressive capability. Accuracy-Bandwidth Trade-Off Comparison. Fig. 5 shows the accuracy-bandwidth trade-off of the proposed CoSDH and previous advanced methods across different datasets. When the bandwidth grater than 1 Mbps, Where2comm [11] demonstrates a good enough accuracybandwidth trade-off, maintaining high detection accuracy as bandwidth decreases. However, as bandwidth further decreases, its accuracy quickly declines, even falling below that of late fusion method. CoSDH can leverage late fusion at low bandwidths to maintain high accuracy, achieving a better performance-bandwidth trade-off. Notably, on the DAIR-V2X dataset, the late fusion method performs worse than the no-fusion case because the naive late fusion method unselectively merges detection results from other agents, introducing a large number of low-quality detections. CoSDH uses a confidence-aware late fusion method to select higher-quality detection results, improving this situation and enhancing perception accuracy. We also observe that the AP@0.7 of CoSDH on the V2XSim dataset initially increases slightly as bandwidth decreases before subsequently declining, indicating that selecting key collaboration areas can help reduce interference from other regions to some extent, thereby slightly improving detection accuracy. 6840\n\nFigure 6. Visualization of detection results under real-world communication rate limitations on the DAIR-V2X [42] dataset. Green represents ground truth box, and red represents predicted box. 4.3. Qualitative Evaluation Fig. 6 shows the visualization of detection results for our method compared to other methods under simulated realworld communication rate limitations on the DAIR-V2X dataset. A comparison of the detection results reveals that, while using less bandwidth, our method achieves the same recall rate as CoAlign [25] and Where2comm [11], with fewer false positive predictions and more accurate object localization, demonstrating that our method performs better under low-bandwidth conditions. Comparing the two images at bottom, it can be seen that under low bandwidth conditions, late fusion effectively compensates for the results of intermediate fusion, improving the recall rate of objects. 4.4. Ablation Studies Table 2 presents the results of the ablation study on various modules of the proposed method using the OPV2V [36] dataset. The results show that after using the autoencoder for compression, the bandwidth is reduced by a factor of c0, and there is no significant loss in perception accuracy, even with a slight improvement in AP@0.7. Converting intermediate features to float16 can nearly halve the bandwidth without significant loss, demonstrating the effectiveness of our proposed message compression module. After performing information selection based on the supply mask, perception accuracy slightly decreases, but the bandwidth is reduced by about 95%, which is a worthwhile trade-off. Further using the demand mask results in no significant decrease in accuracy but reduces bandwidth by about 10%. This demonstrates that our supply-demand-aware information selection can reduce bandwidth while maintaining accuracy. After adding late fusion, accuracy improves at a relatively small bandwidth cost. However, since this phase uses more bandwidth, the improvement in accuracy is not signifCompression Selection Late Fusion AP@0.5‚Üë AP@0.7‚Üë BD‚Üì Autoencoder FP16 Supply Demand 96.62% 92.62% 1,155.00 Mbps ‚úì 96.59% 92.99% 72.19 Mbps ‚úì ‚úì 96.59% 92.99% 36.09 Mbps ‚úì ‚úì ‚úì 96.30% 92.62% 1.99 Mbps ‚úì ‚úì ‚úì ‚úì 96.31% 92.60% 1.82 Mbps ‚úì ‚úì ‚úì ‚úì ‚úì 96.75% 92.92% 1.97 Mbps Table 2. Ablation study of the modules in CoSDH on the OPV2V dataset. ‚ÄúAutoencoder‚Äù refers to the use of autoencoders to compress features along the channel dimension, with a compression ratio of c0 = 16. ‚ÄúFP16‚Äù refers to converting features from float32 to float16 for transmission. ‚ÄúSupply‚Äù and ‚ÄúDemand‚Äù refer to using supply and demand masks for information selection, and ‚ÄúLate Fusion‚Äù refers to applying confidence-aware late fusion. Late Fusion Œµc 0.01 0.02 0.03 0.05 0.07 AP@0.5‚Üë 96.59% 96.31% 96.19% 95.18% 93.35% AP@0.7‚Üë 92.95% 92.60% 92.27% 90.67% 87.60% BD‚Üì 13.26 Mbps 1.82 Mbps 0.54 Mbps 0.12 Mbps 0.05 Mbps ‚úì AP@0.5‚Üë 96.83% 96.75% 96.72% 96.54% 96.41% AP@0.7‚Üë 92.99% 92.92% 92.73% 92.23% 91.68% BD‚Üì 13.40 Mbps 1.97 Mbps 0.68 Mbps 0.26 Mbps 0.19 Mbps Table 3. Ablation study of confidence-aware late fusion under different bandwidths on the OPV2V [36] dataset. The table shows the impact of late fusion on accuracy and bandwidth with different values of Œµc. icant. Table 3 shows the accuracy improvement due to late collaboration under different bandwidth conditions. It can be observed that late collaboration provides more accuracy improvements under lower bandwidth conditions. For example, when the bandwidth used for intermediate fusion is 0.05 Mbps, late collaboration with a bandwidth cost of 0.14 Mbps leads to a 3% improvement in AP@0.5 and a 4% improvement in AP@0.7. This is one of the key reasons why our method achieves significantly higher detection accuracy under lower bandwidth compared to Where2comm [11]. 5. Conclusion In this paper, we propose CoSDH, a novel communicationefficient collaborative perception framework for 3D object detection. By finely modeling the supply-demand relationship between agents, it selects key and sparse regions for collaboration. Additionally, we innovatively incorporate confidence-aware late fusion on top of intermediate collaboration to form an intermediate-late hybrid collaborative perception method. Experiments on multiple datasets show that our method offers a better accuracy-bandwidth tradeoff, demonstrating outstanding accuracy under bandwidth constraints close to real-world communication limits, making it highly valuable for practical applications. Acknowledgment This work is supported by the National Key Research and Development Plan (2024YFB3309302). 6841\n\nReferences [1] Fabio Arena and Giovanni Pau. An overview of vehicular communications. Future internet, 11(2):27, 2019. 1, 6 [2] Eduardo Arnold, Mehrdad Dianati, Robert de Temple, and Saber Fallah. Cooperative perception for 3d object detection in driving scenarios using infrastructure sensors. IEEE Transactions on Intelligent Transportation Systems, 23(3): 1852‚Äì1864, 2020. 3 [3] Qi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and Song Fu. F-cooper: Feature based cooperative perception for autonomous vehicle edge computing system using 3d point clouds. In Proceedings of the 4th ACM/IEEE Symposium on Edge Computing, pages 88‚Äì100, 2019. 1, 3, 6, 7, 2 [4] Qi Chen, Sihai Tang, Qing Yang, and Song Fu. Cooper: Cooperative perception for connected autonomous vehicles based on 3d point clouds. In 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), pages 514‚Äì524. IEEE, 2019. 3 [5] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast point r-cnn. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9775‚Äì9784, 2019. 2 [6] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Dsgn: Deep stereo geometry network for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12536‚Äì12545, 2020. 2 [7] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. In Proceedings of the AAAI conference on artificial intelligence, pages 1201‚Äì1209, 2021. 2 [8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 1‚Äì16. PMLR, 2017. 1 [9] Sebastian Gr¬®afling, Petri M¬®ah¬®onen, and Janne Riihij¬®arvi. Performance evaluation of ieee 1609 wave and ieee 802.11 p for vehicular communications. In 2010 second international conference on ubiquitous and future networks (ICUFN), pages 344‚Äì348. IEEE, 2010. 2 [10] Jingda Guo, Dominic Carrillo, Sihai Tang, Qi Chen, Qing Yang, Song Fu, Xi Wang, Nannan Wang, and Paparao Palacharla. Coff: Cooperative spatial feature fusion for 3- d object detection on autonomous vehicles. IEEE Internet of Things Journal, 8(14):11078‚Äì11087, 2021. 3 [11] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen. Where2comm: Communication-efficient collaborative perception via spatial confidence maps. Advances in neural information processing systems, 35:4874‚Äì4886, 2022. 1, 2, 3, 4, 6, 7, 8 [12] Yue Hu, Juntong Peng, Sifei Liu, Junhao Ge, Si Liu, and Siheng Chen. Communication-efficient collaborative perception via information filling with codebook. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15481‚Äì15490, 2024. 2 [13] Zheng Jiang, Jinqing Zhang, Yanan Zhang, Qingjie Liu, Zhenghui Hu, Baohui Wang, and Yunhong Wang. Fsd-bev: Foreground self-distillation for multi-view 3d object detection. In European Conference on Computer Vision, pages 110‚Äì126. Springer, 2024. 2 [14] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [15] Daniel Krajzewicz, Jakob Erdmann, Michael Behrisch, and Laura Bieker. Recent development and applications of sumosimulation of urban mobility. International journal on advances in systems and measurements, 5(3&4), 2012. 1 [16] Jason Ku, Alex D Pon, and Steven L Waslander. Monocular 3d object detection leveraging accurate proposals and shape reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11867‚Äì 11876, 2019. 2 [17] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12697‚Äì12705, 2019. 3, 6 [18] Peiliang Li, Xiaozhi Chen, and Shaojie Shen. Stereo r-cnn based 3d object detection for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7644‚Äì7652, 2019. 2 [19] Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, Chen Feng, and Wenjun Zhang. Learning distilled collaboration graph for multi-agent perception. Advances in Neural Information Processing Systems, 34:29541‚Äì29552, 2021. 3, 6 [20] Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi Zhong, Siheng Chen, and Chen Feng. V2x-sim: Multi-agent collaborative perception dataset and benchmark for autonomous driving. IEEE Robotics and Automation Letters, 7(4):10914‚Äì 10921, 2022. 1, 2, 3, 5, 6, 7 [21] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira. When2com: Multi-agent perception via communication graph grouping. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 4106‚Äì4115, 2020. 3, 6 [22] Yen-Cheng Liu, Junjiao Tian, Chih-Yao Ma, Nathan Glaser, Chia-Wen Kuo, and Zsolt Kira. Who2com: Collaborative perception via learnable handshake communication. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 6876‚Äì6883. IEEE, 2020. 3 [23] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multitask multi-sensor fusion with unified bird‚Äôs-eye view representation. In 2023 IEEE international conference on robotics and automation (ICRA), pages 2774‚Äì2781. IEEE, 2023. 3 [24] Jose Manuel Lozano Dominguez and Tomas Jesus Mateo Sanguino. Review on v2x, i2x, and p2x communications and their applications: a comprehensive analysis over time. Sensors, 19(12):2756, 2019. 1, 6 [25] Yifan Lu, Quanhao Li, Baoan Liu, Mehrdad Dianati, Chen Feng, Siheng Chen, and Yanfeng Wang. Robust collaborative 3d object detection in presence of pose errors. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 4812‚Äì4818. IEEE, 2023. 5, 6, 7, 8, 1, 2 [26] Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. 3d object detection for autonomous driving: A 6842\n\ncomprehensive survey. International Journal of Computer Vision, 131(8):1909‚Äì1963, 2023. 2 [27] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao Huang. 3d object detection with pointformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7463‚Äì7472, 2021. 2 [28] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 770‚Äì779, 2019. 2 [29] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10529‚Äì10538, 2020. 2 [30] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE transactions on pattern analysis and machine intelligence, 43(8):2647‚Äì2664, 2020. 2 [31] Rui Song, Chenwei Liang, Hu Cao, Zhiran Yan, Walter Zimmer, Markus Gross, Andreas Festag, and Alois Knoll. Collaborative semantic occupancy prediction with hybrid feature fusion in connected automated vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17996‚Äì18006, 2024. 1 [32] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2vnet: Vehicle-to-vehicle communication for joint perception and prediction. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part II 16, pages 605‚Äì621. Springer, 2020. 3, 6 [33] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudolidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8445‚Äì8453, 2019. 2 [34] Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Hao Xiang, and Jiaqi Ma. Opencda: an open cooperative driving automation framework integrated with co-simulation. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pages 1155‚Äì1162. IEEE, 2021. 1 [35] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, MingHsuan Yang, and Jiaqi Ma. V2x-vit: Vehicle-to-everything cooperative perception with vision transformer. In European conference on computer vision, pages 107‚Äì124. Springer, 2022. 1, 3, 5, 6, 2 [36] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and Jiaqi Ma. Opv2v: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication. In 2022 International Conference on Robotics and Automation (ICRA), pages 2583‚Äì2589. IEEE, 2022. 1, 2, 3, 5, 6, 7, 8 [37] Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei Zhou, and Jiaqi Ma. Cobevt: Cooperative bird‚Äôs eye view semantic segmentation with sparse transformers. In Conference on Robot Learning, pages 989‚Äì1000. PMLR, 2023. 1, 3 [38] Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, et al. V2v4real: A real-world large-scale dataset for vehicle-to-vehicle cooperative perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13712‚Äì13722, 2023. 1, 2 [39] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018. 2 [40] Dingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi Xu, Rongbin Yin, Peng Zhai, and Lihua Zhang. How2comm: Communication-efficient and collaboration-pragmatic multiagent perception. Advances in Neural Information Processing Systems, 36, 2024. 4 [41] Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, and Wenguan Wang. Is-fusion: Instance-scene collaborative fusion for multimodal 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14905‚Äì 14915, 2024. 3 [42] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, et al. Dair-v2x: A large-scale dataset for vehicleinfrastructure cooperative 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21361‚Äì21370, 2022. 2, 3, 5, 6, 7, 8, 1 [43] Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang, Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan, Ning Sun, et al. V2x-seq: A large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5486‚Äì5495, 2023. 1 [44] Jinqing Zhang, Yanan Zhang, Qingjie Liu, and Yunhong Wang. Sa-bev: Generating semantic-aware bird‚Äôs-eye-view feature for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3348‚Äì3357, 2023. 2 [45] Jinqing Zhang, Yanan Zhang, Yunlong Qi, Zehua Fu, Qingjie Liu, and Yunhong Wang. Geobev: Learning geometric bev representation for multi-view 3d object detection. arXiv preprint arXiv:2409.01816, 2024. 2 [46] Yanan Zhang, Di Huang, and Yunhong Wang. Pc-rgnn: Point cloud completion and graph neural network for 3d object detection. In Proceedings of the AAAI conference on artificial intelligence, pages 3430‚Äì3437, 2021. 2 [47] Yanan Zhang, Jiaxin Chen, and Di Huang. Cat-det: Contrastively augmented transformer for multi-modal 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 908‚Äì917, 2022. 3 [48] Chao Zhou, Yanan Zhang, Jiaxin Chen, and Di Huang. Octr: Octree-based transformer for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5166‚Äì5175, 2023. 2 6843",
  "pages": [
    {
      "page": 1,
      "text_raw": "CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand\nAwareness and Intermediate-Late Hybridization\nJunhao Xu1*, Yanan Zhang2*, Zhi Cai1, Di Huang1‚Ä†\n1State Key Laboratory of Complex and Critical Software Environment, Beihang University, Beijing, China\n2School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China\njunhaoxu@buaa.edu.cn, yananzhang@hfut.edu.cn, {caizhi97, dhuang}@buaa.edu.cn\nAbstract\nMulti-agent collaborative perception enhances perceptual\ncapabilities by utilizing information from multiple agents\nand is considered a fundamental solution to the prob-\nlem of weak single-vehicle perception in autonomous driv-\ning.\nHowever, existing collaborative perception meth-\nods face a dilemma between communication efficiency\nand perception accuracy. To address this issue, we pro-\npose a novel communication-efficient collaborative per-\nception framework based on supply-demand awareness\nand intermediate-late hybridization, dubbed as CoSDH. By\nmodeling the supply-demand relationship between agents,\nthe framework refines the selection of collaboration re-\ngions, reducing unnecessary communication cost while\nmaintaining accuracy.\nIn addition, we innovatively in-\ntroduce the intermediate-late hybrid collaboration mode,\nwhere late-stage collaboration compensates for the per-\nformance degradation in collaborative perception under\nlow communication bandwidth. Extensive experiments on\nmultiple datasets, including both simulated and real-world\nscenarios, demonstrate that CoSDH\nachieves state-of-\nthe-art detection accuracy and optimal bandwidth trade-\noffs, delivering superior detection precision under real\ncommunication bandwidths, thus proving its effectiveness\nand practical applicability. The code will be released at\nhttps://github.com/Xu2729/CoSDH.\n1. Introduction\nCollaborative perception allows multiple agents to ex-\nchange complementary perception information. It funda-\nmentally addresses the issues of limited perception range,\nsensor blind spots, and occlusion from obstacles inherent in\nsingle-agent perception, thereby improving both the range\nand accuracy of perception. Recent studies have demon-\nstrated that collaborative perception can be applied to var-\n*Equal Contribution. ‚Ä†Corresponding Author.\nious autonomous driving tasks, including 3D object detec-\ntion [3, 35, 36], semantic segmentation [20, 37], 3D occu-\npancy prediction [31], and trajectory prediction [43], ex-\nhibiting enhanced performance and improved robustness to\nocclusions, thus making it a crucial approach for advancing\nautonomous driving systems.\nCommunication efficiency is a key issue in collabora-\ntive perception.\nAn early approach to improving com-\nmunication efficiency was to use autoencoders to com-\npress the intermediate features that need to be transmit-\nted [36]. Later, communication-efficient methods such as\nWhere2comm [11] were proposed, which select important\nand sparse foreground regions for collaboration. However,\nthese methods still lack sufficient precision in area selec-\ntion and require more bandwidth than the real-world con-\nstraints1.\nAs shown in Fig. 1a, when the bandwidth is\nlimited to 27 Mbps, Where2comm [11] experiences a de-\ncrease of about 1.5% in average precision (AP@0.7) on the\nOPV2V [36] dataset with an intersection-over-union (IoU)\nthreshold of 0.7. When the bandwidth is further limited to 5\nMbps and 1 Mbps, AP@0.7 drops by approximately 7.5%\nand 14.5%, respectively, even falling below the accuracy of\nlate fusion methods at the result level. This makes it diffi-\ncult for existing collaborative perception methods to meet\nthe demands of practical applications.\nIn this paper, we first analyze the problem of select-\ning collaboration areas and refine this selection based on\nthe supply-demand relationship between agents. As shown\nin Fig. 1b, for Ego, low-foreground confidence areas can\nbe divided into three types: Area 1 (low conficence due\nto absence of foreground), Area 2 (low conficence due to\nocclusion), and Area 3 (low conficence due to sparsity of\nthe point cloud). Although all three areas appear as back-\nground from the Ego‚Äôs perspective, Area 1 is well-observed\nand does not require collaboration, while Areas 2 and 3,\n1To the best of our knowledge, the common V2X communication ap-\nproach uses IEEE 802.11p-based DSRC (Dedicated Short-Range Commu-\nnications) technology [1, 24], which provides an information transmission\nrate of approximately 27 Mbps.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6834\n",
      "text_clean": "CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization Junhao Xu1*, Yanan Zhang2*, Zhi Cai1, Di Huang1‚Ä† 1State Key Laboratory of Complex and Critical Software Environment, Beihang University, Beijing, China 2School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China junhaoxu@buaa.edu.cn, yananzhang@hfut.edu.cn, {caizhi97, dhuang}@buaa.edu.cn Abstract Multi-agent collaborative perception enhances perceptual capabilities by utilizing information from multiple agents and is considered a fundamental solution to the problem of weak single-vehicle perception in autonomous driving. However, existing collaborative perception methods face a dilemma between communication efficiency and perception accuracy. To address this issue, we propose a novel communication-efficient collaborative perception framework based on supply-demand awareness and intermediate-late hybridization, dubbed as CoSDH. By modeling the supply-demand relationship between agents, the framework refines the selection of collaboration regions, reducing unnecessary communication cost while maintaining accuracy. In addition, we innovatively introduce the intermediate-late hybrid collaboration mode, where late-stage collaboration compensates for the performance degradation in collaborative perception under low communication bandwidth. Extensive experiments on multiple datasets, including both simulated and real-world scenarios, demonstrate that CoSDH achieves state-ofthe-art detection accuracy and optimal bandwidth tradeoffs, delivering superior detection precision under real communication bandwidths, thus proving its effectiveness and practical applicability. The code will be released at https://github.com/Xu2729/CoSDH. 1. Introduction Collaborative perception allows multiple agents to exchange complementary perception information. It fundamentally addresses the issues of limited perception range, sensor blind spots, and occlusion from obstacles inherent in single-agent perception, thereby improving both the range and accuracy of perception. Recent studies have demonstrated that collaborative perception can be applied to var- *Equal Contribution. ‚Ä†Corresponding Author. ious autonomous driving tasks, including 3D object detection [3, 35, 36], semantic segmentation [20, 37], 3D occupancy prediction [31], and trajectory prediction [43], exhibiting enhanced performance and improved robustness to occlusions, thus making it a crucial approach for advancing autonomous driving systems. Communication efficiency is a key issue in collaborative perception. An early approach to improving communication efficiency was to use autoencoders to compress the intermediate features that need to be transmitted [36]. Later, communication-efficient methods such as Where2comm [11] were proposed, which select important and sparse foreground regions for collaboration. However, these methods still lack sufficient precision in area selection and require more bandwidth than the real-world constraints1. As shown in Fig. 1a, when the bandwidth is limited to 27 Mbps, Where2comm [11] experiences a decrease of about 1.5% in average precision (AP@0.7) on the OPV2V [36] dataset with an intersection-over-union (IoU) threshold of 0.7. When the bandwidth is further limited to 5 Mbps and 1 Mbps, AP@0.7 drops by approximately 7.5% and 14.5%, respectively, even falling below the accuracy of late fusion methods at the result level. This makes it difficult for existing collaborative perception methods to meet the demands of practical applications. In this paper, we first analyze the problem of selecting collaboration areas and refine this selection based on the supply-demand relationship between agents. As shown in Fig. 1b, for Ego, low-foreground confidence areas can be divided into three types: Area 1 (low conficence due to absence of foreground), Area 2 (low conficence due to occlusion), and Area 3 (low conficence due to sparsity of the point cloud). Although all three areas appear as background from the Ego‚Äôs perspective, Area 1 is well-observed and does not require collaboration, while Areas 2 and 3, 1To the best of our knowledge, the common V2X communication approach uses IEEE 802.11p-based DSRC (Dedicated Short-Range Communications) technology [1, 24], which provides an information transmission rate of approximately 27 Mbps. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6834",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 2,
      "text_raw": "Late Fusion\n(~0.7 Mbps)\n(a) The 3D detection accuracy of Where2comm [11] and\nCoSDH on OPV2V dataset [36] with different bandwidth\nlimit. Assume the number of collaborative agents is 4 and\ndetection frequency is 10Hz.\nArea 1: Low confidence due \nto the absence of foreground\nArea 2: Low confidence \ndue to occlusion\nArea 3: Low confidence \ndue to sparsity of the \npoint cloud\nEgo\n(b) Point cloud map and classification of areas with low foreground confidence. Although Area\n1 has low confidence due to the absence of foreground, it allows for good observation without\ncollaboration. In contrast, Areas 2 and 3 exhibit poor observation and require collaboration, with\nArea 2 having low confidence due to occlusion and Area 3 due to sparsity of the point cloud.\nFigure 1. Issues of existing communication-efficient collaborative perception methods.\nwith poorer observation, need collaboration. Additionally,\nto address the issue of significant accuracy degradation un-\nder real bandwidth constraints, we propose that compen-\nsating intermediate collaboration results with late fusion\nis a feasible solution.\nAs shown in Fig. 1a, late fusion\nachieves much higher accuracy than Where2comm [11]\neven with a bandwidth of approximately 0.7 Mbps, com-\npared to Where2comm‚Äôs accuracy with bandwidth less than\n5 Mbps. So the use of intermediate-late hybrid collabora-\ntion can greatly improve the accuracy lower bound under\nlow-bandwidth conditions.\nBased on these insights, we propose CoSDH, a novel\ncommunication-efficient collaborative perception frame-\nwork for 3D object detection.\nAs shown in Fig. 2,\nCoSDH\nconsists of three key components:\ni) Supply-\ndemand-aware information selection, which chooses sparse\nyet crucial regions for collaboration; ii) Intermediate fea-\nture transmission and fusion, which transmits and ef-\nfectively aggregates information from multiple agents in\na communication-efficient manner; iii) Confidence-aware\nlate fusion, which compensates for the intermediate fu-\nsion results at a minimal communication cost to im-\nprove accuracy. To evaluate CoSDH, we conducted exten-\nsive experiments on the simulation datasets OPV2V [36],\nV2XSim [20], and the real-world dataset DAIR-V2X [42].\nThe experimental results show that i) In the baseline case,\nCoSDH uses less bandwidth while achieving satisfactory\naccuracy, demonstrating a better accuracy-bandwidth trade-\noff; ii) Under the simulated real-world communication con-\ndition with a total bandwidth limit of 27 Mbps, CoSDH ex-\nperiences less accuracy degradation and outperforms other\nmethods while using less bandwidth.\nIn summary, our contributions are as follows:\n‚Ä¢ We present CoSDH, an innovative communication-\nefficient collaborative perception framework with better\naccuracy-bandwidth trade-offs for 3D object detection.\n‚Ä¢ We propose a novel supply-demand-aware information\nselection module, further refining the collaboration area\nselection to achieve more efficient communication.\n‚Ä¢ We design a novel intermediate-late hybrid collaborative\nperception paradigm, where confidence-aware late fusion\ncompensates for the intermediate fusion results to main-\ntain high accuracy under low-bandwidth conditions.\n‚Ä¢ We conduct extensive experiments across multiple\ndatasets with bandwidth constraints set closer to real-\nworld conditions, and CoSDH achieves state-of-the-art\ndetection accuracy along with optimal bandwidth trade-\noffs, demonstrating its feasibility in real-world collabora-\ntive perception scenarios.\n2. Related Work\n2.1. 3D Object Detection\n3D object detection, a key technology in autonomous driv-\ning, identifies and localizes objects in 3D scenes using en-\nvironmental data from onboard sensors and can be catego-\nrized into image-based, point-cloud-based, and multimodal-\nfusion-based methods [26]. Image-based methods can be\nfurther classified into monocular [16, 33], stereo [6, 18],\nand multi-view [13, 44, 45] approaches based on the num-\nber of onboard cameras. These methods leverage the rich\ncolor and texture information of image, along with its dense\ndata representation, to achieve cost-effective 3D percep-\ntion for autonomous driving. However, image-based meth-\nods are inherently limited by the lack of depth information,\nwhich restricts their performance. In contrast, point-cloud-\nbased methods, which adopt voxel-based [7, 39, 48], raw\npoint cloud [27, 28, 46], or point-voxel hybrid [5, 29, 30]\nrepresentations, can fully leverage the precise 3D geomet-\nric information provided by LiDAR, significantly improv-\ning perception capabilities. Considering that point-cloud-\nbased methods suffer from sparse characteristic and lack\nrich semantic information, multimodal-fusion-based meth-\n6835\n",
      "text_clean": "Late Fusion (~0.7 Mbps) (a) The 3D detection accuracy of Where2comm [11] and CoSDH on OPV2V dataset [36] with different bandwidth limit. Assume the number of collaborative agents is 4 and detection frequency is 10Hz. Area 1: Low confidence due to the absence of foreground Area 2: Low confidence due to occlusion Area 3: Low confidence due to sparsity of the point cloud Ego (b) Point cloud map and classification of areas with low foreground confidence. Although Area 1 has low confidence due to the absence of foreground, it allows for good observation without collaboration. In contrast, Areas 2 and 3 exhibit poor observation and require collaboration, with Area 2 having low confidence due to occlusion and Area 3 due to sparsity of the point cloud. Figure 1. Issues of existing communication-efficient collaborative perception methods. with poorer observation, need collaboration. Additionally, to address the issue of significant accuracy degradation under real bandwidth constraints, we propose that compensating intermediate collaboration results with late fusion is a feasible solution. As shown in Fig. 1a, late fusion achieves much higher accuracy than Where2comm [11] even with a bandwidth of approximately 0.7 Mbps, compared to Where2comm‚Äôs accuracy with bandwidth less than 5 Mbps. So the use of intermediate-late hybrid collaboration can greatly improve the accuracy lower bound under low-bandwidth conditions. Based on these insights, we propose CoSDH, a novel communication-efficient collaborative perception framework for 3D object detection. As shown in Fig. 2, CoSDH consists of three key components: i) Supplydemand-aware information selection, which chooses sparse yet crucial regions for collaboration; ii) Intermediate feature transmission and fusion, which transmits and effectively aggregates information from multiple agents in a communication-efficient manner; iii) Confidence-aware late fusion, which compensates for the intermediate fusion results at a minimal communication cost to improve accuracy. To evaluate CoSDH, we conducted extensive experiments on the simulation datasets OPV2V [36], V2XSim [20], and the real-world dataset DAIR-V2X [42]. The experimental results show that i) In the baseline case, CoSDH uses less bandwidth while achieving satisfactory accuracy, demonstrating a better accuracy-bandwidth tradeoff; ii) Under the simulated real-world communication condition with a total bandwidth limit of 27 Mbps, CoSDH experiences less accuracy degradation and outperforms other methods while using less bandwidth. In summary, our contributions are as follows: ‚Ä¢ We present CoSDH, an innovative communicationefficient collaborative perception framework with better accuracy-bandwidth trade-offs for 3D object detection. ‚Ä¢ We propose a novel supply-demand-aware information selection module, further refining the collaboration area selection to achieve more efficient communication. ‚Ä¢ We design a novel intermediate-late hybrid collaborative perception paradigm, where confidence-aware late fusion compensates for the intermediate fusion results to maintain high accuracy under low-bandwidth conditions. ‚Ä¢ We conduct extensive experiments across multiple datasets with bandwidth constraints set closer to realworld conditions, and CoSDH achieves state-of-the-art detection accuracy along with optimal bandwidth tradeoffs, demonstrating its feasibility in real-world collaborative perception scenarios. 2. Related Work 2.1. 3D Object Detection 3D object detection, a key technology in autonomous driving, identifies and localizes objects in 3D scenes using environmental data from onboard sensors and can be categorized into image-based, point-cloud-based, and multimodalfusion-based methods [26]. Image-based methods can be further classified into monocular [16, 33], stereo [6, 18], and multi-view [13, 44, 45] approaches based on the number of onboard cameras. These methods leverage the rich color and texture information of image, along with its dense data representation, to achieve cost-effective 3D perception for autonomous driving. However, image-based methods are inherently limited by the lack of depth information, which restricts their performance. In contrast, point-cloudbased methods, which adopt voxel-based [7, 39, 48], raw point cloud [27, 28, 46], or point-voxel hybrid [5, 29, 30] representations, can fully leverage the precise 3D geometric information provided by LiDAR, significantly improving perception capabilities. Considering that point-cloudbased methods suffer from sparse characteristic and lack rich semantic information, multimodal-fusion-based meth- 6835",
      "figure_captions": [
        {
          "bbox": [
            150.44400024414062,
            262.3100280761719,
            461.5601501464844,
            271.27642822265625
          ],
          "text": "Figure 1. Issues of existing communication-efficient collaborative perception methods."
        }
      ],
      "images": [
        {
          "image_id": "p2_cluster_001",
          "page": 2,
          "file_path": "images/page_0002_cluster_001.png",
          "bbox": [
            58.71398162841797,
            72.04438781738281,
            552.8811645507812,
            215.561767578125
          ],
          "caption": "a graph showing the number of people who have been diagnosed with cancer",
          "detailed_caption": "The image shows a computer screen with a graph and a bar chart depicting the number of people who have been diagnosed with cancer. The graph is accompanied by text that provides further information about the data being presented.",
          "ocr_text": "APQ.0.7 on OPV2V95Where2com Area 1.Low confidence due.to the absence of ground.COSDH90-Late Fusion-0.07 Mbps Area 2. Low confidence Due to occlusiondue to spartity of thepoint cloud APQ07 (%) 80Ego75-No limit< 27Mbps< 5Mbps< Mops Bandwidth Limit",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>APQ.0.7 on OPV2V95Where2comArea 1.Low confidence due.to the absence of ground.COSDH90-Late Fusion-0.07 MbpsArea 2. Low confidenceDue to occlusiondue to spartity of thepoint cloudAPQ07 (%)80Ego75-No limit< 27Mbps< 5Mbps< MopsBandwidth Limit</s>",
            "parsed": {
              "<OCR>": "APQ.0.7 on OPV2V95Where2comArea 1.Low confidence due.to the absence of ground.COSDH90-Late Fusion-0.07 MbpsArea 2. Low confidenceDue to occlusiondue to spartity of thepoint cloudAPQ07 (%)80Ego75-No limit< 27Mbps< 5Mbps< MopsBandwidth Limit"
            }
          },
          "paper_caption": "Figure 1. Issues of existing communication-efficient collaborative perception methods.",
          "paper_caption_bbox": [
            150.44400024414062,
            262.3100280761719,
            461.5601501464844,
            271.27642822265625
          ]
        }
      ]
    },
    {
      "page": 3,
      "text_raw": "ods [23, 41, 47] further enhance performance by leveraging\nthe complementary advantages of different modalities.\nHowever, these single-agent-based 3D object detection\nmethods are limited by sensor range and susceptible to oc-\nclusion, making them unable to detect objects that are fur-\nther away or completely occluded. This paper focuses on\ncollaborative-perception-based 3D object detection meth-\nods, which improve detection performance by supplement-\ning the limitations of single-agent detection with informa-\ntion from other agents.\n2.2. Collaborative Perception\nCollaborative perception can be categorized into early col-\nlaboration, intermediate collaboration, and late collabo-\nration based on the collaboration timing.\nEarly collab-\noration [2, 4, 42] shares raw perception data, providing\ngood perception accuracy but with high bandwidth. Late\ncollaboration [20, 36] shares perception results, signifi-\ncantly reducing bandwidth, but leads to a decline in per-\nception accuracy.\nIntermediate collaboration operates at\nthe feature level and can achieve a better trade-off be-\ntween accuracy and bandwidth by adjusting the interme-\ndiate features transmitted [3, 10, 11, 19, 21, 22, 32, 35‚Äì\n37], which is why it has been widely studied. Some of\nthis research has focused on improving perception accuracy.\nFCooper [3] and CoFF [10] used manual modeling to fuse\nmulti-agent features. Who2com [22] and When2com [21]\nperform selective communication and use attention-based\nfusion. V2VNet [32] and DiscoNet [19] employ communi-\ncation graph-based fusion methods. However, these meth-\nods typically transmit complete, uncompressed intermedi-\nate BEV features, leading to enormous bandwidth require-\nments, which makes them challenging to apply in practice.\nTo address the large bandwidth demand in collabora-\ntive perception, AttFuse [36] was the first to use autoen-\ncoders to compress intermediate features along the chan-\nnel dimension, which was later adopted by V2X-ViT [35],\nCoBEVT [37] and others. However, this method leads to\nsignificant accuracy degradation at high compression rates.\nWhere2comm [11] reduces bandwidth requirements by se-\nlecting sparse but important foreground regions for collab-\noration while maintaining perception accuracy. However,\nas bandwidth is further limited, perception accruacy still\nrapidly degrades, potentially even falling below the accu-\nracy of simple late collaboration methods. To address this\nissue, this paper proposes a hybrid collaborative method\nbased on both intermediate and late collaboration, which ef-\nficiently compensates for the intermediate collaborative re-\nsults using late collaboration under bandwidth constraints.\nFurthermore, we also adopt and improve methods based on\nauto-encoders and information selection to save bandwidth\nwhile maintaining accuracy.\n3. Method\n3.1. Problem Definition\nIn this paper, we consider the problem of collaborative per-\nception with N agents. Let Xi and yi represent the raw ob-\nservation and the corresponding ground truth supervision\nof the i-th agent, respectively, and let Pj‚Üíi be the message\nsent from agent j to agent i. In the collaborative perception,\nagent i aggregates its own observations and the messages\n{Pj‚Üíi}N\nj=1 sent from other agents to perform 3D object de-\ntection task. Our goal is to maximize the collaborative per-\nception 3D object detection accuracy while ensuring that\neach agent has a communication budget B:\nŒæŒ¶(B) = arg max\nŒ∏\nN\n‚àë\ni=1\ng(Œ¶Œ∏(Xi,{Pj‚Üíi}N\nj=1),yi),\n(1)\ns.t.\nN\n‚àë\nj=1\n|Pj‚Üíi| ‚â§B\n(2)\nwhere Œ¶Œ∏ represents the collaborative perception 3D object\ndetection model, Œ∏ denotes the model parameters, |Pj‚Üíi|\nrepresents the communication volume of the message sent\nfrom agent j to agent i, and g(¬∑,¬∑) denotes the 3D object\ndetection evaluation metric.\n3.2. Overall Architecture\nThe overall architecture of the proposed CoSDH is shown in\nFig. 2. Each agent first processes its locally observed point\ncloud Xi through a backbone network based on PointPil-\nlar [17] and a demand generator to obtain multi-scale BEV\nfeatures {F(l)\ni\n}l=1,2,...,L and a demand mask Di, respectively.\nConsidering the collaboration between Ego agent i and col-\nlaborating agent j, agent j generates a supply mask Sj from\nits multi-scale features {F(l)\nj }l=1,2,..,L via a supply genera-\ntor, and multiplies it element-wise with the received demand\nmatrix to obtain the supply-demand mask Mj‚Üíi. Agent j\nthen performs supply-demand-aware information selection\nby multiplying {F(l)\nj }l=1,2,..,L with Mj‚Üíi element-wise to\nobtain sparse spatial features {Z(l)\nj }l=1,2,..,L. Subsequently,\nagent j compresses the features through an autoencoder,\nsending the non-zero parts of the features along with their\ncorresponding coordinates as the message Pj‚Üíi to agent i.\nUpon receiving Pj‚Üíi, agent i first decodes the features to\nrestore their dimensions and then fuses them with its local\nfeatures {F(l)\ni\n}l=1,2,...,L across multiple scales to obtain the\nfused features { Àú\nF(l)\ni\n}l=1,2,...,L, which are then passed to the\ndetection head for intermediate collaborative detection re-\nsults Àúyi. Afterward, we apply confidence-aware late fusion:\nagent j filters and suppresses its own detection results yj\nbased on confidence and sends them to agent i for late fu-\nsion, yielding the final hybrid collaborative perception de-\ntection results ÀÜyi.\n6836\n",
      "text_clean": "ods [23, 41, 47] further enhance performance by leveraging the complementary advantages of different modalities. However, these single-agent-based 3D object detection methods are limited by sensor range and susceptible to occlusion, making them unable to detect objects that are further away or completely occluded. This paper focuses on collaborative-perception-based 3D object detection methods, which improve detection performance by supplementing the limitations of single-agent detection with information from other agents. 2.2. Collaborative Perception Collaborative perception can be categorized into early collaboration, intermediate collaboration, and late collaboration based on the collaboration timing. Early collaboration [2, 4, 42] shares raw perception data, providing good perception accuracy but with high bandwidth. Late collaboration [20, 36] shares perception results, significantly reducing bandwidth, but leads to a decline in perception accuracy. Intermediate collaboration operates at the feature level and can achieve a better trade-off between accuracy and bandwidth by adjusting the intermediate features transmitted [3, 10, 11, 19, 21, 22, 32, 35‚Äì 37], which is why it has been widely studied. Some of this research has focused on improving perception accuracy. FCooper [3] and CoFF [10] used manual modeling to fuse multi-agent features. Who2com [22] and When2com [21] perform selective communication and use attention-based fusion. V2VNet [32] and DiscoNet [19] employ communication graph-based fusion methods. However, these methods typically transmit complete, uncompressed intermediate BEV features, leading to enormous bandwidth requirements, which makes them challenging to apply in practice. To address the large bandwidth demand in collaborative perception, AttFuse [36] was the first to use autoencoders to compress intermediate features along the channel dimension, which was later adopted by V2X-ViT [35], CoBEVT [37] and others. However, this method leads to significant accuracy degradation at high compression rates. Where2comm [11] reduces bandwidth requirements by selecting sparse but important foreground regions for collaboration while maintaining perception accuracy. However, as bandwidth is further limited, perception accruacy still rapidly degrades, potentially even falling below the accuracy of simple late collaboration methods. To address this issue, this paper proposes a hybrid collaborative method based on both intermediate and late collaboration, which efficiently compensates for the intermediate collaborative results using late collaboration under bandwidth constraints. Furthermore, we also adopt and improve methods based on auto-encoders and information selection to save bandwidth while maintaining accuracy. 3. Method 3.1. Problem Definition In this paper, we consider the problem of collaborative perception with N agents. Let Xi and yi represent the raw observation and the corresponding ground truth supervision of the i-th agent, respectively, and let Pj‚Üíi be the message sent from agent j to agent i. In the collaborative perception, agent i aggregates its own observations and the messages {Pj‚Üíi}N j=1 sent from other agents to perform 3D object detection task. Our goal is to maximize the collaborative perception 3D object detection accuracy while ensuring that each agent has a communication budget B: ŒæŒ¶(B) = arg max Œ∏ N ‚àë i=1 g(Œ¶Œ∏(Xi,{Pj‚Üíi}N j=1),yi), (1) s.t. N ‚àë j=1 |Pj‚Üíi| ‚â§B (2) where Œ¶Œ∏ represents the collaborative perception 3D object detection model, Œ∏ denotes the model parameters, |Pj‚Üíi| represents the communication volume of the message sent from agent j to agent i, and g(¬∑,¬∑) denotes the 3D object detection evaluation metric. 3.2. Overall Architecture The overall architecture of the proposed CoSDH is shown in Fig. 2. Each agent first processes its locally observed point cloud Xi through a backbone network based on PointPillar [17] and a demand generator to obtain multi-scale BEV features {F(l) i }l=1,2,...,L and a demand mask Di, respectively. Considering the collaboration between Ego agent i and collaborating agent j, agent j generates a supply mask Sj from its multi-scale features {F(l) j }l=1,2,..,L via a supply generator, and multiplies it element-wise with the received demand matrix to obtain the supply-demand mask Mj‚Üíi. Agent j then performs supply-demand-aware information selection by multiplying {F(l) j }l=1,2,..,L with Mj‚Üíi element-wise to obtain sparse spatial features {Z(l) j }l=1,2,..,L. Subsequently, agent j compresses the features through an autoencoder, sending the non-zero parts of the features along with their corresponding coordinates as the message Pj‚Üíi to agent i. Upon receiving Pj‚Üíi, agent i first decodes the features to restore their dimensions and then fuses them with its local features {F(l) i }l=1,2,...,L across multiple scales to obtain the fused features { Àú F(l) i }l=1,2,...,L, which are then passed to the detection head for intermediate collaborative detection results Àúyi. Afterward, we apply confidence-aware late fusion: agent j filters and suppresses its own detection results yj based on confidence and sends them to agent i for late fusion, yielding the final hybrid collaborative perception detection results ÀÜyi. 6836",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 4,
      "text_raw": "ùëøùíä\n{ùë≠ùíã\n(ùíç)}\n{ùíÅùíã‚Üíùíä\n(ùíç) }\n{‡∑´\nùë≠ùíä\n(ùíç)}\n{ùë≠ùíä\n(ùíç)}\nSupply-Demand-Aware Information Selection\nMessage Compression\nDecoder\nEncoder\nùë∑ùíã‚Üíùíä\n‡∑•ùíöùíä\n‡∑ùùíöùíä\nConfidence-Aware Late Fusion\nBackbone\nDemand\nGenerator\nSupply\nGenerator\nùë´ùíä\nùë∫ùíã\nMulti-scale \nFusion\nDetector\nDetector\nùíöùíã\nLate \nFusion\nùë¥ùíã‚Üíùíä\nObservation\nMulti-scale features\nSupply mask\nDemand mask\nSelection mask\nùëøùíä\nùë≠ùíä\n(ùíç)\nùë∫ùíã\nùë´ùíä\nùë¥ùíã‚Üíùíä\nSpare features\nMessage\nFused features\nSingle detection results\nFused detection results\nùíÅùíä\n(ùíç)\nùë∑ùíã‚Üíùíä\nùíöùíã\n‡∑•ùíöùíä, ‡∑ùùíöùíä\n‡∑™\nùë≠ùíä\n(ùíç)\nEgo agent\nFilter and Suppress \nby Confidence\nFigure 2. The overall architecture of CoSDH. The Supply-Demand-Aware Information Selection module selects sparse but important\ninformation, which is then further compressed by the Message Compression module to achieve efficient communication. Confidence-\nAware Late Fusion compensates for the intermediate fusion detection results to improve accuracy.\n3.3. Supply-Demand-Aware Information Selection\nPrevious\nmethods\nsuch\nas\nWhere2comm\n[11]\nand\nHow2comm [40] generally use symmetric supply-demand\nrelationships to select sparse features for collaboration, be-\nlieving that areas with high foreground confidence in the\ncollaborating agent‚Äôs view should be provided, and con-\nversely, areas with low foreground confidence in the Ego\nagent‚Äôs view need collaboration. However, we believe that\nareas with low foreground confidence from the Ego‚Äôs per-\nspective can be divided into areas that are hard to observe\nand those that can be observed but belong to the back-\nground. The latter do not require collaboration. Based on\nthis insight, we propose a novel supply-demand-aware in-\nformation selection method.\nThe demand mask Di indicates where agent i needs in-\nformation from collaborating agents. Intuitively, the agent\nrequires information from areas that are distant or occluded,\nwhich have the common characteristic of having low point\ncloud density or no point cloud at all. For agent i, we con-\nsider using the number of point clouds in each pillar to\nrepresent point cloud density, and we map it to the range\n[0,1], i.e., Ai ‚àà[0,1]H√óW, where H and W represent the\nnumber of Pillars along the height and width dimensions.\nWe then select areas where the point cloud density is be-\nlow a threshold Œµa to obtain the demand mask for agent\ni, Di = Ai < Œµa ‚àà{0,1}H√óW. The demand mask indicates\nwhere agent i has poor perception and needs collaborative\ninformation from other agents. Filtering information from\nother agents using the demand mask not only helps save\nbandwidth but also avoids interference from other agents‚Äô\ninformation in well-perceived areas.\nFor the object detection task, foreground information is\nmore valuable. Providing sparse foreground features can\neffectively assist other agents in supplementing undetected\nConcat\nPFE\nLarger communication volume\nDet\na. Single-scale Fusion\nFeature before fusion\nFeature after fusion\nDet\nInsufficient fusion\nPFE\nb. Multi-scale Fusion but \nSingle-scale Compression\nFuse\nCompress\nConv\nDeconv\nc. Multi-scale Compression and Fusion (Ours)\nùëØ√ó ùëæ√ó ùë™\nùëØ\nùüê√ó ùëæ\nùüê√ó ùë™\nùëØ\nùüí√ó ùëæ\nùüí√ó ùüêùë™\nùëØ\nùüñ√ó ùëæ\nùüñ√ó ùüíùë™\nùëØ\nùüê√ó ùëæ\nùüê√ó ùüêùë™\nùëØ\nùüê√ó ùëæ\nùüê√ó ùüîùë™\nùëØ\nùüê√ó ùëæ\nùüê√ó ùüíùë™\nùëØ\nùüê√ó ùëæ\nùüê√ó ùüíùë™\nùëØ√ó ùëæ√ó ùë™\nFigure 3. Comparison of our multi-scale compression and fusion\nwith other methods. It can achieve thorough fusion with smaller\ncommunication volume.\nand incomplete targets while using less bandwidth. Follow-\ning previous work [11], we use the spatial confidence map\nCi ‚àà[0,1]H√óW output by the detection head to select po-\ntential foreground areas. We use a supply threshold Œµc to\nobtain the supply mask S(l)\ni\n= Ci > Œµc ‚àà{0,1}H√óW. By ad-\njusting the threshold, we can dynamically adjust the band-\nwidth used for collaborative perception to adapt to varying\ncommunication conditions.\nDuring collaboration, agent j generates a binary supply-\ndemand selection mask Mj‚Üíi = Di ‚äôSj ‚àà{0,1}H√óW based\non its supply mask Sj and agent i‚Äôs demand mask Di,\nand samples it to multiply element-wise with multi-scale\nBEV features {F(l)\nj }l=1,2,...,L, obtaining sparse features\n{Z(l)\nj‚Üíi}l=1,2,...,L. During communication, only the non-zero\nparts and their corresponding coordinates need to be trans-\nmitted.\n6837\n",
      "text_clean": "ùëøùíä {ùë≠ùíã (ùíç)} {ùíÅùíã‚Üíùíä (ùíç) } {‡∑´ ùë≠ùíä (ùíç)} {ùë≠ùíä (ùíç)} Supply-Demand-Aware Information Selection Message Compression Decoder Encoder ùë∑ùíã‚Üíùíä ‡∑•ùíöùíä ‡∑ùùíöùíä Confidence-Aware Late Fusion Backbone Demand Generator Supply Generator ùë´ùíä ùë∫ùíã Multi-scale Fusion Detector Detector ùíöùíã Late Fusion ùë¥ùíã‚Üíùíä Observation Multi-scale features Supply mask Demand mask Selection mask ùëøùíä ùë≠ùíä (ùíç) ùë∫ùíã ùë´ùíä ùë¥ùíã‚Üíùíä Spare features Message Fused features Single detection results Fused detection results ùíÅùíä (ùíç) ùë∑ùíã‚Üíùíä ùíöùíã ‡∑•ùíöùíä, ‡∑ùùíöùíä ‡∑™ ùë≠ùíä (ùíç) Ego agent Filter and Suppress by Confidence Figure 2. The overall architecture of CoSDH. The Supply-Demand-Aware Information Selection module selects sparse but important information, which is then further compressed by the Message Compression module to achieve efficient communication. ConfidenceAware Late Fusion compensates for the intermediate fusion detection results to improve accuracy. 3.3. Supply-Demand-Aware Information Selection Previous methods such as Where2comm [11] and How2comm [40] generally use symmetric supply-demand relationships to select sparse features for collaboration, believing that areas with high foreground confidence in the collaborating agent‚Äôs view should be provided, and conversely, areas with low foreground confidence in the Ego agent‚Äôs view need collaboration. However, we believe that areas with low foreground confidence from the Ego‚Äôs perspective can be divided into areas that are hard to observe and those that can be observed but belong to the background. The latter do not require collaboration. Based on this insight, we propose a novel supply-demand-aware information selection method. The demand mask Di indicates where agent i needs information from collaborating agents. Intuitively, the agent requires information from areas that are distant or occluded, which have the common characteristic of having low point cloud density or no point cloud at all. For agent i, we consider using the number of point clouds in each pillar to represent point cloud density, and we map it to the range [0,1], i.e., Ai ‚àà[0,1]H√óW, where H and W represent the number of Pillars along the height and width dimensions. We then select areas where the point cloud density is below a threshold Œµa to obtain the demand mask for agent i, Di = Ai < Œµa ‚àà{0,1}H√óW. The demand mask indicates where agent i has poor perception and needs collaborative information from other agents. Filtering information from other agents using the demand mask not only helps save bandwidth but also avoids interference from other agents‚Äô information in well-perceived areas. For the object detection task, foreground information is more valuable. Providing sparse foreground features can effectively assist other agents in supplementing undetected Concat PFE Larger communication volume Det a. Single-scale Fusion Feature before fusion Feature after fusion Det Insufficient fusion PFE b. Multi-scale Fusion but Single-scale Compression Fuse Compress Conv Deconv c. Multi-scale Compression and Fusion (Ours) ùëØ√ó ùëæ√ó ùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùë™ ùëØ ùüí√ó ùëæ ùüí√ó ùüêùë™ ùëØ ùüñ√ó ùëæ ùüñ√ó ùüíùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüêùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüîùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüíùë™ ùëØ ùüê√ó ùëæ ùüê√ó ùüíùë™ ùëØ√ó ùëæ√ó ùë™ Figure 3. Comparison of our multi-scale compression and fusion with other methods. It can achieve thorough fusion with smaller communication volume. and incomplete targets while using less bandwidth. Following previous work [11], we use the spatial confidence map Ci ‚àà[0,1]H√óW output by the detection head to select potential foreground areas. We use a supply threshold Œµc to obtain the supply mask S(l) i = Ci > Œµc ‚àà{0,1}H√óW. By adjusting the threshold, we can dynamically adjust the bandwidth used for collaborative perception to adapt to varying communication conditions. During collaboration, agent j generates a binary supplydemand selection mask Mj‚Üíi = Di ‚äôSj ‚àà{0,1}H√óW based on its supply mask Sj and agent i‚Äôs demand mask Di, and samples it to multiply element-wise with multi-scale BEV features {F(l) j }l=1,2,...,L, obtaining sparse features {Z(l) j‚Üíi}l=1,2,...,L. During communication, only the non-zero parts and their corresponding coordinates need to be transmitted. 6837",
      "figure_captions": [
        {
          "bbox": [
            58.5,
            248.77194213867188,
            553.5037231445312,
            280.1153564453125
          ],
          "text": "Figure 2. The overall architecture of CoSDH. The Supply-Demand-Aware Information Selection module selects sparse but important information, which is then further compressed by the Message Compression module to achieve efficient communication. Confidence- Aware Late Fusion compensates for the intermediate fusion detection results to improve accuracy."
        },
        {
          "bbox": [
            317.25,
            464.16998291015625,
            553.4967651367188,
            495.05438232421875
          ],
          "text": "Figure 3. Comparison of our multi-scale compression and fusion with other methods. It can achieve thorough fusion with smaller communication volume."
        }
      ],
      "images": [
        {
          "image_id": "p4_cluster_001",
          "page": 4,
          "file_path": "images/page_0004_cluster_001.png",
          "bbox": [
            71.97811126708984,
            185.79226684570312,
            110.02421569824219,
            236.4728546142578
          ],
          "caption": "a map with the words ego age on it",
          "detailed_caption": "The image shows a map with a blue circle in the center and the words \"ego age\" at the bottom. The blue circle is surrounded by a black border, giving the map a distinct look.",
          "ocr_text": "Ego age",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>Ego age</s>",
            "parsed": {
              "<OCR>": "Ego age"
            }
          },
          "paper_caption": "Figure 2. The overall architecture of CoSDH. The Supply-Demand-Aware Information Selection module selects sparse but important information, which is then further compressed by the Message Compression module to achieve efficient communication. Confidence- Aware Late Fusion compensates for the intermediate fusion detection results to improve accuracy.",
          "paper_caption_bbox": [
            58.5,
            248.77194213867188,
            553.5037231445312,
            280.1153564453125
          ]
        }
      ]
    },
    {
      "page": 5,
      "text_raw": "3.4. Message Compression and Fusion\nTo further reduce communication bandwidth, we also use\nautoencoders to compress intermediate features along the\nchannel dimension before communication, while making\nfurther improvements to existing compression and fusion\nschemes. As shown in Fig. 3, traditional single-scale fu-\nsion schemes [35, 36] only compress and fuse a single-layer\nBEV feature map before passing it to the detection head,\nresulting in insufficient fusion. CoAlign [25] proposed a\nsolution that performs layer-wise fusion on multi-scale fea-\ntures, addressing the issue of insufficient fusion. However,\nit compresses large-scale single-layer features extracted af-\nter Pillar Feature Extraction (PFE), which leads to higher\nbandwidth. Therefore, we propose a multi-scale compres-\nsion and fusion approach, where separate autoencoders are\ndesigned for each scale of features to perform compression,\nfollowed by layer-wise fusion. This approach enables more\nthorough fusion while using less bandwidth.\nFor message fusion, we use Max fusion, which has two\nmain advantages: i) it is computationally simple and effi-\ncient, with the complexity increasing linearly with the num-\nber of agents; ii) max fusion selects the maximum value of\nfeatures from multiple agents, achieving information com-\nplementarity. Considering the collaboration between Ego\nagent i and collaborating agent j, the process of message\ncompression and fusion can be expressed as:\nZ\n‚Ä≤(l)\nj‚Üíi = f (l)\nencode(Z(l)\nj‚Üíi) ‚ààR\nCl\nc0 √óHl√óWl\n(3)\nF(l)\nj‚Üíi = f (l)\ndecode(Z\n‚Ä≤(l)\nj‚Üíi) ‚ààRCl√óHl√óWl\n(4)\nF\n‚Ä≤(l)\nj‚Üíi = ftrans form(F(l)\nj‚Üíi,Œæ j‚Üíi) ‚ààRCl√óHl√óWl\n(5)\nÀú\nF(l)\ni\n= max(F(l)\ni\n,{F\n‚Ä≤(l)\nj‚Üíi}jÃ∏=i) ‚ààRCl√óHl√óWl\n(6)\nwhere Z(l)\nj‚Üíi ‚ààRCl√óHl√óWl represents the sparse features\nselected based on supply-demand relationships from the\nprevious stage, and Cl,Hl,Wl denote the channel, height,\nand width dimensions of the l-th layer feature map.\nf (l)\nencode, f (l)\ndecode represent the encoder and decoder of the l-th\nlayer autoencoder, c0 is the compression ratio in the channel\ndimension, and ftrans form represents coordinate transforma-\ntion. During communication, {Z\n‚Ä≤(l)\nj‚Üíi}l=1,2,..,L is first con-\nverted from float32 to float16, and then only the non-zero\nparts and corresponding coordinates are transmitted to save\nbandwidth. Upon receiving the message, agent i decodes\nthe feature dimensions, then aligns the features to its own\ncoordinate system using the coordinate transformation ma-\ntrix Œæj‚Üíi to obtain F\n‚Ä≤(l)\nj‚Üíi, and finally fuses its own features\nwith the collaborating agent‚Äôs features using max fusion to\nobtain the fused features\nÀú\nF(l)\ni\n.\nFigure 4. Our confidence-aware late fusion. It filters detection re-\nsults based on confidence and suppresses suboptimal results from\ncollaborative agents, improving overall detection accuracy.\n3.5. Confidence-Aware Late Fusion\nExisting collaborative perception methods focus on the sin-\ngular intermediate collaboration architecture and neglect\nthe advantages of late collaboration. As shown in Table 1,\nthe experimental results indicate that late fusion can demon-\nstrate acceptable perception accuracy with extremely low\nbandwidth on the OPV2V [36] and V2XSim [20] datasets.\nTherefore, late fusion can be used to compensate for the\nresults of intermediate fusion, achieving a better accuracy-\nbandwidth trade-off.\nNaive late collaboration methods [20, 36] directly merge\nresults from other agents and then apply NMS (Non-\nMaximum Suppression) to deduplicate and obtain final re-\nsults. This approach can effectively improve recall for col-\nlaborative perception object detection; however, during the\nmerging process, it may introduce some low-confidence\nfalse positives, and suboptimal detection results from col-\nlaborating agents may override the Ego agent‚Äôs detection\nresults, lowering precision and consequently decreasing fi-\nnal AP. The lower AP in the late fusion method in Table 1\non the DAIR-V2X [42] dataset is due to this issue.\nTo address this problem, we propose a new confidence-\naware late fusion method. As shown in Fig. 4, during late\nfusion, we first filter based on the confidence of the target\nboxes, discarding target boxes from other agents with con-\nfidence lower than Œµl. Furthermore, considering that if both\nthe Ego agent and other agents detect the same target, the\ndetection result from the Ego agent, which has undergone\nintermediate fusion, is of higher quality. Therefore, we sup-\npress the detection results from other agents to prevent their\nlower-quality results from degrading the Ego agent‚Äôs bet-\nter detection results. Specifically, before merging the de-\ntection results, we multiply the confidence of target boxes\nfrom other agents by a coefficient Œ≤ ‚àà(0,1).\n6838\n",
      "text_clean": "3.4. Message Compression and Fusion To further reduce communication bandwidth, we also use autoencoders to compress intermediate features along the channel dimension before communication, while making further improvements to existing compression and fusion schemes. As shown in Fig. 3, traditional single-scale fusion schemes [35, 36] only compress and fuse a single-layer BEV feature map before passing it to the detection head, resulting in insufficient fusion. CoAlign [25] proposed a solution that performs layer-wise fusion on multi-scale features, addressing the issue of insufficient fusion. However, it compresses large-scale single-layer features extracted after Pillar Feature Extraction (PFE), which leads to higher bandwidth. Therefore, we propose a multi-scale compression and fusion approach, where separate autoencoders are designed for each scale of features to perform compression, followed by layer-wise fusion. This approach enables more thorough fusion while using less bandwidth. For message fusion, we use Max fusion, which has two main advantages: i) it is computationally simple and efficient, with the complexity increasing linearly with the number of agents; ii) max fusion selects the maximum value of features from multiple agents, achieving information complementarity. Considering the collaboration between Ego agent i and collaborating agent j, the process of message compression and fusion can be expressed as: Z ‚Ä≤(l) j‚Üíi = f (l) encode(Z(l) j‚Üíi) ‚ààR Cl c0 √óHl√óWl (3) F(l) j‚Üíi = f (l) decode(Z ‚Ä≤(l) j‚Üíi) ‚ààRCl√óHl√óWl (4) F ‚Ä≤(l) j‚Üíi = ftrans form(F(l) j‚Üíi,Œæ j‚Üíi) ‚ààRCl√óHl√óWl (5) Àú F(l) i = max(F(l) i ,{F ‚Ä≤(l) j‚Üíi}jÃ∏=i) ‚ààRCl√óHl√óWl (6) where Z(l) j‚Üíi ‚ààRCl√óHl√óWl represents the sparse features selected based on supply-demand relationships from the previous stage, and Cl,Hl,Wl denote the channel, height, and width dimensions of the l-th layer feature map. f (l) encode, f (l) decode represent the encoder and decoder of the l-th layer autoencoder, c0 is the compression ratio in the channel dimension, and ftrans form represents coordinate transformation. During communication, {Z ‚Ä≤(l) j‚Üíi}l=1,2,..,L is first converted from float32 to float16, and then only the non-zero parts and corresponding coordinates are transmitted to save bandwidth. Upon receiving the message, agent i decodes the feature dimensions, then aligns the features to its own coordinate system using the coordinate transformation matrix Œæj‚Üíi to obtain F ‚Ä≤(l) j‚Üíi, and finally fuses its own features with the collaborating agent‚Äôs features using max fusion to obtain the fused features Àú F(l) i . Figure 4. Our confidence-aware late fusion. It filters detection results based on confidence and suppresses suboptimal results from collaborative agents, improving overall detection accuracy. 3.5. Confidence-Aware Late Fusion Existing collaborative perception methods focus on the singular intermediate collaboration architecture and neglect the advantages of late collaboration. As shown in Table 1, the experimental results indicate that late fusion can demonstrate acceptable perception accuracy with extremely low bandwidth on the OPV2V [36] and V2XSim [20] datasets. Therefore, late fusion can be used to compensate for the results of intermediate fusion, achieving a better accuracybandwidth trade-off. Naive late collaboration methods [20, 36] directly merge results from other agents and then apply NMS (NonMaximum Suppression) to deduplicate and obtain final results. This approach can effectively improve recall for collaborative perception object detection; however, during the merging process, it may introduce some low-confidence false positives, and suboptimal detection results from collaborating agents may override the Ego agent‚Äôs detection results, lowering precision and consequently decreasing final AP. The lower AP in the late fusion method in Table 1 on the DAIR-V2X [42] dataset is due to this issue. To address this problem, we propose a new confidenceaware late fusion method. As shown in Fig. 4, during late fusion, we first filter based on the confidence of the target boxes, discarding target boxes from other agents with confidence lower than Œµl. Furthermore, considering that if both the Ego agent and other agents detect the same target, the detection result from the Ego agent, which has undergone intermediate fusion, is of higher quality. Therefore, we suppress the detection results from other agents to prevent their lower-quality results from degrading the Ego agent‚Äôs better detection results. Specifically, before merging the detection results, we multiply the confidence of target boxes from other agents by a coefficient Œ≤ ‚àà(0,1). 6838",
      "figure_captions": [
        {
          "bbox": [
            317.25,
            228.71597290039062,
            553.4967041015625,
            259.600341796875
          ],
          "text": "Figure 4. Our confidence-aware late fusion. It filters detection re- sults based on confidence and suppresses suboptimal results from collaborative agents, improving overall detection accuracy."
        }
      ],
      "images": [
        {
          "image_id": "p5_cluster_001",
          "page": 5,
          "file_path": "images/page_0005_cluster_001.png",
          "bbox": [
            317.25,
            72.00425720214844,
            559.783447265625,
            217.88201904296875
          ],
          "caption": "a diagram showing the different types of fusion results",
          "detailed_caption": "The image shows a flowchart with text written on it, depicting the results of an intermediate fusion test and a naive late fusion API. The flowchart is composed of several boxes connected by arrows, each box representing a different stage of the test. The text on the flowchart provides further details about the results, such as the type of fusion and the intensity of the results.",
          "ocr_text": "Intermediate Fusion Result Naive Late Fusion (AP.) 0.90.30.090.90.0.2 (Lowconfidence FP) 0.090.80.9 (Sub-optimal result) 0, 90.70.90Ego predict0.70suppress0.00.9-0.7Coll predict0, 9filter Ground truth0.20.8Confidence-Aware Late Fusion (AP)",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>Intermediate Fusion ResultNaive Late Fusion (AP.)0.90.30.090.90.0.2 (Lowconfidence FP)0.090.80.9 (Sub-optimal result)0,90.70.90Ego predict0.70suppress0.00.9-0.7Coll predict0,9filterGround truth0.20.8Confidence-Aware Late Fusion (AP)</s>",
            "parsed": {
              "<OCR>": "Intermediate Fusion ResultNaive Late Fusion (AP.)0.90.30.090.90.0.2 (Lowconfidence FP)0.090.80.9 (Sub-optimal result)0,90.70.90Ego predict0.70suppress0.00.9-0.7Coll predict0,9filterGround truth0.20.8Confidence-Aware Late Fusion (AP)"
            }
          },
          "paper_caption": "Figure 4. Our confidence-aware late fusion. It filters detection re- sults based on confidence and suppresses suboptimal results from collaborative agents, improving overall detection accuracy.",
          "paper_caption_bbox": [
            317.25,
            228.71597290039062,
            553.4967041015625,
            259.600341796875
          ]
        }
      ]
    },
    {
      "page": 6,
      "text_raw": "Setting\nMethod\nOPV2V [36]\nV2XSim [20]\nDAIR-V2X [42]\nAP@0.5‚Üë\nAP@0.7‚Üë\nBD‚Üì\nAP@0.5‚Üë\nAP@0.7‚Üë\nBD‚Üì\nAP@0.5‚Üë\nAP@0.7‚Üë\nBD‚Üì\nBasic\nNo Fusion\n79.78%\n67.16%\n0.0 Mbps\n70.31%\n58.55%\n0.0 Mbps\n66.51%\n55.46%\n0.0 Mbps\nEarly Fusion\n95.05%\n88.98%\n83.1 Mbps\n95.68%\n88.03%\n55.5 Mbps\n74.52%\n59.22%\n50.2 Mbps\nLate Fusion\n94.70%\n87.18%\n0.2 Mbps\n86.88%\n78.13%\n0.1 Mbps\n67.86%\n50.47%\n0.2 Mbps\nNo Limit\nWhen2com [21]\n91.75%\n81.77%\n1,320.0 Mbps\n72.65%\n62.92%\n250.0 Mbps\n64.08%\n49.14%\n984.4 Mbps\nFCooper [3]\n90.06%\n74.03%\n2,640.0 Mbps\n72.96%\n57.61%\n500.0 Mbps\n74.58%\n56.47%\n1,968.8 Mbps\nAttFuse [36]\n94.31%\n82.03%\n2,640.0 Mbps\n78.06%\n64.84%\n500.0 Mbps\n73.80%\n56.86%\n1,968.8 Mbps\nV2VNet [32]\n96.66%\n92.44%\n5,280.0 Mbps\n88.97%\n85.18%\n1,000.0 Mbps\n66.63%\n47.39%\n3,937.5 Mbps\nDiscoNet [19]\n90.93%\n78.90%\n2,640.0 Mbps\n77.34%\n68.77%\n500.0 Mbps\n73.58%\n58.45%\n1,968.8 Mbps\nV2XViT [35]\n95.87%\n89.88%\n2,640.0 Mbps\n89.01%\n80.26%\n500.0 Mbps\n76.68%\n57.57%\n1,920.0 Mbps\nWhere2comm [11]\n95.59%\n91.39%\n48.7 Mbps\n88.18%\n83.66%\n27.5 Mbps\n76.13%\n60.16%\n172.3 Mbps\nCoAlign [25]\n96.63%\n92.63%\n2,640.0 Mbps\n88.87%\n85.23%\n500.0 Mbps\n78.06%\n63.09%\n1,968.8 Mbps\nCoSDH\n96.83%\n92.99%\n13.4 Mbps\n89.23%\n86.31%\n1.1 Mbps\n76.75%\n63.85%\n7.1 Mbps\nBD ‚â§6.75 Mbps\nWhen2com [21]\n79.97%\n50.88%\n5.2 Mbps\n62.02%\n43.37%\n4.0 Mbps\n62.37%\n44.01%\n3.8 Mbps\nFCooper [3]\n90.37%\n72.92%\n5.2 Mbps\n76.52%\n61.83%\n4.0 Mbps\n67.71%\n47.65%\n3.8 Mbps\nAttFuse [36]\n93.45%\n80.02%\n5.2 Mbps\n84.58%\n71.36%\n4.0 Mbps\n71.06%\n49.57%\n3.8 Mbps\nV2VNet [32]\n95.71%\n87.16%\n5.2 Mbps\n85.66%\n73.72%\n4.0 Mbps\n66.49%\n45.61%\n3.8 Mbps\nDiscoNet [19]\n90.00%\n76.72%\n5.2 Mbps\n78.04%\n68.18%\n4.0 Mbps\n70.83%\n53.40%\n3.8 Mbps\nV2XViT [35]\n95.85%\n87.13%\n5.2 Mbps\n88.94%\n81.47%\n4.0 Mbps\n71.00%\n52.78%\n3.8 Mbps\nWhere2comm [11]\n94.91%\n89.86%\n5.4 Mbps\n87.51%\n82.12%\n4.7 Mbps\n74.98%\n59.51%\n5.3 Mbps\nCoAlign [25]\n94.10%\n85.99%\n5.2 Mbps\n88.01%\n83.97%\n4.0 Mbps\n75.26%\n60.19%\n3.8 Mbps\nCoSDH\n96.75%\n92.92%\n2.0 Mbps\n89.23%\n86.31%\n1.1 Mbps\n76.47%\n63.76%\n1.4 Mbps\nTable 1. Comparison of detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20], and DAIR-V2X [42]\ndatasets. ‚ÄúBD‚Äù represents the bandwidth required for each collaborative agent, assuming Ego agent collaborates with up to 4 agents and\nthe detection frequency is 10Hz. ‚ÄúBD‚â§6.75 Mbps‚Äù is used to simulate real-world communication limits, assuming a total communication\nbandwidth of 27 Mbps, with each collaborating agent‚Äôs bandwidth consumption limited to less than 6.75 Mbps. For intermediate collabo-\nration methods without information selection, we provide a compressed version using autoencoders to meet the bandwidth constraints. For\nintermediate collaboration methods with information selection, the selection ratio is adjusted to meet the bandwidth constraints.\n4. Experiments\n4.1. Datasets and Experimental Settings\nDatasets. We evaluate the proposed CoSDH against other\nmethods on three different collaborative perception datasets\n(OPV2V [36], V2XSim [20], and DAIR-V2X [42]) for\nLiDAR-based 3D object detection.\nThe datasets include\nboth simulated and real-world scenarios, and cover two\ntypes of collaboration: V2V (Vehicle to Vehicle) and V2I\n(Vehicle to Infrastructure).\nEvaluation Metrics We use the average precision (AP)\nwith intersection-over-union (IoU) thresholds of 0.5 and 0.7\nto evaluate the performance of different methods on 3D ob-\nject detection. We assume the target detection frequency\nis 10Hz and calculate the communication bandwidth based\non the average data transmitted by each collaborative agent\nto the Ego agent, in order to evaluate the communication\ncost of different methods.\nSpecifically, we consider the\nbandwidth limitations in real-world collaborative percep-\ntion scenarios, setting the vehicle‚Äôs communication rate to\n27 Mbps [1, 24]. Considering the typical case where the\nEgo agent collaborates with up to 4 other agents [36], the\nbandwidth limit for each collaborative agent is 27/4 = 6.75\nMbps.\nImplementation Our experiments are based on the\nOpenCOOD [36] framework, using PointPillar [17] as the\nencoder with a grid size of (0.4m,0.4m), and a maximum of\n32 points per Pillar. For our method, we set the number of\nintermediate feature layers to L = 3, the demand threshold\nŒµa = 4/32 = 0.125, and the supply threshold Œµc = 0.01. For\nthe OPV2V [36] and DAIR-V2X [42] datasets, the com-\npression rate is c0 = 16, and for the V2XSim [20] dataset,\nthe compression rate is c0 = 8 due to its smaller percep-\ntion range and lower inherent bandwidth requirement. The\nlate fusion threshold is set to Œµl = 0.3, with a suppression\ncoefficient Œ≤ = 0.9 for OPV2V and V2XSim dataset and\nŒ≤ = 0.8 for DAIR-V2X dataset because of its difficulty.\nIn late fusion, the dense prediction results before NMS are\ntransmitted, as this reduces the computational burden on the\ncollaborating agents. The experiments use the Adam [14]\noptimizer, with an initial learning rate set between 0.0001\nand 0.002 based on the model‚Äôs testing complexity to en-\nsure proper training. The maximum number of collaborat-\ning agents is set to 5. The number of training epochs is\nset to 40 to ensure model convergence. Other experimental\nparameters are kept consistent with the OpenCOOD frame-\nwork. All methods are trained on four NVIDIA GeForce\nRTX 3090 GPUs.\n4.2. Quantitative Evaluation\nBenchmark Comparison. Table 1 presents the collabora-\ntive 3D object detection accuracy and required bandwidth\nof the proposed CoSDH\ncompared to previous methods\nacross different datasets. Experimental results show that,\nwith the default uncompressed settings, CoSDH achieves\nthe highest accuracy on the OPV2V [36] and V2XSim [20]\ndatasets while requiring only about 1/100 to 1/1000 of the\nbandwidth compared to other non-communication-efficient\n6839\n",
      "text_clean": "Setting Method OPV2V [36] V2XSim [20] DAIR-V2X [42] AP@0.5‚Üë AP@0.7‚Üë BD‚Üì AP@0.5‚Üë AP@0.7‚Üë BD‚Üì AP@0.5‚Üë AP@0.7‚Üë BD‚Üì Basic No Fusion 79.78% 67.16% 0.0 Mbps 70.31% 58.55% 0.0 Mbps 66.51% 55.46% 0.0 Mbps Early Fusion 95.05% 88.98% 83.1 Mbps 95.68% 88.03% 55.5 Mbps 74.52% 59.22% 50.2 Mbps Late Fusion 94.70% 87.18% 0.2 Mbps 86.88% 78.13% 0.1 Mbps 67.86% 50.47% 0.2 Mbps No Limit When2com [21] 91.75% 81.77% 1,320.0 Mbps 72.65% 62.92% 250.0 Mbps 64.08% 49.14% 984.4 Mbps FCooper [3] 90.06% 74.03% 2,640.0 Mbps 72.96% 57.61% 500.0 Mbps 74.58% 56.47% 1,968.8 Mbps AttFuse [36] 94.31% 82.03% 2,640.0 Mbps 78.06% 64.84% 500.0 Mbps 73.80% 56.86% 1,968.8 Mbps V2VNet [32] 96.66% 92.44% 5,280.0 Mbps 88.97% 85.18% 1,000.0 Mbps 66.63% 47.39% 3,937.5 Mbps DiscoNet [19] 90.93% 78.90% 2,640.0 Mbps 77.34% 68.77% 500.0 Mbps 73.58% 58.45% 1,968.8 Mbps V2XViT [35] 95.87% 89.88% 2,640.0 Mbps 89.01% 80.26% 500.0 Mbps 76.68% 57.57% 1,920.0 Mbps Where2comm [11] 95.59% 91.39% 48.7 Mbps 88.18% 83.66% 27.5 Mbps 76.13% 60.16% 172.3 Mbps CoAlign [25] 96.63% 92.63% 2,640.0 Mbps 88.87% 85.23% 500.0 Mbps 78.06% 63.09% 1,968.8 Mbps CoSDH 96.83% 92.99% 13.4 Mbps 89.23% 86.31% 1.1 Mbps 76.75% 63.85% 7.1 Mbps BD ‚â§6.75 Mbps When2com [21] 79.97% 50.88% 5.2 Mbps 62.02% 43.37% 4.0 Mbps 62.37% 44.01% 3.8 Mbps FCooper [3] 90.37% 72.92% 5.2 Mbps 76.52% 61.83% 4.0 Mbps 67.71% 47.65% 3.8 Mbps AttFuse [36] 93.45% 80.02% 5.2 Mbps 84.58% 71.36% 4.0 Mbps 71.06% 49.57% 3.8 Mbps V2VNet [32] 95.71% 87.16% 5.2 Mbps 85.66% 73.72% 4.0 Mbps 66.49% 45.61% 3.8 Mbps DiscoNet [19] 90.00% 76.72% 5.2 Mbps 78.04% 68.18% 4.0 Mbps 70.83% 53.40% 3.8 Mbps V2XViT [35] 95.85% 87.13% 5.2 Mbps 88.94% 81.47% 4.0 Mbps 71.00% 52.78% 3.8 Mbps Where2comm [11] 94.91% 89.86% 5.4 Mbps 87.51% 82.12% 4.7 Mbps 74.98% 59.51% 5.3 Mbps CoAlign [25] 94.10% 85.99% 5.2 Mbps 88.01% 83.97% 4.0 Mbps 75.26% 60.19% 3.8 Mbps CoSDH 96.75% 92.92% 2.0 Mbps 89.23% 86.31% 1.1 Mbps 76.47% 63.76% 1.4 Mbps Table 1. Comparison of detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20], and DAIR-V2X [42] datasets. ‚ÄúBD‚Äù represents the bandwidth required for each collaborative agent, assuming Ego agent collaborates with up to 4 agents and the detection frequency is 10Hz. ‚ÄúBD‚â§6.75 Mbps‚Äù is used to simulate real-world communication limits, assuming a total communication bandwidth of 27 Mbps, with each collaborating agent‚Äôs bandwidth consumption limited to less than 6.75 Mbps. For intermediate collaboration methods without information selection, we provide a compressed version using autoencoders to meet the bandwidth constraints. For intermediate collaboration methods with information selection, the selection ratio is adjusted to meet the bandwidth constraints. 4. Experiments 4.1. Datasets and Experimental Settings Datasets. We evaluate the proposed CoSDH against other methods on three different collaborative perception datasets (OPV2V [36], V2XSim [20], and DAIR-V2X [42]) for LiDAR-based 3D object detection. The datasets include both simulated and real-world scenarios, and cover two types of collaboration: V2V (Vehicle to Vehicle) and V2I (Vehicle to Infrastructure). Evaluation Metrics We use the average precision (AP) with intersection-over-union (IoU) thresholds of 0.5 and 0.7 to evaluate the performance of different methods on 3D object detection. We assume the target detection frequency is 10Hz and calculate the communication bandwidth based on the average data transmitted by each collaborative agent to the Ego agent, in order to evaluate the communication cost of different methods. Specifically, we consider the bandwidth limitations in real-world collaborative perception scenarios, setting the vehicle‚Äôs communication rate to 27 Mbps [1, 24]. Considering the typical case where the Ego agent collaborates with up to 4 other agents [36], the bandwidth limit for each collaborative agent is 27/4 = 6.75 Mbps. Implementation Our experiments are based on the OpenCOOD [36] framework, using PointPillar [17] as the encoder with a grid size of (0.4m,0.4m), and a maximum of 32 points per Pillar. For our method, we set the number of intermediate feature layers to L = 3, the demand threshold Œµa = 4/32 = 0.125, and the supply threshold Œµc = 0.01. For the OPV2V [36] and DAIR-V2X [42] datasets, the compression rate is c0 = 16, and for the V2XSim [20] dataset, the compression rate is c0 = 8 due to its smaller perception range and lower inherent bandwidth requirement. The late fusion threshold is set to Œµl = 0.3, with a suppression coefficient Œ≤ = 0.9 for OPV2V and V2XSim dataset and Œ≤ = 0.8 for DAIR-V2X dataset because of its difficulty. In late fusion, the dense prediction results before NMS are transmitted, as this reduces the computational burden on the collaborating agents. The experiments use the Adam [14] optimizer, with an initial learning rate set between 0.0001 and 0.002 based on the model‚Äôs testing complexity to ensure proper training. The maximum number of collaborating agents is set to 5. The number of training epochs is set to 40 to ensure model convergence. Other experimental parameters are kept consistent with the OpenCOOD framework. All methods are trained on four NVIDIA GeForce RTX 3090 GPUs. 4.2. Quantitative Evaluation Benchmark Comparison. Table 1 presents the collaborative 3D object detection accuracy and required bandwidth of the proposed CoSDH compared to previous methods across different datasets. Experimental results show that, with the default uncompressed settings, CoSDH achieves the highest accuracy on the OPV2V [36] and V2XSim [20] datasets while requiring only about 1/100 to 1/1000 of the bandwidth compared to other non-communication-efficient 6839",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 7,
      "text_raw": "CoSDH\nLate Fusion\nWhere2comm\nDiscoNet\nV2VNet\nV2X-ViT\nCoAlign\nEarly Fusion\nDiscoNet\nCoAlign\nV2X-ViT\nV2VNet\nEarly Fusion\nWhere2comm\nLate Fusion\nCoSDH\nNo Fusion\nNo Fusion\nNo Fusion\nNo Fusion\nNo Fusion\nNo Fusion\nV2VNet\nV2VNet\nDiscoNet\nDiscoNet\nDiscoNet\nEarly Fusion\nEarly Fusion\nEarly Fusion\nV2X-ViT\nV2VNet\nCoAlign\nCoSDH\nLate Fusion\nWhere2comm\nWhere2comm\nWhere2comm\nWhere2comm\nLate Fusion\nLate Fusion\nCoSDH\nCoSDH\nCoAlign\nCoAlign\nV2X-ViT\nV2X-ViT\nDiscoNet\nCoSDH\nCoAlign\nV2VNet\nEarly Fusion\nLate Fusion\nReal-world limitation\nReal-world limitation\nReal-world limitation\nReal-world limitation\nReal-world limitation\nReal-world limitation\nV2X-ViT\nFigure 5. Comparison of the trade-off between detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20] and\nDAIR-V2X [42] datasets, CoSDH achieves the best accuracy-bandwidth trade-off. The real-world limitation refers to the total bandwidth\nlimit of 27 Mbps, which means that each collaborative agent does not exceed 6.75 Mbps.\nmethods. Although the AP@0.5 of CoSDH on the DAIR-\nV2X [42] dataset is slightly lower than that of CoAlign [25],\nit achieves a higher AP@0.7 with only about 1/300 of the\nbandwidth. Both our CoSDH and Where2comm [11] are\ncommunication-efficient methods based on information se-\nlection, which enable dynamic accuracy-bandwidth trade-\noffs by adjusting the selection ratio. The table shows ac-\ncuracy at a specific bandwidth, and a more detailed com-\nparison of the accuracy-bandwidth curves is provided in the\n‚ÄúAccuracy-Bandwidth Trade-Off Comparison‚Äù part.\nFurthermore, we simulate real-world communication\nrate limitations.\nResult shows that CoSDH\nachieves\nimprovements in AP@0.7 of 3.06%/2.34%/3.75% on\nOPV2V/V2XSim/DAIR-V2X compared to the previous\nbest methods, while using less bandwidth. When compar-\ning the scenarios with and without bandwidth limitations\non the OPV2V and DAIR-V2X datasets, we found that\nCoSDH achieves less than a 0.3% decrease in AP under\nconditions where bandwidth is reduced by 80% to 85%,\nexhibiting less accuracy degradation than Where2comm.\nMost other methods also show varying degrees of accu-\nracy degradation under bandwidth constraints.\nInterest-\ningly, under bandwidth limitations, some methods show an\nimprovement in accuracy on certain datasets after using au-\ntoencoders to compress intermediate features. For exam-\nple, FCooper [3] shows improved accuracy on the V2XSim\ndataset, which may be due to the model‚Äôs initially poor per-\nformance. The compressed version, with the added autoen-\ncoder, increases the model‚Äôs parameter count, thereby en-\nriching its expressive capability.\nAccuracy-Bandwidth Trade-Off Comparison. Fig. 5\nshows the accuracy-bandwidth trade-off of the proposed\nCoSDH\nand previous advanced methods across differ-\nent datasets.\nWhen the bandwidth grater than 1 Mbps,\nWhere2comm [11] demonstrates a good enough accuracy-\nbandwidth trade-off, maintaining high detection accuracy\nas bandwidth decreases. However, as bandwidth further de-\ncreases, its accuracy quickly declines, even falling below\nthat of late fusion method. CoSDH can leverage late fu-\nsion at low bandwidths to maintain high accuracy, achiev-\ning a better performance-bandwidth trade-off. Notably, on\nthe DAIR-V2X dataset, the late fusion method performs\nworse than the no-fusion case because the naive late fusion\nmethod unselectively merges detection results from other\nagents, introducing a large number of low-quality detec-\ntions. CoSDH uses a confidence-aware late fusion method\nto select higher-quality detection results, improving this sit-\nuation and enhancing perception accuracy. We also observe\nthat the AP@0.7 of CoSDH on the V2XSim dataset ini-\ntially increases slightly as bandwidth decreases before sub-\nsequently declining, indicating that selecting key collabora-\ntion areas can help reduce interference from other regions to\nsome extent, thereby slightly improving detection accuracy.\n6840\n",
      "text_clean": "CoSDH Late Fusion Where2comm DiscoNet V2VNet V2X-ViT CoAlign Early Fusion DiscoNet CoAlign V2X-ViT V2VNet Early Fusion Where2comm Late Fusion CoSDH No Fusion No Fusion No Fusion No Fusion No Fusion No Fusion V2VNet V2VNet DiscoNet DiscoNet DiscoNet Early Fusion Early Fusion Early Fusion V2X-ViT V2VNet CoAlign CoSDH Late Fusion Where2comm Where2comm Where2comm Where2comm Late Fusion Late Fusion CoSDH CoSDH CoAlign CoAlign V2X-ViT V2X-ViT DiscoNet CoSDH CoAlign V2VNet Early Fusion Late Fusion Real-world limitation Real-world limitation Real-world limitation Real-world limitation Real-world limitation Real-world limitation V2X-ViT Figure 5. Comparison of the trade-off between detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20] and DAIR-V2X [42] datasets, CoSDH achieves the best accuracy-bandwidth trade-off. The real-world limitation refers to the total bandwidth limit of 27 Mbps, which means that each collaborative agent does not exceed 6.75 Mbps. methods. Although the AP@0.5 of CoSDH on the DAIRV2X [42] dataset is slightly lower than that of CoAlign [25], it achieves a higher AP@0.7 with only about 1/300 of the bandwidth. Both our CoSDH and Where2comm [11] are communication-efficient methods based on information selection, which enable dynamic accuracy-bandwidth tradeoffs by adjusting the selection ratio. The table shows accuracy at a specific bandwidth, and a more detailed comparison of the accuracy-bandwidth curves is provided in the ‚ÄúAccuracy-Bandwidth Trade-Off Comparison‚Äù part. Furthermore, we simulate real-world communication rate limitations. Result shows that CoSDH achieves improvements in AP@0.7 of 3.06%/2.34%/3.75% on OPV2V/V2XSim/DAIR-V2X compared to the previous best methods, while using less bandwidth. When comparing the scenarios with and without bandwidth limitations on the OPV2V and DAIR-V2X datasets, we found that CoSDH achieves less than a 0.3% decrease in AP under conditions where bandwidth is reduced by 80% to 85%, exhibiting less accuracy degradation than Where2comm. Most other methods also show varying degrees of accuracy degradation under bandwidth constraints. Interestingly, under bandwidth limitations, some methods show an improvement in accuracy on certain datasets after using autoencoders to compress intermediate features. For example, FCooper [3] shows improved accuracy on the V2XSim dataset, which may be due to the model‚Äôs initially poor performance. The compressed version, with the added autoencoder, increases the model‚Äôs parameter count, thereby enriching its expressive capability. Accuracy-Bandwidth Trade-Off Comparison. Fig. 5 shows the accuracy-bandwidth trade-off of the proposed CoSDH and previous advanced methods across different datasets. When the bandwidth grater than 1 Mbps, Where2comm [11] demonstrates a good enough accuracybandwidth trade-off, maintaining high detection accuracy as bandwidth decreases. However, as bandwidth further decreases, its accuracy quickly declines, even falling below that of late fusion method. CoSDH can leverage late fusion at low bandwidths to maintain high accuracy, achieving a better performance-bandwidth trade-off. Notably, on the DAIR-V2X dataset, the late fusion method performs worse than the no-fusion case because the naive late fusion method unselectively merges detection results from other agents, introducing a large number of low-quality detections. CoSDH uses a confidence-aware late fusion method to select higher-quality detection results, improving this situation and enhancing perception accuracy. We also observe that the AP@0.7 of CoSDH on the V2XSim dataset initially increases slightly as bandwidth decreases before subsequently declining, indicating that selecting key collaboration areas can help reduce interference from other regions to some extent, thereby slightly improving detection accuracy. 6840",
      "figure_captions": [
        {
          "bbox": [
            58.49999237060547,
            343.1309814453125,
            553.4991455078125,
            374.015380859375
          ],
          "text": "Figure 5. Comparison of the trade-off between detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20] and DAIR-V2X [42] datasets, CoSDH achieves the best accuracy-bandwidth trade-off. The real-world limitation refers to the total bandwidth limit of 27 Mbps, which means that each collaborative agent does not exceed 6.75 Mbps."
        }
      ],
      "images": [
        {
          "image_id": "p7_cluster_001",
          "page": 7,
          "file_path": "images/page_0007_cluster_001.png",
          "bbox": [
            224.10418701171875,
            72.24565124511719,
            549.7529296875,
            331.9622497558594
          ],
          "caption": "a graph showing the number of different types of fusion",
          "detailed_caption": "The image shows a plot of the real-world limitations of the v2x-vit and v2-v2x, with different colors and text on a white background. The plot displays the correlation between the two variables, providing a visual representation of the data.",
          "ocr_text": "V2Sim Early Fusion DAIR-V2XCo Align95-COSDHCos DHV2x-VIT75-VC2X-VITE90-Cos DHCOSDHAv2XVIT70-Co Align85-C0Align Disco Net V2VNet70-Early Fusion80-APQ/0.5 (%) V2ZVNet75-Disc Net65-Where2Comm No Fusion70-Real-world limitation101520Real-World limitation1515Bandwidth (lq2 Kbps) 20V2Sim Dair-VZXCOSDHEarly Fusion Co Al IGN85-Co Algn80-60-Early Fusion70-Late Fusion V2NNet Disc Net75V2Net5-5-No Fusion Disco Net Where2comm V2KVIT65-AQ/O.7 (%) Where2comm50-Late Fusion60-No Fusion V2/Net Real-worl limitation5-10 Real-world limitation1515202050-1015Bandswidth (loq2 KBps) Bandwidth [loq3 Kbps]",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>V2SimEarly FusionDAIR-V2XCoAlign95-COSDHCosDHV2x-VIT75-VC2X-VITE90-CosDHCOSDHAv2XVIT70-CoAlign85-C0AlignDiscoNetV2VNet70-Early Fusion80-APQ/0.5 (%)V2ZVNet75-DiscNet65-Where2CommNo Fusion70-Real-world limitation101520Real-World limitation1515Bandwidth (lq2 Kbps)20V2SimDair-VZXCOSDHEarly FusionCoAlIGN85-CoAlgn80-60-Early Fusion70-Late FusionV2NNetDiscNet75V2Net5-5-No FusionDiscoNetWhere2commV2KVIT65-AQ/O.7 (%)Where2comm50-Late Fusion60-No FusionV2/NetReal-worl limitation5-10 Real-world limitation1515202050-1015Bandswidth (loq2 KBps)Bandwidth [loq3 Kbps]</s>",
            "parsed": {
              "<OCR>": "V2SimEarly FusionDAIR-V2XCoAlign95-COSDHCosDHV2x-VIT75-VC2X-VITE90-CosDHCOSDHAv2XVIT70-CoAlign85-C0AlignDiscoNetV2VNet70-Early Fusion80-APQ/0.5 (%)V2ZVNet75-DiscNet65-Where2CommNo Fusion70-Real-world limitation101520Real-World limitation1515Bandwidth (lq2 Kbps)20V2SimDair-VZXCOSDHEarly FusionCoAlIGN85-CoAlgn80-60-Early Fusion70-Late FusionV2NNetDiscNet75V2Net5-5-No FusionDiscoNetWhere2commV2KVIT65-AQ/O.7 (%)Where2comm50-Late Fusion60-No FusionV2/NetReal-worl limitation5-10 Real-world limitation1515202050-1015Bandswidth (loq2 KBps)Bandwidth [loq3 Kbps]"
            }
          },
          "paper_caption": "Figure 5. Comparison of the trade-off between detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20] and DAIR-V2X [42] datasets, CoSDH achieves the best accuracy-bandwidth trade-off. The real-world limitation refers to the total bandwidth limit of 27 Mbps, which means that each collaborative agent does not exceed 6.75 Mbps.",
          "paper_caption_bbox": [
            58.49999237060547,
            343.1309814453125,
            553.4991455078125,
            374.015380859375
          ]
        },
        {
          "image_id": "p7_cluster_002",
          "page": 7,
          "file_path": "images/page_0007_cluster_002.png",
          "bbox": [
            61.27910232543945,
            72.24565124511719,
            224.1034698486328,
            331.9622497558594
          ],
          "caption": "a graph showing the number of different types of fusion",
          "detailed_caption": "The image shows a plot of the correlation between the real-world limitation and the real fusion of a single-channel amplifier. The plot is composed of two graphs, one in blue and one in red, with text at the top and bottom of the image. The blue graph shows the relationship between the two variables, while the red graph displays the correlation.",
          "ocr_text": "OPV2VCo SDHv2VNet95-Early Fusion V2X-VITLate Fusion Co Align V2x-Vit90-Disc Net85-Where2comm APQ (0.5%) No Fusion80-Real-world limitation101520Bandwidth (log2 Kbps) OPVP2VCOAlign90-V2K-VIt85Late Fusion Early Fusion80Disco Net75-Apo (0, 7%) Where2comm70-No Fusion Real-World limitation05101520Bandwidth [log2Kbps)",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>OPV2VCoSDHv2VNet95-Early FusionV2X-VITLate FusionCoAlignV2x-Vit90-DiscNet85-Where2commAPQ(0.5%)No Fusion80-Real-world limitation101520Bandwidth (log2 Kbps)OPVP2VCOAlign90-V2K-VIt85Late FusionEarly Fusion80DiscoNet75-Apo(0,7%)Where2comm70-No FusionReal-World limitation05101520Bandwidth [log2Kbps)</s>",
            "parsed": {
              "<OCR>": "OPV2VCoSDHv2VNet95-Early FusionV2X-VITLate FusionCoAlignV2x-Vit90-DiscNet85-Where2commAPQ(0.5%)No Fusion80-Real-world limitation101520Bandwidth (log2 Kbps)OPVP2VCOAlign90-V2K-VIt85Late FusionEarly Fusion80DiscoNet75-Apo(0,7%)Where2comm70-No FusionReal-World limitation05101520Bandwidth [log2Kbps)"
            }
          },
          "paper_caption": "Figure 5. Comparison of the trade-off between detection accuracy and bandwidth of different methods on OPV2V [36], V2XSim [20] and DAIR-V2X [42] datasets, CoSDH achieves the best accuracy-bandwidth trade-off. The real-world limitation refers to the total bandwidth limit of 27 Mbps, which means that each collaborative agent does not exceed 6.75 Mbps.",
          "paper_caption_bbox": [
            58.49999237060547,
            343.1309814453125,
            553.4991455078125,
            374.015380859375
          ]
        }
      ]
    },
    {
      "page": 8,
      "text_raw": "Figure 6. Visualization of detection results under real-world com-\nmunication rate limitations on the DAIR-V2X [42] dataset. Green\nrepresents ground truth box, and red represents predicted box.\n4.3. Qualitative Evaluation\nFig. 6 shows the visualization of detection results for our\nmethod compared to other methods under simulated real-\nworld communication rate limitations on the DAIR-V2X\ndataset. A comparison of the detection results reveals that,\nwhile using less bandwidth, our method achieves the same\nrecall rate as CoAlign [25] and Where2comm [11], with\nfewer false positive predictions and more accurate object\nlocalization, demonstrating that our method performs better\nunder low-bandwidth conditions. Comparing the two im-\nages at bottom, it can be seen that under low bandwidth con-\nditions, late fusion effectively compensates for the results of\nintermediate fusion, improving the recall rate of objects.\n4.4. Ablation Studies\nTable 2 presents the results of the ablation study on vari-\nous modules of the proposed method using the OPV2V [36]\ndataset. The results show that after using the autoencoder\nfor compression, the bandwidth is reduced by a factor of\nc0, and there is no significant loss in perception accuracy,\neven with a slight improvement in AP@0.7. Converting\nintermediate features to float16 can nearly halve the band-\nwidth without significant loss, demonstrating the effective-\nness of our proposed message compression module. After\nperforming information selection based on the supply mask,\nperception accuracy slightly decreases, but the bandwidth\nis reduced by about 95%, which is a worthwhile trade-off.\nFurther using the demand mask results in no significant de-\ncrease in accuracy but reduces bandwidth by about 10%.\nThis demonstrates that our supply-demand-aware informa-\ntion selection can reduce bandwidth while maintaining ac-\ncuracy.\nAfter adding late fusion, accuracy improves at a rela-\ntively small bandwidth cost. However, since this phase uses\nmore bandwidth, the improvement in accuracy is not signif-\nCompression\nSelection\nLate Fusion\nAP@0.5‚Üë\nAP@0.7‚Üë\nBD‚Üì\nAutoencoder\nFP16\nSupply\nDemand\n96.62%\n92.62%\n1,155.00 Mbps\n‚úì\n96.59%\n92.99%\n72.19 Mbps\n‚úì\n‚úì\n96.59%\n92.99%\n36.09 Mbps\n‚úì\n‚úì\n‚úì\n96.30%\n92.62%\n1.99 Mbps\n‚úì\n‚úì\n‚úì\n‚úì\n96.31%\n92.60%\n1.82 Mbps\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n96.75%\n92.92%\n1.97 Mbps\nTable 2. Ablation study of the modules in CoSDH on the OPV2V\ndataset. ‚ÄúAutoencoder‚Äù refers to the use of autoencoders to com-\npress features along the channel dimension, with a compression\nratio of c0 = 16. ‚ÄúFP16‚Äù refers to converting features from float32\nto float16 for transmission. ‚ÄúSupply‚Äù and ‚ÄúDemand‚Äù refer to us-\ning supply and demand masks for information selection, and ‚ÄúLate\nFusion‚Äù refers to applying confidence-aware late fusion.\nLate Fusion\nŒµc\n0.01\n0.02\n0.03\n0.05\n0.07\nAP@0.5‚Üë\n96.59%\n96.31%\n96.19%\n95.18%\n93.35%\nAP@0.7‚Üë\n92.95%\n92.60%\n92.27%\n90.67%\n87.60%\nBD‚Üì\n13.26 Mbps\n1.82 Mbps\n0.54 Mbps\n0.12 Mbps\n0.05 Mbps\n‚úì\nAP@0.5‚Üë\n96.83%\n96.75%\n96.72%\n96.54%\n96.41%\nAP@0.7‚Üë\n92.99%\n92.92%\n92.73%\n92.23%\n91.68%\nBD‚Üì\n13.40 Mbps\n1.97 Mbps\n0.68 Mbps\n0.26 Mbps\n0.19 Mbps\nTable 3. Ablation study of confidence-aware late fusion under dif-\nferent bandwidths on the OPV2V [36] dataset. The table shows\nthe impact of late fusion on accuracy and bandwidth with different\nvalues of Œµc.\nicant. Table 3 shows the accuracy improvement due to late\ncollaboration under different bandwidth conditions. It can\nbe observed that late collaboration provides more accuracy\nimprovements under lower bandwidth conditions. For ex-\nample, when the bandwidth used for intermediate fusion is\n0.05 Mbps, late collaboration with a bandwidth cost of 0.14\nMbps leads to a 3% improvement in AP@0.5 and a 4% im-\nprovement in AP@0.7. This is one of the key reasons why\nour method achieves significantly higher detection accuracy\nunder lower bandwidth compared to Where2comm [11].\n5. Conclusion\nIn this paper, we propose CoSDH, a novel communication-\nefficient collaborative perception framework for 3D object\ndetection. By finely modeling the supply-demand relation-\nship between agents, it selects key and sparse regions for\ncollaboration.\nAdditionally, we innovatively incorporate\nconfidence-aware late fusion on top of intermediate col-\nlaboration to form an intermediate-late hybrid collaborative\nperception method. Experiments on multiple datasets show\nthat our method offers a better accuracy-bandwidth trade-\noff, demonstrating outstanding accuracy under bandwidth\nconstraints close to real-world communication limits, mak-\ning it highly valuable for practical applications.\nAcknowledgment\nThis work is supported by the National Key Research and\nDevelopment Plan (2024YFB3309302).\n6841\n",
      "text_clean": "Figure 6. Visualization of detection results under real-world communication rate limitations on the DAIR-V2X [42] dataset. Green represents ground truth box, and red represents predicted box. 4.3. Qualitative Evaluation Fig. 6 shows the visualization of detection results for our method compared to other methods under simulated realworld communication rate limitations on the DAIR-V2X dataset. A comparison of the detection results reveals that, while using less bandwidth, our method achieves the same recall rate as CoAlign [25] and Where2comm [11], with fewer false positive predictions and more accurate object localization, demonstrating that our method performs better under low-bandwidth conditions. Comparing the two images at bottom, it can be seen that under low bandwidth conditions, late fusion effectively compensates for the results of intermediate fusion, improving the recall rate of objects. 4.4. Ablation Studies Table 2 presents the results of the ablation study on various modules of the proposed method using the OPV2V [36] dataset. The results show that after using the autoencoder for compression, the bandwidth is reduced by a factor of c0, and there is no significant loss in perception accuracy, even with a slight improvement in AP@0.7. Converting intermediate features to float16 can nearly halve the bandwidth without significant loss, demonstrating the effectiveness of our proposed message compression module. After performing information selection based on the supply mask, perception accuracy slightly decreases, but the bandwidth is reduced by about 95%, which is a worthwhile trade-off. Further using the demand mask results in no significant decrease in accuracy but reduces bandwidth by about 10%. This demonstrates that our supply-demand-aware information selection can reduce bandwidth while maintaining accuracy. After adding late fusion, accuracy improves at a relatively small bandwidth cost. However, since this phase uses more bandwidth, the improvement in accuracy is not signifCompression Selection Late Fusion AP@0.5‚Üë AP@0.7‚Üë BD‚Üì Autoencoder FP16 Supply Demand 96.62% 92.62% 1,155.00 Mbps ‚úì 96.59% 92.99% 72.19 Mbps ‚úì ‚úì 96.59% 92.99% 36.09 Mbps ‚úì ‚úì ‚úì 96.30% 92.62% 1.99 Mbps ‚úì ‚úì ‚úì ‚úì 96.31% 92.60% 1.82 Mbps ‚úì ‚úì ‚úì ‚úì ‚úì 96.75% 92.92% 1.97 Mbps Table 2. Ablation study of the modules in CoSDH on the OPV2V dataset. ‚ÄúAutoencoder‚Äù refers to the use of autoencoders to compress features along the channel dimension, with a compression ratio of c0 = 16. ‚ÄúFP16‚Äù refers to converting features from float32 to float16 for transmission. ‚ÄúSupply‚Äù and ‚ÄúDemand‚Äù refer to using supply and demand masks for information selection, and ‚ÄúLate Fusion‚Äù refers to applying confidence-aware late fusion. Late Fusion Œµc 0.01 0.02 0.03 0.05 0.07 AP@0.5‚Üë 96.59% 96.31% 96.19% 95.18% 93.35% AP@0.7‚Üë 92.95% 92.60% 92.27% 90.67% 87.60% BD‚Üì 13.26 Mbps 1.82 Mbps 0.54 Mbps 0.12 Mbps 0.05 Mbps ‚úì AP@0.5‚Üë 96.83% 96.75% 96.72% 96.54% 96.41% AP@0.7‚Üë 92.99% 92.92% 92.73% 92.23% 91.68% BD‚Üì 13.40 Mbps 1.97 Mbps 0.68 Mbps 0.26 Mbps 0.19 Mbps Table 3. Ablation study of confidence-aware late fusion under different bandwidths on the OPV2V [36] dataset. The table shows the impact of late fusion on accuracy and bandwidth with different values of Œµc. icant. Table 3 shows the accuracy improvement due to late collaboration under different bandwidth conditions. It can be observed that late collaboration provides more accuracy improvements under lower bandwidth conditions. For example, when the bandwidth used for intermediate fusion is 0.05 Mbps, late collaboration with a bandwidth cost of 0.14 Mbps leads to a 3% improvement in AP@0.5 and a 4% improvement in AP@0.7. This is one of the key reasons why our method achieves significantly higher detection accuracy under lower bandwidth compared to Where2comm [11]. 5. Conclusion In this paper, we propose CoSDH, a novel communicationefficient collaborative perception framework for 3D object detection. By finely modeling the supply-demand relationship between agents, it selects key and sparse regions for collaboration. Additionally, we innovatively incorporate confidence-aware late fusion on top of intermediate collaboration to form an intermediate-late hybrid collaborative perception method. Experiments on multiple datasets show that our method offers a better accuracy-bandwidth tradeoff, demonstrating outstanding accuracy under bandwidth constraints close to real-world communication limits, making it highly valuable for practical applications. Acknowledgment This work is supported by the National Key Research and Development Plan (2024YFB3309302). 6841",
      "figure_captions": [
        {
          "bbox": [
            58.5,
            239.23098754882812,
            294.7467041015625,
            270.1153564453125
          ],
          "text": "Figure 6. Visualization of detection results under real-world com- munication rate limitations on the DAIR-V2X [42] dataset. Green represents ground truth box, and red represents predicted box."
        },
        {
          "bbox": [
            58.5,
            305.180419921875,
            294.75311279296875,
            446.64990234375
          ],
          "text": "Fig. 6 shows the visualization of detection results for our method compared to other methods under simulated real- world communication rate limitations on the DAIR-V2X dataset. A comparison of the detection results reveals that, while using less bandwidth, our method achieves the same recall rate as CoAlign [25] and Where2comm [11], with fewer false positive predictions and more accurate object localization, demonstrating that our method performs better under low-bandwidth conditions. Comparing the two im- ages at bottom, it can be seen that under low bandwidth con- ditions, late fusion effectively compensates for the results of intermediate fusion, improving the recall rate of objects."
        }
      ],
      "images": [
        {
          "image_id": "p8_cluster_001",
          "page": 8,
          "file_path": "images/page_0008_cluster_001.png",
          "bbox": [
            58.5,
            71.99397277832031,
            303.5130615234375,
            228.39697265625
          ],
          "caption": "a series of four images showing different types of data",
          "detailed_caption": "The image shows a collage of four different types of data, each with a black background and text written on it. The text appears to be a diagram of a computer screen with a variety of data points, such as a satellite, a satellite dish, and a satellite antenna. The data points are arranged in a grid-like pattern, with each point representing a different type of data. The colors of the data points range from light blue to dark blue, and the text is written in a bold font.",
          "ocr_text": "Co Align Bandwidth: 3.85 Mbps Where2comm Band Width: 5.26 Mbps0.0.27Coalign (w/o late fusion) Bandwidth: 0.15 Mbps Co SDHBandwidth; 0.29 Mbps",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>CoAlignBandwidth: 3.85 MbpsWhere2commBandWidth: 5.26 Mbps0.0.27Coalign(w/o late fusion) Bandwidth: 0.15 MbpsCoSDHBandwidth; 0.29 Mbps</s>",
            "parsed": {
              "<OCR>": "CoAlignBandwidth: 3.85 MbpsWhere2commBandWidth: 5.26 Mbps0.0.27Coalign(w/o late fusion) Bandwidth: 0.15 MbpsCoSDHBandwidth; 0.29 Mbps"
            }
          },
          "paper_caption": "Figure 6. Visualization of detection results under real-world com- munication rate limitations on the DAIR-V2X [42] dataset. Green represents ground truth box, and red represents predicted box.",
          "paper_caption_bbox": [
            58.5,
            239.23098754882812,
            294.7467041015625,
            270.1153564453125
          ]
        }
      ]
    },
    {
      "page": 9,
      "text_raw": "References\n[1] Fabio Arena and Giovanni Pau. An overview of vehicular\ncommunications. Future internet, 11(2):27, 2019. 1, 6\n[2] Eduardo Arnold, Mehrdad Dianati, Robert de Temple, and\nSaber Fallah. Cooperative perception for 3d object detec-\ntion in driving scenarios using infrastructure sensors. IEEE\nTransactions on Intelligent Transportation Systems, 23(3):\n1852‚Äì1864, 2020. 3\n[3] Qi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and\nSong Fu. F-cooper: Feature based cooperative perception for\nautonomous vehicle edge computing system using 3d point\nclouds. In Proceedings of the 4th ACM/IEEE Symposium on\nEdge Computing, pages 88‚Äì100, 2019. 1, 3, 6, 7, 2\n[4] Qi Chen, Sihai Tang, Qing Yang, and Song Fu.\nCooper:\nCooperative perception for connected autonomous vehicles\nbased on 3d point clouds. In 2019 IEEE 39th International\nConference on Distributed Computing Systems (ICDCS),\npages 514‚Äì524. IEEE, 2019. 3\n[5] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast\npoint r-cnn. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 9775‚Äì9784, 2019. 2\n[6] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Dsgn:\nDeep stereo geometry network for 3d object detection. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 12536‚Äì12545, 2020. 2\n[7] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou,\nYanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards\nhigh performance voxel-based 3d object detection. In Pro-\nceedings of the AAAI conference on artificial intelligence,\npages 1201‚Äì1209, 2021. 2\n[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-\nnio Lopez, and Vladlen Koltun. Carla: An open urban driv-\ning simulator. In Conference on robot learning, pages 1‚Äì16.\nPMLR, 2017. 1\n[9] Sebastian Gr¬®afling, Petri M¬®ah¬®onen, and Janne Riihij¬®arvi.\nPerformance evaluation of ieee 1609 wave and ieee 802.11 p\nfor vehicular communications. In 2010 second international\nconference on ubiquitous and future networks (ICUFN),\npages 344‚Äì348. IEEE, 2010. 2\n[10] Jingda Guo, Dominic Carrillo, Sihai Tang, Qi Chen, Qing\nYang, Song Fu, Xi Wang, Nannan Wang, and Paparao\nPalacharla. Coff: Cooperative spatial feature fusion for 3-\nd object detection on autonomous vehicles. IEEE Internet of\nThings Journal, 8(14):11078‚Äì11087, 2021. 3\n[11] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Si-\nheng Chen. Where2comm: Communication-efficient collab-\norative perception via spatial confidence maps.\nAdvances\nin neural information processing systems, 35:4874‚Äì4886,\n2022. 1, 2, 3, 4, 6, 7, 8\n[12] Yue Hu, Juntong Peng, Sifei Liu, Junhao Ge, Si Liu, and Si-\nheng Chen. Communication-efficient collaborative percep-\ntion via information filling with codebook. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 15481‚Äì15490, 2024. 2\n[13] Zheng Jiang, Jinqing Zhang, Yanan Zhang, Qingjie Liu,\nZhenghui Hu, Baohui Wang, and Yunhong Wang. Fsd-bev:\nForeground self-distillation for multi-view 3d object detec-\ntion. In European Conference on Computer Vision, pages\n110‚Äì126. Springer, 2024. 2\n[14] Diederik P Kingma. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980, 2014. 6\n[15] Daniel Krajzewicz, Jakob Erdmann, Michael Behrisch, and\nLaura Bieker. Recent development and applications of sumo-\nsimulation of urban mobility. International journal on ad-\nvances in systems and measurements, 5(3&4), 2012. 1\n[16] Jason Ku, Alex D Pon, and Steven L Waslander. Monocular\n3d object detection leveraging accurate proposals and shape\nreconstruction. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 11867‚Äì\n11876, 2019. 2\n[17] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12697‚Äì12705, 2019. 3, 6\n[18] Peiliang Li, Xiaozhi Chen, and Shaojie Shen. Stereo r-cnn\nbased 3d object detection for autonomous driving. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 7644‚Äì7652, 2019. 2\n[19] Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, Chen\nFeng, and Wenjun Zhang. Learning distilled collaboration\ngraph for multi-agent perception. Advances in Neural Infor-\nmation Processing Systems, 34:29541‚Äì29552, 2021. 3, 6\n[20] Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi Zhong,\nSiheng Chen, and Chen Feng. V2x-sim: Multi-agent col-\nlaborative perception dataset and benchmark for autonomous\ndriving. IEEE Robotics and Automation Letters, 7(4):10914‚Äì\n10921, 2022. 1, 2, 3, 5, 6, 7\n[21] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt\nKira. When2com: Multi-agent perception via communica-\ntion graph grouping. In Proceedings of the IEEE/CVF Con-\nference on computer vision and pattern recognition, pages\n4106‚Äì4115, 2020. 3, 6\n[22] Yen-Cheng Liu, Junjiao Tian, Chih-Yao Ma, Nathan Glaser,\nChia-Wen Kuo, and Zsolt Kira. Who2com: Collaborative\nperception via learnable handshake communication. In 2020\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 6876‚Äì6883. IEEE, 2020. 3\n[23] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\nHuizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-\ntask multi-sensor fusion with unified bird‚Äôs-eye view repre-\nsentation. In 2023 IEEE international conference on robotics\nand automation (ICRA), pages 2774‚Äì2781. IEEE, 2023. 3\n[24] Jose Manuel Lozano Dominguez and Tomas Jesus Ma-\nteo Sanguino. Review on v2x, i2x, and p2x communications\nand their applications: a comprehensive analysis over time.\nSensors, 19(12):2756, 2019. 1, 6\n[25] Yifan Lu, Quanhao Li, Baoan Liu, Mehrdad Dianati, Chen\nFeng, Siheng Chen, and Yanfeng Wang. Robust collabora-\ntive 3d object detection in presence of pose errors. In 2023\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 4812‚Äì4818. IEEE, 2023. 5, 6, 7, 8, 1, 2\n[26] Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hong-\nsheng Li. 3d object detection for autonomous driving: A\n6842\n",
      "text_clean": "References [1] Fabio Arena and Giovanni Pau. An overview of vehicular communications. Future internet, 11(2):27, 2019. 1, 6 [2] Eduardo Arnold, Mehrdad Dianati, Robert de Temple, and Saber Fallah. Cooperative perception for 3d object detection in driving scenarios using infrastructure sensors. IEEE Transactions on Intelligent Transportation Systems, 23(3): 1852‚Äì1864, 2020. 3 [3] Qi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and Song Fu. F-cooper: Feature based cooperative perception for autonomous vehicle edge computing system using 3d point clouds. In Proceedings of the 4th ACM/IEEE Symposium on Edge Computing, pages 88‚Äì100, 2019. 1, 3, 6, 7, 2 [4] Qi Chen, Sihai Tang, Qing Yang, and Song Fu. Cooper: Cooperative perception for connected autonomous vehicles based on 3d point clouds. In 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), pages 514‚Äì524. IEEE, 2019. 3 [5] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast point r-cnn. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9775‚Äì9784, 2019. 2 [6] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Dsgn: Deep stereo geometry network for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12536‚Äì12545, 2020. 2 [7] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. In Proceedings of the AAAI conference on artificial intelligence, pages 1201‚Äì1209, 2021. 2 [8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 1‚Äì16. PMLR, 2017. 1 [9] Sebastian Gr¬®afling, Petri M¬®ah¬®onen, and Janne Riihij¬®arvi. Performance evaluation of ieee 1609 wave and ieee 802.11 p for vehicular communications. In 2010 second international conference on ubiquitous and future networks (ICUFN), pages 344‚Äì348. IEEE, 2010. 2 [10] Jingda Guo, Dominic Carrillo, Sihai Tang, Qi Chen, Qing Yang, Song Fu, Xi Wang, Nannan Wang, and Paparao Palacharla. Coff: Cooperative spatial feature fusion for 3- d object detection on autonomous vehicles. IEEE Internet of Things Journal, 8(14):11078‚Äì11087, 2021. 3 [11] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen. Where2comm: Communication-efficient collaborative perception via spatial confidence maps. Advances in neural information processing systems, 35:4874‚Äì4886, 2022. 1, 2, 3, 4, 6, 7, 8 [12] Yue Hu, Juntong Peng, Sifei Liu, Junhao Ge, Si Liu, and Siheng Chen. Communication-efficient collaborative perception via information filling with codebook. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15481‚Äì15490, 2024. 2 [13] Zheng Jiang, Jinqing Zhang, Yanan Zhang, Qingjie Liu, Zhenghui Hu, Baohui Wang, and Yunhong Wang. Fsd-bev: Foreground self-distillation for multi-view 3d object detection. In European Conference on Computer Vision, pages 110‚Äì126. Springer, 2024. 2 [14] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [15] Daniel Krajzewicz, Jakob Erdmann, Michael Behrisch, and Laura Bieker. Recent development and applications of sumosimulation of urban mobility. International journal on advances in systems and measurements, 5(3&4), 2012. 1 [16] Jason Ku, Alex D Pon, and Steven L Waslander. Monocular 3d object detection leveraging accurate proposals and shape reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11867‚Äì 11876, 2019. 2 [17] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12697‚Äì12705, 2019. 3, 6 [18] Peiliang Li, Xiaozhi Chen, and Shaojie Shen. Stereo r-cnn based 3d object detection for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7644‚Äì7652, 2019. 2 [19] Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, Chen Feng, and Wenjun Zhang. Learning distilled collaboration graph for multi-agent perception. Advances in Neural Information Processing Systems, 34:29541‚Äì29552, 2021. 3, 6 [20] Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi Zhong, Siheng Chen, and Chen Feng. V2x-sim: Multi-agent collaborative perception dataset and benchmark for autonomous driving. IEEE Robotics and Automation Letters, 7(4):10914‚Äì 10921, 2022. 1, 2, 3, 5, 6, 7 [21] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira. When2com: Multi-agent perception via communication graph grouping. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 4106‚Äì4115, 2020. 3, 6 [22] Yen-Cheng Liu, Junjiao Tian, Chih-Yao Ma, Nathan Glaser, Chia-Wen Kuo, and Zsolt Kira. Who2com: Collaborative perception via learnable handshake communication. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 6876‚Äì6883. IEEE, 2020. 3 [23] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multitask multi-sensor fusion with unified bird‚Äôs-eye view representation. In 2023 IEEE international conference on robotics and automation (ICRA), pages 2774‚Äì2781. IEEE, 2023. 3 [24] Jose Manuel Lozano Dominguez and Tomas Jesus Mateo Sanguino. Review on v2x, i2x, and p2x communications and their applications: a comprehensive analysis over time. Sensors, 19(12):2756, 2019. 1, 6 [25] Yifan Lu, Quanhao Li, Baoan Liu, Mehrdad Dianati, Chen Feng, Siheng Chen, and Yanfeng Wang. Robust collaborative 3d object detection in presence of pose errors. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 4812‚Äì4818. IEEE, 2023. 5, 6, 7, 8, 1, 2 [26] Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. 3d object detection for autonomous driving: A 6842",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 10,
      "text_raw": "comprehensive survey. International Journal of Computer\nVision, 131(8):1909‚Äì1963, 2023. 2\n[27] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao\nHuang. 3d object detection with pointformer. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 7463‚Äì7472, 2021. 2\n[28] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 770‚Äì779, 2019.\n2\n[29] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\nvoxel feature set abstraction for 3d object detection. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10529‚Äì10538, 2020. 2\n[30] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang,\nand Hongsheng Li. From points to parts: 3d object detection\nfrom point cloud with part-aware and part-aggregation net-\nwork. IEEE transactions on pattern analysis and machine\nintelligence, 43(8):2647‚Äì2664, 2020. 2\n[31] Rui Song, Chenwei Liang, Hu Cao, Zhiran Yan, Walter Zim-\nmer, Markus Gross, Andreas Festag, and Alois Knoll. Col-\nlaborative semantic occupancy prediction with hybrid fea-\nture fusion in connected automated vehicles. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 17996‚Äì18006, 2024. 1\n[32] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang,\nBin Yang, Wenyuan Zeng, and Raquel Urtasun.\nV2vnet:\nVehicle-to-vehicle communication for joint perception and\nprediction.\nIn Computer Vision‚ÄìECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23‚Äì28, 2020, Pro-\nceedings, Part II 16, pages 605‚Äì621. Springer, 2020. 3, 6\n[33] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hari-\nharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-\nlidar from visual depth estimation: Bridging the gap in 3d\nobject detection for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8445‚Äì8453, 2019. 2\n[34] Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Hao Xiang, and\nJiaqi Ma. Opencda: an open cooperative driving automa-\ntion framework integrated with co-simulation. In 2021 IEEE\nInternational Intelligent Transportation Systems Conference\n(ITSC), pages 1155‚Äì1162. IEEE, 2021. 1\n[35] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-\nHsuan Yang, and Jiaqi Ma. V2x-vit: Vehicle-to-everything\ncooperative perception with vision transformer. In European\nconference on computer vision, pages 107‚Äì124. Springer,\n2022. 1, 3, 5, 6, 2\n[36] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and\nJiaqi Ma. Opv2v: An open benchmark dataset and fusion\npipeline for perception with vehicle-to-vehicle communica-\ntion. In 2022 International Conference on Robotics and Au-\ntomation (ICRA), pages 2583‚Äì2589. IEEE, 2022. 1, 2, 3, 5,\n6, 7, 8\n[37] Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei\nZhou, and Jiaqi Ma. Cobevt: Cooperative bird‚Äôs eye view\nsemantic segmentation with sparse transformers. In Confer-\nence on Robot Learning, pages 989‚Äì1000. PMLR, 2023. 1,\n3\n[38] Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang,\nZhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong,\nRui Song, et al. V2v4real: A real-world large-scale dataset\nfor vehicle-to-vehicle cooperative perception. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 13712‚Äì13722, 2023. 1, 2\n[39] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\nded convolutional detection. Sensors, 18(10):3337, 2018. 2\n[40] Dingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi\nXu, Rongbin Yin, Peng Zhai, and Lihua Zhang. How2comm:\nCommunication-efficient and collaboration-pragmatic multi-\nagent perception. Advances in Neural Information Process-\ning Systems, 36, 2024. 4\n[41] Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang\nYang, Pascal Frossard, and Wenguan Wang.\nIs-fusion:\nInstance-scene collaborative fusion for multimodal 3d ob-\nject detection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 14905‚Äì\n14915, 2024. 3\n[42] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang,\nYifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui\nYuan, et al.\nDair-v2x: A large-scale dataset for vehicle-\ninfrastructure cooperative 3d object detection. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 21361‚Äì21370, 2022. 2, 3, 5, 6,\n7, 8, 1\n[43] Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang,\nYingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan,\nNing Sun, et al. V2x-seq: A large-scale sequential dataset for\nvehicle-infrastructure cooperative perception and forecast-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 5486‚Äì5495,\n2023. 1\n[44] Jinqing Zhang, Yanan Zhang, Qingjie Liu, and Yunhong\nWang. Sa-bev: Generating semantic-aware bird‚Äôs-eye-view\nfeature for multi-view 3d object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3348‚Äì3357, 2023. 2\n[45] Jinqing Zhang, Yanan Zhang, Yunlong Qi, Zehua Fu, Qingjie\nLiu, and Yunhong Wang. Geobev: Learning geometric bev\nrepresentation for multi-view 3d object detection.\narXiv\npreprint arXiv:2409.01816, 2024. 2\n[46] Yanan Zhang, Di Huang, and Yunhong Wang. Pc-rgnn: Point\ncloud completion and graph neural network for 3d object de-\ntection. In Proceedings of the AAAI conference on artificial\nintelligence, pages 3430‚Äì3437, 2021. 2\n[47] Yanan Zhang, Jiaxin Chen, and Di Huang. Cat-det: Con-\ntrastively augmented transformer for multi-modal 3d object\ndetection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 908‚Äì917,\n2022. 3\n[48] Chao Zhou, Yanan Zhang, Jiaxin Chen, and Di Huang. Octr:\nOctree-based transformer for 3d object detection. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5166‚Äì5175, 2023. 2\n6843\n",
      "text_clean": "comprehensive survey. International Journal of Computer Vision, 131(8):1909‚Äì1963, 2023. 2 [27] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao Huang. 3d object detection with pointformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7463‚Äì7472, 2021. 2 [28] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 770‚Äì779, 2019. 2 [29] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10529‚Äì10538, 2020. 2 [30] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE transactions on pattern analysis and machine intelligence, 43(8):2647‚Äì2664, 2020. 2 [31] Rui Song, Chenwei Liang, Hu Cao, Zhiran Yan, Walter Zimmer, Markus Gross, Andreas Festag, and Alois Knoll. Collaborative semantic occupancy prediction with hybrid feature fusion in connected automated vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17996‚Äì18006, 2024. 1 [32] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2vnet: Vehicle-to-vehicle communication for joint perception and prediction. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part II 16, pages 605‚Äì621. Springer, 2020. 3, 6 [33] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudolidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8445‚Äì8453, 2019. 2 [34] Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Hao Xiang, and Jiaqi Ma. Opencda: an open cooperative driving automation framework integrated with co-simulation. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pages 1155‚Äì1162. IEEE, 2021. 1 [35] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, MingHsuan Yang, and Jiaqi Ma. V2x-vit: Vehicle-to-everything cooperative perception with vision transformer. In European conference on computer vision, pages 107‚Äì124. Springer, 2022. 1, 3, 5, 6, 2 [36] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and Jiaqi Ma. Opv2v: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication. In 2022 International Conference on Robotics and Automation (ICRA), pages 2583‚Äì2589. IEEE, 2022. 1, 2, 3, 5, 6, 7, 8 [37] Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei Zhou, and Jiaqi Ma. Cobevt: Cooperative bird‚Äôs eye view semantic segmentation with sparse transformers. In Conference on Robot Learning, pages 989‚Äì1000. PMLR, 2023. 1, 3 [38] Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, et al. V2v4real: A real-world large-scale dataset for vehicle-to-vehicle cooperative perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13712‚Äì13722, 2023. 1, 2 [39] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018. 2 [40] Dingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi Xu, Rongbin Yin, Peng Zhai, and Lihua Zhang. How2comm: Communication-efficient and collaboration-pragmatic multiagent perception. Advances in Neural Information Processing Systems, 36, 2024. 4 [41] Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, and Wenguan Wang. Is-fusion: Instance-scene collaborative fusion for multimodal 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14905‚Äì 14915, 2024. 3 [42] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, et al. Dair-v2x: A large-scale dataset for vehicleinfrastructure cooperative 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21361‚Äì21370, 2022. 2, 3, 5, 6, 7, 8, 1 [43] Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang, Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan, Ning Sun, et al. V2x-seq: A large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5486‚Äì5495, 2023. 1 [44] Jinqing Zhang, Yanan Zhang, Qingjie Liu, and Yunhong Wang. Sa-bev: Generating semantic-aware bird‚Äôs-eye-view feature for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3348‚Äì3357, 2023. 2 [45] Jinqing Zhang, Yanan Zhang, Yunlong Qi, Zehua Fu, Qingjie Liu, and Yunhong Wang. Geobev: Learning geometric bev representation for multi-view 3d object detection. arXiv preprint arXiv:2409.01816, 2024. 2 [46] Yanan Zhang, Di Huang, and Yunhong Wang. Pc-rgnn: Point cloud completion and graph neural network for 3d object detection. In Proceedings of the AAAI conference on artificial intelligence, pages 3430‚Äì3437, 2021. 2 [47] Yanan Zhang, Jiaxin Chen, and Di Huang. Cat-det: Contrastively augmented transformer for multi-modal 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 908‚Äì917, 2022. 3 [48] Chao Zhou, Yanan Zhang, Jiaxin Chen, and Di Huang. Octr: Octree-based transformer for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5166‚Äì5175, 2023. 2 6843",
      "figure_captions": [],
      "images": []
    }
  ]
}