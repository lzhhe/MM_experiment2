{
  "pdf_path_rel": "../../../papers/CV/Song_Collaborative_Semantic_Occupancy_Prediction_with_Hybrid_Feature_Fusion_in_Connected_CVPR_2024_paper.pdf",
  "pdf_path_abs": "E:\\Desktop\\M502083B 多模态机器学习\\MM_experiment2\\papers\\CV\\Song_Collaborative_Semantic_Occupancy_Prediction_with_Hybrid_Feature_Fusion_in_Connected_CVPR_2024_paper.pdf",
  "num_pages": 11,
  "full_text_clean": "Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles Rui Song1,2 *, Chenwei Liang1, Hu Cao2, Zhiran Yan3, Walter Zimmer2, Markus Gross1, Andreas Festag1,3, Alois Knoll2 1Fraunhofer IVI 2Technical University of Munich 3Technische Hochschule Ingolstadt https://rruisong.github.io/publications/CoHFF Figure 1. Collaborative semantic occupancy prediction leverages the power of collaboration in multi-agent systems for 3D occupancy prediction and semantic segmentation. This approach enables a deeper understanding of the 3D road environment by sharing latent features among connected automated vehicles (CAVs), surpassing the ground truth captured by a multi-camera system in the ego vehicle. Abstract Collaborative perception in automated vehicles leverages the exchange of information between agents, aiming to elevate perception results. Previous camera-based collaborative 3D perception methods typically employ 3D bounding boxes or bird’s eye views as representations of the environment. However, these approaches fall short in offering a comprehensive 3D environmental prediction. To bridge this gap, we introduce the first method for collaborative 3D semantic occupancy prediction. Particularly, it improves local 3D semantic occupancy predictions by hybrid fusion of (i) semantic and occupancy task features, and (ii) compressed orthogonal attention features shared between vehicles. Additionally, due to the lack of a collaborative perception dataset designed for semantic occupancy prediction, we augment a current collaborative perception dataset to include 3D collaborative semantic occupancy labels for a more robust evaluation. The experimental findings highlight that: (i) our collaborative semantic occupancy predictions excel above the results from single vehicles by over 30%, and (ii) models anchored on semantic occupancy outpace *Corresponding author, email address: rui.song@ivi.fraunhofer.de state-of-the-art collaborative 3D detection techniques in subsequent perception applications, showcasing enhanced accuracy and enriched semantic-awareness in road environments. 1. Introduction Collaborative perception, also known as cooperative perception, significantly improves the accuracy and completeness of each connected and automated vehicle’s (CAV) sensing capabilities by integrating multiple viewpoints, surpassing the limitations of single-vehicle systems [11, 12, 15, 16, 25, 26, 35, 42, 45, 50]. This approach enables CAVs to achieve comparable or superior perception abilities, even with cost-effective sensors. Notably, recent research in [12] suggests that camera-based systems may outperform LiDAR in 3D perception through collaboration in Vehicle-to-Everything (V2X) communication networks. Previous studies in camera-based collaborative perception typically processed inputs from various CAVs into simplified formats such as 3D bounding boxes or Bird’s Eye View (BEV) segmentation. While efficient, these methods tend to miss important 3D semantic details, which are indispensThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 17996\n\nable for holistic scene understanding and reliable execution of downstream applications. Lately, camera-based 3D semantic occupancy prediction, also known as semantic scene completion [31], has become a pioneering method in 3D perception [2, 5, 7, 13, 14, 19, 23, 29, 30, 32, 34, 37–39, 46, 51, 52]. This approach uses RGB camera data to predict the semantic occupancy status of voxels in 3D space, involving both the determination of voxel occupancy and semantic classes of occupied voxels. This research enhances single CAVs’ environmental understanding, improving decision-making in downstream applications for automated vehicles. However, this task based on RGB imagery through collaborative methods has not been explored. To bridge this gap, we delve into the feasibility of 3D semantic occupancy prediction in the context of collaborative perception, as shown in Fig. 1, and introduce the Collaborative Hybrid Feature Fusion (CoHFF) Framework. Our approach involves separate pre-training for the dual subtasks of predicting both semantics and occupancy. We then extract the high-dimensional features from these pretrained models for dual fusion processes: inter-CAV semantic information fusion via V2X Feature Fusion, and intra-CAV fusion of semantic information with occupancy status through task feature fusion. This fusion yields a comprehensive decoding of each voxel’s occupancy and semantic details in 3D space. In order to evaluate the performance of our framework, we extend the existing collaborative perception dataset OPV2V [41]. By reproducing OPV2V scenarios in the CARLA simulator, we collect comprehensive 3D voxel groundtruth with semantic labels across 12 categories. Our experiments show, that for the task of semantic occupancy prediction, a collaborative approach significantly outperforms single-vehicle performance in most categories, as intuitively expected. We also validate the effectiveness of task feature fusion: our findings show that the task fusion, by incorporating features as prior knowledge of each other, enhances subtask performance beyond what separately trained models achieved. Additionally, training tasks independently result in more task-specific features and thus can be easier to compress. Our experiments prove that we achieve more complex 3D perception with a communication volume comparable to existing methods. Contributions To summarize, our main contributions are threefold: • We introduce the first camera-based framework for collaborative semantic occupancy prediction, enabling more precise and comprehensive 3D semantic occupancy segmentation than single-vehicle systems through feature sharing in V2X communication networks. The performance can be enhanced by over 30% via collaboration. • We propose the hybrid feature fusion approach, which not only facilitates efficient collaboration among CAVs, but also markedly enhances the performance over models pre-trained solely for occupancy prediction or semantic voxel segmentation. • We enrich the collaborative perception dataset OPV2V [41] with voxel ground truth containing 12 categories semantic, bolstering the framework evaluation. Our method, CoHFF, achieves comparable results to current leading methods in subsequent 3D perception applications, and additionally offers more semantic details in road environment. 2. Related work 2.1. Collaborative perception In intelligent transportation systems, collaborative perception empowers CAVs to attain a more accurate and holistic understanding of the road environment via V2X communication and data fusion. Typically, data fusion in collaborative perception falls into three categories: early, middle, and late fusion. Given the bandwidth limitations of V2X networks, the prevalent approach is middle fusion, where deep latent space features are exchanged [11, 12, 15, 16, 25, 26, 35, 42, 45, 50]. The advantage of middle fusion lies in its ability to convey critical information beyond mere objectlevel details, bypassing the need to share raw data. The development of datasets specifically designed for collaborative perception [8, 11, 17, 27, 44, 48, 49, 55] has led to remarkable progress in learning-based approaches in recent years. However, these datasets fall short in offering ground truth data for 3D semantic occupancy, which motivates us to extend the dataset in this work, aiming to access the performance of collaborative semantic occupancy prediction. Collaborative Camera 3D Perception. Compared to LiDAR-driven collaborative perception [44], camera-based methods are often more challenging, due to the absence of explicit depth information in RGB data. However, given the lower price and smaller weight of cameras, they inherently have a higher potential for large-scale deployment. Previous work in [40] and [12] has validated that, with collaboration, camera-based 3D perception can match or even outperform LiDAR performance. Given that current research on camera-based collaborative perception either focuses on 3D bounding box detection and BEV semantic segmentation, there remains a research gap in semantic occupancy prediction. Hence, in this study, our aim is to pioneer and explore the topic of collaborative occupancy segmentation. 2.2. Camera-based semantic occupancy prediction Occupancy segmentation, which segments a voxel-based 3D environment model [28, 53], has achieved notable success in the realm of autonomous driving. Original occu- 17997\n\nFigure 2. The CoHFF Framework consists of four key modules: (1) Occupancy Prediction Task Net, for occupancy feature extraction; (2) Semantic Segmentation Task Net, creating semantic plane-based embeddings; (3) V2X Feature Fusion, merging CAV features via deformable self-attention; and (4) Task Feature Fusion, uniting all task features to enhance semantic occupancy prediction. pancy segmentation methods lean heavily on LiDAR, since its point cloud inherits 3D information, aligning naturally with voxel-based environmental models. The recent work proposed in [15] explored the collaborative semantic occupancy prediction based on LiDAR. However, with cameras offering richer environmental details, camera-driven 3D occupancy segmentation is gradually emerging as a novel domain. Recent work in the past year, e.g. [2, 5, 7, 9, 13, 14, 19, 20, 23, 29, 30, 32, 34, 37–39, 51, 52] have also delved into methods for achieving semantic occupancy prediction based on RGB data, yielding promising performance, but only for single vehicle perception. Furthermore, the datasets for the vision-based 3D Semantic Occupancy Prediction, e.g. SemanticKITTI [1], SSC-Benchmark [18], OpenOccupancy [36], and Occ3D [33] have been developed specifically for camera-based 3D occupancy segmentation tasks, thus offering resources for continued research. However, those datasets do not support collaborative perception in multi-agent scenarios. Generally, agents sharing different perspective information through collaboration can further enhance voxel-based occupancy segmentation. Due to semantic occupancy prediction offering a more nuanced 3D environmental understanding than collaborative 3D perception methods focused on bounding boxes or BEV perception, it likely requires the exchange of more complex, higher-dimensional features. Determining the most effective information for communication to facilitate the transmission of denser, more informative data stands as a significant challenge. 2.3. Plane-based features TPVFormer [13] decomposes features for occupancy segmentation into a 3D space. [6] introduced a K-Planes decomposition technique designed to reconstruct static 3D scenes and dynamic 4D videos. Building on the foundations laid by [6], and drawing inspiration from [13], we consider to project semantically relevant information onto orthogonal planes, facilitating information sharing through more streamlined communication. By sharing these planebased features, we establish the foundational structure of our approach. 3. Methodology Our CoHFF framework consists of four key modules, namely occupancy prediction Task Net, Semantic Segmentation Task Net, V2X Feature Fusion and Task Feature Fusion, as shown in Fig. 2. It achieves camera-based collaborative semantic occupancy prediction by sharing planebased semantic features via V2X communication. 3.1. Problem formulation Given a network of CAVs, defined by a global communication network represented as an undirect graph G = (N, E). For each CAV i, the set of connected CAVs, is denoted by Ni = {j|(i, j) ∈E}, where E is the existing communication links between two CAVs, and j denotes the index of the CAVs connecting to i. We consider the input data in RGB format, and denote Ii as the image data for a CAV i. The environment model is represented as a 3D voxel grid in one hot embedding V ∈RX×Y ×Z×C, where X, Y and Z are voxel grid dimensions. For each CAV i, Vi represents the predicted occupancy of voxels, while V(0) i represents the ground truth of these voxels. The objective of collaborative semantic occupancy prediction, as aligned with the optimization problem in [11, 12], is defined as follows: \\m ax _ { \\theta , M} \\sum _i g(\\Phi _\\theta (\\mathcal {I}_i, \\{\\mathcal {M}_{i\\rightarrow j}|j\\in \\mathcal {N}_{i}\\}), \\mathbf {V}_i^{(0)}), \\\\ s.t. \\sum _i |\\{\\mathcal {M}_{i\\rightarrow j}|j\\in \\mathcal {N}_{i}\\}|\\leq B, \\label {eq:cp} (1) where g(·) is the perception metric for optimization. Φ represents the model parametrized by θ, and Mi→j denotes 17998\n\nthe message transmitted from CAV i to CAV j. The size of these messages is constrained by a communication budget upper bound B ∈R+. Considering the communication upper bound, instead of directly sending high-dimensional voxel-sized features FV , we opt to transmit features FP from orthogonal planes. This approach reduces the messages from MFV ∈ RX×Y ×Z×F to MPxz ∈ RX×Z×F and MPyz ∈ RY ×Z×F , where Pxz and Pyz denote the features projected on the xz- and yz-planes respectively. F represents the length of a single feature vector. For instance, in a voxel space of 100 × 100 × 8 with a feature dimension of 128, transmitting orthogonal plane features can reduce communication volume by 50 ×, from 39.05 MB to 0.78 MB, which is comparable to existing collaborative perception methods, yet it offers more extensive and detailed semantic 3D scene information. Based on these considerations, we introduce our framework in the following section. 3.2. Framework design We divide our method into two distinct pre-communication tasks: 3D occupancy prediction and semantic voxel segmentation. We believe that occupancy features enhance the semantic segmentation performance by providing geometry insight of distinct object classes. Meanwhile, semantic information can suggest changes of a voxel occupancy. Based on this interplay, our approach initially focuses on independent pre-training for each task. Then we fuse the features from both tasks to learn a combined semantic occupancy predictor that yields better performance for each individual task. This assumption is experimentally validated by the ablation study in Tab. 3. Consequently, our framework comprises two specialized pre-trained networks: an occupancy prediction task network and a semantic segmentation task network, as shown in Fig. 2. Occupancy prediction task network. The occupancy prediction necessitates the conversion of 2D image data into a 3D occupancy grid. We first use an off-the-shelf depth prediction network Φdepth(·) to determine the depth of each pixel. Following the work in [11, 12], we employ CaDNN [3] for depth estimation. This depth data is then embedded into voxel space through a 3D Emedder, resulting in a preliminary voxel representation. This voxelbased road environment is further completed by a 3D occupancy encoder Φocc(·). Finally, the occupancy task features Focc ∈RX×Y ×Z×F is extracted for task fusion. Semantic segmentation task network. In the segmentation network, we process RGB data to generate feature maps Fseg using Φimg(·), which are then subjected to deformable cross-attention [54] to facilitate mapping onto a 3D semantic segmentation space. Drawing inspiration from K-Planes [6] and TPVformer [13], we project these features onto three spatially orthogonal planes P = {Pxy, Pxz, Pyz}. Among these dense and informative 3D feature representations, two are transmitted via V2X messages, i.e. M = {MPxz, MPyz}. The reason behind not sending the Pxy plane, is that the we use the Pxy of the ego vehicle for reconstructing the 3D features, which facilitates the alignment of the feature space with the detection range of interest of ego vehicle. Both networks generate high-dimensional features that are fed into a hybrid feature fusion network, thereby forming the core of CoHFF for semantic occupancy prediction. 3.3. Hybrid feature fusion V2X Feature Fusion. Given one CAV j communicating to the ego vehicle i, the features of the CAV condensed by the segmentation network can contain overlapping information, particularly regarding semantics in proximity to the ego vehicle, which the ego vehicle itself can accurately predict. We implement a masking technique to selectively filter these plane-based features of the CAV, before they are communicated to the ego vehicle. By adjusting a sparsification rate hyperparameter, we reduce the volume of the CAV´s plane-based features shared during collaboration, in line with the communication budget. The compressed message ¯ M = { ¯ MPxz, ¯ MPyz} can be acquired as follows: \\b a r {\\m a t hbf { P}} _ j ^{x z } ,\\b a r {\\mathbf {P}}_j^{yz} \\leftarrow \\mathbf {P}_j^{xz} \\odot \\mathbf {H}_j^{xz} ,\\mathbf {P}_j^{yz} \\odot \\mathbf {H}_j^{yz}, (2) where Hxz j and Hyz j represent the learnable feature masks for features on x-z and y-z planes. Additionally, we ensure relative pose awareness between the ego vehicle and other CAVs. Specifically, we feed the filtered plane features and the relative pose information into an MLP network combined with a Sigmoid function, in line with the methodology proposed in [24]. We now attend these pose-aware filtered plane features from the CAV (¯Pxz j , ¯Pyz j ) over the three plane features of the ego vehicle (Pxy i , Pxz i , Pyz i ). In particular, we use deformable self-attention to update the all five feature planes. The fusion and updating of these planes are accomplished by plane self-attention (PSA), as follows: PSA( \\ mathb f {p}) = D A ( \\mat h bf {p},\\mathcal {R},\\{\\mathbf {P}_i, \\bar {\\mathbf {P}}_j^{xz}, \\bar {\\mathbf {P}}_j^{yz}|j\\in \\mathcal {N}_i\\} ), \\label {eq:psa_fusion} (3) where DA(·) is deformable self-attention, p ∈RF is a query and R is a set of reference points, as described in [54]. Pi denotes all the three planes in ego vehicle. The updated 2D plane features are used in the next step to reconstruct 3D semantic segmentation features Fseg. The semantic segmentation feature f seg x,y,z at a specific Voxel location x, y, z can be reconstructed as follows: \\m athbf {f} _{x , y,z} ^{s e g} = \\m a th b f { p}^{xy}_{i,z} + \\bar {\\mathbf {p}}^{xz}_{j,y} + \\bar {\\mathbf {p}}^{yz}_{j,x} \\in \\mathbb {R}^{F}, \\forall j \\in \\mathcal {N}_i, \\label {eq:expand} (4) where ¯pxz j,y and ¯pyz j,x is plane features from CAV j, and pxy i,z is the plane (BEV) features from ego vehicle. This idea of 17999\n\nAlgorithm 1 : CoHFF framework for collaborative semantic occupancy prediction. 1: for each CAV i in parallel do 2: Focc i ←Φocc(Proj(Φdepth(Ii), Ii))) 3: Fimg i ←Φimg(Ii) 4: update plane-based features Pxz i , Pyz i , Pxy i using deformable cross- and self-attention [54] 5: ¯Pxz i , ¯Pyz i ←Pxz i ⊙Hxz i , Pyz i ⊙Hyz i 6: ¯ Mi ←{¯Pxz i , ¯Pyz i } 7: CAV i broadcasts messages ¯ Mi 8: for j ∈Ni do 9: CAV i receives messages ¯ Mj 10: end for 11: update {Pi, ¯Pxz j , ¯Pyz j |j ∈Ni} using self-attention based on (3) 12: reconstruct F seg j based on (4) \\triangleright VFF 13: Vi ←Φtff(Focc i , Fseg i , {Fseg j |j ∈Ni}) \\triangleright TFF 14: end for sum of projected features for 3D reconstruction is originally proposed in [13], with our work adapting it to multi-agent scenarios. Task Feature Fusion. After retrieving global semantic information as Fseg, the final step aims at fusion with features Focc from the occupancy prediction task. To accomplish this, Fseg and Focc are concatenated and passed to a 3D depth-wise convolution network [47], in order to produce the final semantic voxel map. This task feature fusion network Φtff(·) is implemented as follows: \\ mathbf {V _ i } = \\ P hi ^{ t ff } (\\ma t hbf {F}_i^{occ}, \\mathbf {F}_i^{seg}, \\{\\mathbf {F}_j^{seg}|j\\in \\mathcal {N}_i\\})\\in \\mathbb {R}^{X \\times Y \\times Z \\times C}. (5) The CoHFF pseudocode is given in Algorithm 1. 3.4. Losses We train the completion network training using focal loss proposed in [21], applying it to a dataset with binary labels {0, 1}. For both the segmentation network and the hybrid feature fusion network, we employ a weighted crossentropy loss to train for semantic labels. Notably, in this context, the label for the empty is also designated as 0. 4. Dataset To effectively evaluate collaborative semantic occupancy prediction, a dataset that supports collaborative perception and includes 3D semantic occupancy labels is crucial. Thus, we enhance the OPV2V dataset [41] by integrating 12 different 3D semantic occupancy labels, as shown in Tab. 4 This enhancement is achieved using the high-fidelity CARLA simulator [4] and the OpenCDA autonomous driving simulation framework [43]. We position four semantic LiDARs at the original camera sites to precisely capture the Table 1. Comparison 3D object detection with AP2 of vehicles. Approach # Agents AP@0.5 AP@0.7 DiscoNet (NeurIPS 21) Up to 7 36.00 12.50 V2X-ViT (ECCV 22) Up to 7 39.82 16.43 Where2Comm (NeurIPS 22) Up to 7 47.30 19.30 CoCa3D (CVPR 23) 71 69.10 49.50 CoHFF Up to 7 48.51 36.39 CoCa3D-2 (CVPR 23) 2 25.90 12.60 CoHFF 2 36.63 27.95 1 CoCa3D is trained on OPV2V+, where extended agents provide more input information for better results. 2 We calculate the 3D IoU by comparing the predicted voxels with the ground truth voxels for each object, rather than using 3D bounding boxes due to the potential unnecessary occupancy in 3D bounding boxes. Table 2. Comparison of BEV semantic segmentation with IoU in the class of Vehicle, Road and Others. Approach # Agents Vehicle Road Others1 CoBEVT (CoRL 22) 2 46.13 52.41 - CoHFF 2 47.40 63.36 40.27 CoBEVT (CoRL 22) Up to 7 60.40 63.00 - CoHFF Up to 7 64.44 57.28 45.89 1 It refers to additional object classes identified through semantic segmentation predictions projected onto the BEV plane. These categories include buildings, fences, terrain, poles, vegetation, walls, guard rails, traffic signs, and bridges. The IoU for these objects is calculated and reported as IoU. semantic occupancy ground truth within the cameras’ FoV. In addition, we associate ground truth data from all CAVs to create a detailed collaborative ground truth for collaborative supervision. Furthermore, to comprehensively capture occluded semantic occupancies for all CAVs, we include a simulation replay in our data collection process, where each CAV is equipped with 18 semantic LiDARs. This strategic configuration is crucial for effectively evaluating completion tasks, as it guarantees extensive data collection, encompassing areas not visible in direct associated FoV. In alignment with the original OPV2V protocol, we replay the simulation and generate a multi-tier ground truth. 5. Experimental evaluation 5.1. Experiment setup Baselines. Considering the unexplored domain of collaborative occupancy segmentation, we extend the findings from CoHFF to address downstream applications, including BEV perception and 3D detection. In our analysis, we evaluate these outcomes with those from state-of-the-art collaborative perception models that employ multi-view cameras: CoBEVT [40] for BEV perception and CoCa3D [12] for 3D detection. Furthermore, we examine contemporary 18000\n\nTable 3. CoHFF achieves robust IoU and mIoU performance, when the communication volume (CV) is reduced by setting various sparsification rates (Spar. Rate). Spar. Rate 0.00 0.50 0.80 0.95 0.99 CV (MB) (↓) 16.53 8.27 3.31 0.83 0.17 IoU (↑) 50.46 49.56 49.53 48.52 48.02 mIoU (↑) 34.16 32.97 32.70 30.13 29.48 methods that integrate alternative modalities, particularly those blending LiDAR with camera inputs or relying solely on LiDAR, including DiscoNet [16], V2X-ViT [42] and Where2Comm [11]. Implementation details. Following the previous work for collaborative perception evaluation on the OPV2V dataset used in [11], we utilize a 40×40×3.2 meter detection area with a grid size of 100 x 100 x 8, resulting in a voxel size of 0.4 m3. We allow CAVs to transmit and share features with a length of 128 for V2X Feature Fusion. Our experiment incorporates the analysis of 12 semantic labels plus an additional empty label. We employ CaDNN [3] with 50 depth categories and a single out-of-range category for depth estimation, as well as ResNet101 [10] and FPN [22] as RGB the image backbone. For Voxel completion, we utilize a 3D depth-wise CNN [47] and use deformable attention [54] in hybrid feature fusion. Evaluation metrics. Following the evaluation of semantic occupancy prediction in previous work, such as [2, 13, 19], we primarily utilize the metric Intersection over Union (IoU) for evaluation. This involves calculating IoU for each individual class and the mean IoU (mIoU) across all classes. Additionally, for evaluations in subsequent applications, we compute the Average Precision (AP) at IoU threshold of 0.5 and 0.7, and BEV 2D IoU to compare with other baselines. Specifically, the AP value is calculated only for voxels labeled as vehicles, and the IoU is determined for each pair of predicted and actual vehicles. For BEV IoU, voxels are projected onto the BEV plane and categorized into the corresponding semantic classes. 5.2. Comparison Collaborative 3D object detection. First, we compare the performance of CoHFF in 3D detection applications. As shown in Tab. 1, with up to 7 agents’ collaborative perception, CoHFF achieves comparable performance to Where2comm at AP@0.5 and obtains an 88.5% improvement at AP@0.7. We believe this is primarily due to semantic occupancy prediction, which makes the perception results closer to the actual observed shapes, rather than inferring a non-existent bounding box in the scenarios. We also observe that CoCa3D, on the OPV2V+ dataset [12], achieves significantly better performance due to receiving more information from CAVs. To compare directly with CoCa3D, we also conduct scenarios where only two agents communicated at a time. We can see that CoHFF has made significant improvements at both AP@0.5 and AP@0.7. Collaborative BEV segementation. Tab. 2 presents a comparison between CoHFF and CoBEVT in BEV semantic segmentation. Note that errors in height prediction from 3D voxel occupancy mapping to the BEV plane may be overlooked during the projection process. Despite this, CoHFF achieves even better performance in predicting vehicles and roads in BEV compared to CoBEVT. Additionally, CoHFF is capable of detecting a wider range of other semantic categories in 3D occupancy. 5.3. Ablation study To validate our hypothesis that independently obtained semantic and occupancy feature information can simultaneously strengthen the original semantic and occupancy tasks, we have decomposed the semantic occupancy prediction into two separate tasks. Tab. 4 shows an ablation study by altering the components used. Meanwhile, we also verify the enhancement of collaborative perception over single vehicle perception in terms of semantic occupancy. CoHFF for occupancy prediction. When focusing solely on binary occupancy predictions (as shown at Occ. Pred. in Tab. 4), we use voxels processed from raw LiDAR point clouds as a reference, and analyze the IoU in different semantic classes based on semantic occupancy in ground truth. It is observed that by utilizing an occupancy prediction task network to process depth predictions, the overall prediction accuracy is enhanced. Additionally, significant improvements in predicting large objects in occupancy results are noted by integrating features from a semantic segmentation task network, leading to an increased overall IoU. However, a concurrent decline in the mIoU is observed alongside the increase in IoU. This phenomenon is attributed to the influence of semantic features, which seem to steer the model towards prioritizing easily detectable categories, potentially at the expense of smaller or less distinct categories. Finally, through collaboration, the overall IoU and mIoU are further strengthened on the basis of task feature fusion. CoHFF for semantic segmentation. In our semantic segmentation task (as shown at Sem. Seg. in Tab. 4), after integrating features from occupancy prediction, we observe an approximate 2% increase in IoU, but a more substantial over 41% enhancement in mIoU. We attribute this improvement to the features derived from occupancy prediction, which seem to aid the easier detection of smaller-scale objects, thereby refining their semantic predictions. Consistent with the occupancy prediction task, the final collaboration further elevates the results of semantic segmentation. 18001\n\nTable 4. Component ablation study on occupancy prediction (Occ. Pred.), semantic segmentation (Sem. Seg.), and semantic occupancy prediction (Sem. Occ. Pred.) tasks. The components include: Occupancy Prediction Task Net (OPTN), Semantic Segmentation Task Net (SSTN), Task Feature Fusion (TFF) and V2X Feature Fusion (VFF). The gray color in table cells indicates that the corresponding component is not applicable for the task. Task type Occ. Pred. Sem. Seg. Sem. Occ. Pred. OPTN RL 1 ✓ ✓ ✓ ✓ ✓ SSTN ✓ ✓ ✓ ✓ ✓ TFF ✓ ✓ ✓ ✓ ✓ ✓ VFF (Collaboration) ✓ ✓ ✓ IoU (↑) 49.35 67.22 76.62 86.37 41.30 42.11 51.38 38.52 50.46 mIoU (↑) 57.12 64.01 59.16 69.15 21.59 30.51 35.91 24.85 34.16 Building (5.40%) 67.50 68.36 41.29 48.41 9.65 27.25 15.06 21.04 25.72 Fence (0.85%) 59.40 62.05 51.60 65.01 11.67 30.29 30.91 20.50 27.83 Terrain (4.80%) 43.60 49.78 68.21 79.81 51.18 51.41 61.98 43.93 48.30 Pole (0.39%) 66.30 70.67 62.31 64.12 2.14 36.80 40.74 31.66 42.74 Road (40.53%) 51.47 77.78 91.26 93.00 56.82 60.02 64.09 55.83 61.77 Side walk (35.64%) 45.46 58.46 74.37 90.53 25.22 16.87 36.03 17.31 39.62 Vegetation (1.11%) 43.61 44.43 38.87 41.57 9.12 22.13 20.99 14.49 20.59 Vehicles (9.14%) 41.40 63.53 59.52 76.48 59.58 69.81 75.88 58.55 63.28 Wall (2.01%) 71.51 79.35 49.63 81.20 32.55 39.80 58.49 33.30 58.27 Guard rail (0.04%) 49.67 46.03 41.35 43.33 1.10 1.95 1.80 1.54 1.94 Traffic signs (0.05%) 68.98 69.41 52.35 62.54 0.00 9.77 11.69 0.00 16.33 Bridge (0.04%) 76.53 78.23 79.08 83.84 0.00 0.00 13.30 0.00 3.53 1 RL (Raw LiDAR) is used as a baseline for the evaluation on the task of occupancy prediction. Collaboration enhances semantic occupancy prediction. In the final evaluation of our semantic occupancy prediction (see column Sem. Occ. Pred. in Tab. 4), we further demonstrate the benefits brought by collaboration. By collaboration, the IoU for each category is improved. Notably, some previously undetectable, low-prevalence categories such as traffic signs and bridges can be detected after collaboration. Ultimately, there is an approximate 31% increase in overall IoU and around a 37% enhancement in mIoU. 5.4. Robustness with low communication budget In Tab. 3, we increase the sparsification rate to mask the plane-based features transmitted by CAVs, achieving efficient V2X information exchange under a low communication budget. The CoHFF model exhibits stable IoU performance across various levels of sparsification. Even when the communication volume is shrinked by 97 ×, the accuracy only decreases by 5% compared to the original. Meanwhile, the mIoU drops by 15%. Despite this, due to the model’s training under collaborative supervision, it still outperforms the non-collaborative approach. 5.5. Visual analysis Fig. 3 presents visual results from the CoHFF model, which are compared from multiple perspectives with the ground truth data, i.e. the ground truth in the ego vehicle’s FoV (Ego GT) and the ground truth across all CAVs FoVs (Collaborative GT). It is evident that, overall, the model accurately predicts voxels in various classes such as roads, sidewalks, traffic signs, walls, and fences. We particularly focus on vehicle predictions, as they are among the most critical categories in road environment perception. For clarity, each vehicle object in the figure is numbered. Vehicle geometry completion. The CoHFF model predicts more complete vehicle objects than those in the Ego GT, such as vehicles 1, 3, 4, and 7. In some instances, the predictions even surpass the completeness of vehicle shapes found in Collaborative GT. Occluded vehicle detection. CoHFF successfully predicts vehicles outside of the FoV, such as vehicle 6, by utilizing minimal pixel information. This demonstrates that CoHFF can effectively detect occluded vehicles. 6. Conclusion In this work, we explore the task of camera-based semantic occupancy prediction through the lens of collaborative perception. We introduce the CoHFF framework, which significantly enhances the perception performance by over 30% through integrating features from different tasks and various CAVs. Since currently no dataset specifically de- 18002\n\nFigure 3. Illustration of collaborative semantic occupancy prediction from multiple perspectives, compared to the ground truth in the ego vehicle’s FoV and the collaborative FoV across CAVs. This visualization emphasizes the advanced object detection capabilities in collaborative settings, particularly for objects obscured in the ego vehicle’s FoV, such as the vehicle with ID 6. signed for collaborative semantic occupancy prediction exists, we also extend the OPV2V dataset with 3D semantic occupancy labels. Our experiments validate that collaboration yields better semantic occupancy prediction results than single-vehicle approaches. Limitation. Although we demonstrate the immense potential of collaboration for semantic occupancy prediction using simulation data, its performance with real-world data remains to be verified. The collection and development of a specialized dataset, repleted with semantic occupancy labels and derived from multi-agent perception scenarios in real-world settings, are highly anticipated. 7. Acknowledgements This work was supported by the German Federal Ministry for Digital and Transport (BMVI) in the project ”5GoIng – 5G Innovation Concept Ingolstadt”. 18003\n\nReferences [1] J. Behley et al. Towards 3D LiDAR-based semantic scene understanding of 3D point cloud sequences: The SemanticKITTI Dataset. The International Journal on Robotics Research, 40(8-9):959–967, 2021. DOI: 10.1177/02783649211006735. 3 [2] Anh-Quan Cao and Raoul de Charette. MonoScene: Monocular 3D semantic scene completion. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3991–4001. IEEE, 2022. DOI: 10.1109/CVPR52688.2022.00396. 2, 3, 6 [3] Cody Ceading, Ali Harakeh, Julia Chae, and Steven L. Waslander. Categorical depth distribution network for monocular 3D object detection. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8551–8560, 2021. DOI: 10.1109/CVPR46437.2021.00845. 4, 6 [4] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on Robot Learning, pages 1–16. PMLR, 2017. 5 [5] Shaoheng Fang, Zi Wang, Yiqi Zhong, Junhao Ge, and Siheng Chen. TBP-Former: Learning temporal bird’s-eye-view pyramid for joint perception and prediction in vision-centric autonomous driving. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1368–1378. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00138. 2, 3 [6] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12479–12488. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.01201. 3, 4 [7] Aditya Nalgunda Ganesh, Dhruval Pobbathi Badrinath, Harshith Mohan Kumar, Priya SS, and Surabhi Narayan. OCTraN: 3D occupancy convolutional transformer network in unstructured traffic scenarios. arXiv preprint arXiv:2307.10934, 2023. 2, 3 [8] R Hao et al. Rcooper: A real-world large-scale dataset for roadside cooperative perception. arXiv preprint arXiv:2403.10145, 2024. 2 [9] Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, and Daniel Cremers. S4C: Self-supervised semantic scene completion with neural fields. arXiv preprint arXiv:2310.07522, 2023. 3 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. DOI: 10.1109/CVPR.2016.90. 6 [11] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen. Where2comm: Communication-efficient collaborative perception via spatial confidence maps. Advances in Neural Information Processing Systems (NeurIPS), 35: 4874–4886, 2022. 1, 2, 3, 4, 6 [12] Yue Hu et al. Collaboration helps camera overtake LiDAR in 3D detection. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9243–9252. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00892. 1, 2, 3, 4, 5, 6 [13] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for visionbased 3D semantic occupancy prediction. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9223–9232. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00890. 2, 3, 4, 5, 6 [14] Haoyi Jiang et al. Symphonize 3D semantic scene completion with contextual instance queries. arXiv preprint arXiv:2306.15670, 2023. 2, 3 [15] Yiming Li, Juexiao Zhang, Dekun Ma, Yue Wang, and Chen Feng. Multi-robot scene completion: Towards task-agnostic collaborative perception. In Conference on Robot Learning, pages 2062–2072. PMLR, 2023. 1, 2, 3 [16] Yiming Li et al. Learning distilled collaboration graph for multi-agent perception. Advances in Neural Information Processing Systems (NeurIPS), 34:29541–29552, 2021. 1, 2, 6 [17] Yiming Li et al. V2X-Sim: Multi-agent collaborative perception dataset and benchmark for autonomous driving. IEEE Robotics and Automation Letters, 7(4):10914–10921, 2022. DOI: 10.1109/LRA.2022.3192802. 2 [18] Yiming Li et al. SSCBench: A large-scale 3D semantic scene completion benchmark for autonomous driving. arXiv preprint arXiv:2306.09001, 2023. 3 [19] Yiming Li et al. VoxFormer: Sparse voxel transformer for camera-based 3D semantic scene completion. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9087–9098. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00877. 2, 3, 6 [20] Zhiqi Li et al. FB-OCC: 3D occupancy prediction based on forward-backward view transformation. arXiv preprint arXiv:2307.01492, 2023. 3 [21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318–327, 2020. DOI: 10.1109/TPAMI.2018.2858826. 5 [22] Tsung-Yi Lin et al. Feature pyramid networks for object detection. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2117–2125, 2017. DOI: 10.1109/CVPR.2017.106. 6 [23] Jihao Liu et al. Towards better 3D knowledge transfer via masked image modeling for multi-view 3D understanding. arXiv preprint arXiv:2303.11325, 2023. 2, 3 [24] Yingfei Liu et al. Petrv2: A unified framework for 3D perception from multi-camera images. In 2023 IEEE/CVF International Conference on Computer Vision (CVPR), pages 3262–3272, 2023. 4 [25] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira. When2com: Multi-agent perception via communication graph grouping. In 2020 IEEE/CVF Conference on Computer Vision and Pat- 18004\n\ntern Recognition (CVPR), pages 4106–4115. IEEE, 2020. DOI: 10.1109/CVPR42600.2020.00416. 1, 2 [26] Yen-Cheng Liu et al. Who2com: Collaborative perception via learnable handshake communication. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 6876–6883. IEEE, 2020. DOI: 10.1109/ICRA40945.2020.9197364. 1, 2 [27] C. Ma et al. Holovic: Large-scale dataset and benchmark for multi-sensor holographic intersection and vehicleinfrastructure cooperative. arXiv preprint arXiv:2403.02640, 2024. 2 [28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4460–4470, 2019. DOI: 10.1109/CVPR.2019.00459. 2 [29] Ruihang Miao et al. OccDepth: A depth-aware method for 3D semantic scene completion. arXiv preprint arXiv:2302.13540, 2023. 2, 3 [30] Chen Min et al. Occ-BEV: Multi-camera unified pretraining via 3D scene reconstruction. arXiv preprint arXiv:2305.18829, 2023. 2, 3 [31] Luis Roldao, Raoul De Charette, and Anne VerroustBlondet. 3D semantic scene completion: A survey. International Journal of Computer Vision, 130(8):1978–2005, 2022. 2 [32] Zhiyu Tan et al. Ovo: Open-vocabulary occupancy. arXiv preprint arXiv:2305.16133, 2023. 2, 3 [33] Xiaoyu Tian et al. Occ3D: A large-scale 3D occupancy prediction benchmark for autonomous driving. arXiv preprint arXiv:2304.14365, 2023. 3 [34] Wenwen Tong et al. Scene as occupancy. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8406–8415, 2023. 2, 3 [35] Tsun-Hsuan Wang et al. V2VNet: Vehicle-to-vehicle communication for joint perception and prediction. In 2020 European Conference on Computer Vision (ECCV), pages 605– 621, Glasgow, UK, 2020. Springer. DOI: 10.1007/978-3- 030-58536-5 36. 1, 2 [36] Xiaofeng Wang et al. OpenOccupancy: A large scale benchmark for surrounding semantic occupancy perception. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 17850–17859, 2023. 3 [37] Yuqi Wang, Yuntao Chen, Xingyu Liao, Lue Fan, and Zhaoxiang Zhang. PanoOcc: Unified occupancy representation for camera-based 3D panoptic segmentation. arXiv preprint arXiv:2306.10013, 2023. 2, 3 [38] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. PETNeuS: Positional encoding tri-planes for neural surfaces. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12598–12607. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.01212. [39] Yi Wei et al. SurroundOcc: Multi-camera 3D occupancy prediction for autonomous driving. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 21729–21740, 2023. 2, 3 [40] Runsheng Xu et al. CoBEVT: Cooperative bird’s eye view semantic segmentation with sparse transformers. arXiv preprint arXiv:2207.02202, 2022. 2, 5 [41] Runsheng Xu et al. OPV2V: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication. In 2022 International Conference on Robotics and Automation (ICRA), pages 2583–2589. IEEE, 2022. DOI: 10.1109/ICRA46639.2022.9812038. 2, 5 [42] Runsheng Xu et al. V2X-VIT: Vehicle-to-everything cooperative perception with vision transformer. In 2022 European Conference on Computer Vision (ECCV), pages 107– 124. Springer, 2022. DOI: 10.1007/978-3-031-19842-7 7. 1, 2, 6 [43] Runsheng Xu et al. The OpenCDA open-source ecosystem for cooperative driving automation research. IEEE Transactions on Intelligent Vehicles, 8(4):2698–2711, 2023. DOI: 10.1109/TIV.2023.3244948. 5 [44] Runsheng Xu et al. V2V4Real: a real-world large-scale dataset for vehicle-to-vehicle cooperative perception. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13712–13722. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.01318. 2 [45] Kun Yang et al. Spatio-temporal domain awareness for multi-agent collaborative perception. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 23383–23392, 2023. 1, 2 [46] Jiawei Yao et al. NDC-Scene: Boost monocular 3D semantic scene completion in normalized device coordinates space. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9455–9465, 2023. 2 [47] Rongtian Ye, Fangyu Liu, and Liqiang Zhang. 3D depthwise convolution: Reducing model parameters in 3D vision tasks. In 32nd Canadian Conference on Artificial Intelligence (Canadian AI), Proc. 32, pages 186–199. Springer, 2019. DOI: 10.1007/978-3-030-18305-9 15. 5, 6 [48] Haibao Yu et al. DAIR-V2X: A large-scale dataset for vehicle-infrastructure cooperative 3D object detection. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 21361–21370. IEEE, 2022. DOI: 10.1109/CVPR52688.2022.02067. 2 [49] Haibao Yu et al. V2X-Seq: A large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5486–5495. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00531. 2 [50] Haibao Yu et al. Vehicle-infrastructure cooperative 3D object detection via feature flow prediction. arXiv preprint arXiv:2303.10552, 2023. 1, 2 [51] Yunpeng Zhang, Zheng Zhu, and Dalong Du. OccFormer: Dual-path transformer for vision-based 3D semantic occupancy prediction. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9433–9443, 2023. 2, 3 [52] Zaibin Zhang, Lijun Wang, Yifan Wang, and Huchuan Lu. BEV-IO: Enhancing bird’s-eye-view 3D detection with instance occupancy. arXiv preprint arXiv:2305.16829, 2023. 2, 3 18005\n\n[53] Yin Zhou and Oncel Tuzel. VoxelNet: End-to-end learning for point cloud based 3D object detection. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4490–4499, 2018. DOI: 10.1109/CVPR.2018.00472. 2 [54] Xizhou Zhu et al. Deformable DETR: Deformable transformers for end-to-end object detection. In 2021 International Conference on Learning Representations (ICLR), 2021. 4, 5, 6 [55] W. Zimmer et al. TUMTraf V2X cooperative perception dataset. arXiv preprint arXiv:2403.01316, 2024. 2 18006",
  "pages": [
    {
      "page": 1,
      "text_raw": "Collaborative Semantic Occupancy Prediction\nwith Hybrid Feature Fusion in Connected Automated Vehicles\nRui Song1,2 *, Chenwei Liang1, Hu Cao2, Zhiran Yan3, Walter Zimmer2,\nMarkus Gross1, Andreas Festag1,3, Alois Knoll2\n1Fraunhofer IVI\n2Technical University of Munich\n3Technische Hochschule Ingolstadt\nhttps://rruisong.github.io/publications/CoHFF\nFigure 1. Collaborative semantic occupancy prediction leverages the power of collaboration in multi-agent systems for 3D occupancy\nprediction and semantic segmentation. This approach enables a deeper understanding of the 3D road environment by sharing latent features\namong connected automated vehicles (CAVs), surpassing the ground truth captured by a multi-camera system in the ego vehicle.\nAbstract\nCollaborative perception in automated vehicles lever-\nages the exchange of information between agents, aiming to\nelevate perception results. Previous camera-based collabo-\nrative 3D perception methods typically employ 3D bound-\ning boxes or bird’s eye views as representations of the en-\nvironment. However, these approaches fall short in offering\na comprehensive 3D environmental prediction. To bridge\nthis gap, we introduce the first method for collaborative 3D\nsemantic occupancy prediction. Particularly, it improves\nlocal 3D semantic occupancy predictions by hybrid fusion\nof (i) semantic and occupancy task features, and (ii) com-\npressed orthogonal attention features shared between vehi-\ncles. Additionally, due to the lack of a collaborative percep-\ntion dataset designed for semantic occupancy prediction,\nwe augment a current collaborative perception dataset to\ninclude 3D collaborative semantic occupancy labels for a\nmore robust evaluation. The experimental findings highlight\nthat: (i) our collaborative semantic occupancy predictions\nexcel above the results from single vehicles by over 30%,\nand (ii) models anchored on semantic occupancy outpace\n*Corresponding author, email address: rui.song@ivi.fraunhofer.de\nstate-of-the-art collaborative 3D detection techniques in\nsubsequent perception applications, showcasing enhanced\naccuracy and enriched semantic-awareness in road envi-\nronments.\n1. Introduction\nCollaborative perception, also known as cooperative per-\nception, significantly improves the accuracy and complete-\nness of each connected and automated vehicle’s (CAV)\nsensing capabilities by integrating multiple viewpoints, sur-\npassing the limitations of single-vehicle systems [11, 12,\n15, 16, 25, 26, 35, 42, 45, 50].\nThis approach enables\nCAVs to achieve comparable or superior perception abili-\nties, even with cost-effective sensors. Notably, recent re-\nsearch in [12] suggests that camera-based systems may out-\nperform LiDAR in 3D perception through collaboration\nin Vehicle-to-Everything (V2X) communication networks.\nPrevious studies in camera-based collaborative perception\ntypically processed inputs from various CAVs into simpli-\nfied formats such as 3D bounding boxes or Bird’s Eye View\n(BEV) segmentation. While efficient, these methods tend\nto miss important 3D semantic details, which are indispens-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17996\n",
      "text_clean": "Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles Rui Song1,2 *, Chenwei Liang1, Hu Cao2, Zhiran Yan3, Walter Zimmer2, Markus Gross1, Andreas Festag1,3, Alois Knoll2 1Fraunhofer IVI 2Technical University of Munich 3Technische Hochschule Ingolstadt https://rruisong.github.io/publications/CoHFF Figure 1. Collaborative semantic occupancy prediction leverages the power of collaboration in multi-agent systems for 3D occupancy prediction and semantic segmentation. This approach enables a deeper understanding of the 3D road environment by sharing latent features among connected automated vehicles (CAVs), surpassing the ground truth captured by a multi-camera system in the ego vehicle. Abstract Collaborative perception in automated vehicles leverages the exchange of information between agents, aiming to elevate perception results. Previous camera-based collaborative 3D perception methods typically employ 3D bounding boxes or bird’s eye views as representations of the environment. However, these approaches fall short in offering a comprehensive 3D environmental prediction. To bridge this gap, we introduce the first method for collaborative 3D semantic occupancy prediction. Particularly, it improves local 3D semantic occupancy predictions by hybrid fusion of (i) semantic and occupancy task features, and (ii) compressed orthogonal attention features shared between vehicles. Additionally, due to the lack of a collaborative perception dataset designed for semantic occupancy prediction, we augment a current collaborative perception dataset to include 3D collaborative semantic occupancy labels for a more robust evaluation. The experimental findings highlight that: (i) our collaborative semantic occupancy predictions excel above the results from single vehicles by over 30%, and (ii) models anchored on semantic occupancy outpace *Corresponding author, email address: rui.song@ivi.fraunhofer.de state-of-the-art collaborative 3D detection techniques in subsequent perception applications, showcasing enhanced accuracy and enriched semantic-awareness in road environments. 1. Introduction Collaborative perception, also known as cooperative perception, significantly improves the accuracy and completeness of each connected and automated vehicle’s (CAV) sensing capabilities by integrating multiple viewpoints, surpassing the limitations of single-vehicle systems [11, 12, 15, 16, 25, 26, 35, 42, 45, 50]. This approach enables CAVs to achieve comparable or superior perception abilities, even with cost-effective sensors. Notably, recent research in [12] suggests that camera-based systems may outperform LiDAR in 3D perception through collaboration in Vehicle-to-Everything (V2X) communication networks. Previous studies in camera-based collaborative perception typically processed inputs from various CAVs into simplified formats such as 3D bounding boxes or Bird’s Eye View (BEV) segmentation. While efficient, these methods tend to miss important 3D semantic details, which are indispensThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 17996",
      "figure_captions": [
        {
          "bbox": [
            50.11199951171875,
            384.1119689941406,
            545.1110229492188,
            414.996337890625
          ],
          "text": "Figure 1. Collaborative semantic occupancy prediction leverages the power of collaboration in multi-agent systems for 3D occupancy prediction and semantic segmentation. This approach enables a deeper understanding of the 3D road environment by sharing latent features among connected automated vehicles (CAVs), surpassing the ground truth captured by a multi-camera system in the ego vehicle."
        }
      ],
      "images": [
        {
          "image_id": "p1_cluster_001",
          "page": 1,
          "file_path": "images/page_0001_cluster_001.png",
          "bbox": [
            46.61070251464844,
            257.416259765625,
            546.4961547851562,
            373.27801513671875
          ],
          "caption": "a diagram showing the different types of vehicles in a field",
          "detailed_caption": "The image shows a diagram of a building with a red arrow pointing to the left and a blue arrow pointing down to the right. The diagram is composed of different colors and shapes, with text written on it. The background is white.",
          "ocr_text": "Deep Feature Sharing Ego Vehicle Ego Vehicie CAV 1CAV 2Ego Multi-view Image Input Collaborative Prediction Single-Vehicle Ground Truth",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>Deep Feature SharingEgo VehicleEgo VehicieCAV 1CAV 2Ego Multi-viewImage InputCollaborative PredictionSingle-Vehicle Ground Truth</s>",
            "parsed": {
              "<OCR>": "Deep Feature SharingEgo VehicleEgo VehicieCAV 1CAV 2Ego Multi-viewImage InputCollaborative PredictionSingle-Vehicle Ground Truth"
            }
          },
          "paper_caption": "Figure 1. Collaborative semantic occupancy prediction leverages the power of collaboration in multi-agent systems for 3D occupancy prediction and semantic segmentation. This approach enables a deeper understanding of the 3D road environment by sharing latent features among connected automated vehicles (CAVs), surpassing the ground truth captured by a multi-camera system in the ego vehicle.",
          "paper_caption_bbox": [
            50.11199951171875,
            384.1119689941406,
            545.1110229492188,
            414.996337890625
          ]
        }
      ]
    },
    {
      "page": 2,
      "text_raw": "able for holistic scene understanding and reliable execution\nof downstream applications.\nLately, camera-based 3D semantic occupancy predic-\ntion, also known as semantic scene completion [31], has\nbecome a pioneering method in 3D perception [2, 5, 7, 13,\n14, 19, 23, 29, 30, 32, 34, 37–39, 46, 51, 52]. This approach\nuses RGB camera data to predict the semantic occupancy\nstatus of voxels in 3D space, involving both the determina-\ntion of voxel occupancy and semantic classes of occupied\nvoxels. This research enhances single CAVs’ environmental\nunderstanding, improving decision-making in downstream\napplications for automated vehicles.\nHowever, this task\nbased on RGB imagery through collaborative methods has\nnot been explored.\nTo bridge this gap, we delve into the feasibility of 3D\nsemantic occupancy prediction in the context of collabo-\nrative perception, as shown in Fig. 1, and introduce the\nCollaborative Hybrid Feature Fusion (CoHFF) Framework.\nOur approach involves separate pre-training for the dual\nsubtasks of predicting both semantics and occupancy.\nWe then extract the high-dimensional features from these\npretrained models for dual fusion processes: inter-CAV\nsemantic information fusion via V2X Feature Fusion, and\nintra-CAV fusion of semantic information with occupancy\nstatus through task feature fusion.\nThis fusion yields a\ncomprehensive decoding of each voxel’s occupancy and\nsemantic details in 3D space.\nIn order to evaluate the performance of our framework,\nwe extend the existing collaborative perception dataset\nOPV2V [41].\nBy reproducing OPV2V scenarios in the\nCARLA simulator, we collect comprehensive 3D voxel\ngroundtruth with semantic labels across 12 categories. Our\nexperiments show, that for the task of semantic occupancy\nprediction, a collaborative approach significantly outper-\nforms single-vehicle performance in most categories, as in-\ntuitively expected.\nWe also validate the effectiveness of\ntask feature fusion: our findings show that the task fu-\nsion, by incorporating features as prior knowledge of each\nother, enhances subtask performance beyond what sepa-\nrately trained models achieved. Additionally, training tasks\nindependently result in more task-specific features and thus\ncan be easier to compress. Our experiments prove that we\nachieve more complex 3D perception with a communica-\ntion volume comparable to existing methods.\nContributions To summarize, our main contributions are\nthreefold:\n• We introduce the first camera-based framework for col-\nlaborative semantic occupancy prediction, enabling more\nprecise and comprehensive 3D semantic occupancy seg-\nmentation than single-vehicle systems through feature\nsharing in V2X communication networks. The perfor-\nmance can be enhanced by over 30% via collaboration.\n• We propose the hybrid feature fusion approach, which\nnot only facilitates efficient collaboration among CAVs,\nbut also markedly enhances the performance over models\npre-trained solely for occupancy prediction or semantic\nvoxel segmentation.\n• We\nenrich\nthe\ncollaborative\nperception\ndataset\nOPV2V\n[41]\nwith\nvoxel\nground\ntruth\ncontaining\n12 categories semantic, bolstering the framework evalu-\nation. Our method, CoHFF, achieves comparable results\nto current leading methods in subsequent 3D perception\napplications,\nand additionally offers more semantic\ndetails in road environment.\n2. Related work\n2.1. Collaborative perception\nIn intelligent transportation systems, collaborative percep-\ntion empowers CAVs to attain a more accurate and holistic\nunderstanding of the road environment via V2X communi-\ncation and data fusion. Typically, data fusion in collabora-\ntive perception falls into three categories: early, middle, and\nlate fusion. Given the bandwidth limitations of V2X net-\nworks, the prevalent approach is middle fusion, where deep\nlatent space features are exchanged [11, 12, 15, 16, 25, 26,\n35, 42, 45, 50]. The advantage of middle fusion lies in its\nability to convey critical information beyond mere object-\nlevel details, bypassing the need to share raw data. The\ndevelopment of datasets specifically designed for collabo-\nrative perception [8, 11, 17, 27, 44, 48, 49, 55] has led to\nremarkable progress in learning-based approaches in recent\nyears. However, these datasets fall short in offering ground\ntruth data for 3D semantic occupancy, which motivates us\nto extend the dataset in this work, aiming to access the per-\nformance of collaborative semantic occupancy prediction.\nCollaborative Camera 3D Perception.\nCompared to\nLiDAR-driven collaborative perception [44], camera-based\nmethods are often more challenging, due to the absence of\nexplicit depth information in RGB data. However, given the\nlower price and smaller weight of cameras, they inherently\nhave a higher potential for large-scale deployment. Previ-\nous work in [40] and [12] has validated that, with collabo-\nration, camera-based 3D perception can match or even out-\nperform LiDAR performance. Given that current research\non camera-based collaborative perception either focuses on\n3D bounding box detection and BEV semantic segmenta-\ntion, there remains a research gap in semantic occupancy\nprediction. Hence, in this study, our aim is to pioneer and\nexplore the topic of collaborative occupancy segmentation.\n2.2. Camera-based semantic occupancy prediction\nOccupancy segmentation, which segments a voxel-based\n3D environment model [28, 53], has achieved notable suc-\ncess in the realm of autonomous driving. Original occu-\n17997\n",
      "text_clean": "able for holistic scene understanding and reliable execution of downstream applications. Lately, camera-based 3D semantic occupancy prediction, also known as semantic scene completion [31], has become a pioneering method in 3D perception [2, 5, 7, 13, 14, 19, 23, 29, 30, 32, 34, 37–39, 46, 51, 52]. This approach uses RGB camera data to predict the semantic occupancy status of voxels in 3D space, involving both the determination of voxel occupancy and semantic classes of occupied voxels. This research enhances single CAVs’ environmental understanding, improving decision-making in downstream applications for automated vehicles. However, this task based on RGB imagery through collaborative methods has not been explored. To bridge this gap, we delve into the feasibility of 3D semantic occupancy prediction in the context of collaborative perception, as shown in Fig. 1, and introduce the Collaborative Hybrid Feature Fusion (CoHFF) Framework. Our approach involves separate pre-training for the dual subtasks of predicting both semantics and occupancy. We then extract the high-dimensional features from these pretrained models for dual fusion processes: inter-CAV semantic information fusion via V2X Feature Fusion, and intra-CAV fusion of semantic information with occupancy status through task feature fusion. This fusion yields a comprehensive decoding of each voxel’s occupancy and semantic details in 3D space. In order to evaluate the performance of our framework, we extend the existing collaborative perception dataset OPV2V [41]. By reproducing OPV2V scenarios in the CARLA simulator, we collect comprehensive 3D voxel groundtruth with semantic labels across 12 categories. Our experiments show, that for the task of semantic occupancy prediction, a collaborative approach significantly outperforms single-vehicle performance in most categories, as intuitively expected. We also validate the effectiveness of task feature fusion: our findings show that the task fusion, by incorporating features as prior knowledge of each other, enhances subtask performance beyond what separately trained models achieved. Additionally, training tasks independently result in more task-specific features and thus can be easier to compress. Our experiments prove that we achieve more complex 3D perception with a communication volume comparable to existing methods. Contributions To summarize, our main contributions are threefold: • We introduce the first camera-based framework for collaborative semantic occupancy prediction, enabling more precise and comprehensive 3D semantic occupancy segmentation than single-vehicle systems through feature sharing in V2X communication networks. The performance can be enhanced by over 30% via collaboration. • We propose the hybrid feature fusion approach, which not only facilitates efficient collaboration among CAVs, but also markedly enhances the performance over models pre-trained solely for occupancy prediction or semantic voxel segmentation. • We enrich the collaborative perception dataset OPV2V [41] with voxel ground truth containing 12 categories semantic, bolstering the framework evaluation. Our method, CoHFF, achieves comparable results to current leading methods in subsequent 3D perception applications, and additionally offers more semantic details in road environment. 2. Related work 2.1. Collaborative perception In intelligent transportation systems, collaborative perception empowers CAVs to attain a more accurate and holistic understanding of the road environment via V2X communication and data fusion. Typically, data fusion in collaborative perception falls into three categories: early, middle, and late fusion. Given the bandwidth limitations of V2X networks, the prevalent approach is middle fusion, where deep latent space features are exchanged [11, 12, 15, 16, 25, 26, 35, 42, 45, 50]. The advantage of middle fusion lies in its ability to convey critical information beyond mere objectlevel details, bypassing the need to share raw data. The development of datasets specifically designed for collaborative perception [8, 11, 17, 27, 44, 48, 49, 55] has led to remarkable progress in learning-based approaches in recent years. However, these datasets fall short in offering ground truth data for 3D semantic occupancy, which motivates us to extend the dataset in this work, aiming to access the performance of collaborative semantic occupancy prediction. Collaborative Camera 3D Perception. Compared to LiDAR-driven collaborative perception [44], camera-based methods are often more challenging, due to the absence of explicit depth information in RGB data. However, given the lower price and smaller weight of cameras, they inherently have a higher potential for large-scale deployment. Previous work in [40] and [12] has validated that, with collaboration, camera-based 3D perception can match or even outperform LiDAR performance. Given that current research on camera-based collaborative perception either focuses on 3D bounding box detection and BEV semantic segmentation, there remains a research gap in semantic occupancy prediction. Hence, in this study, our aim is to pioneer and explore the topic of collaborative occupancy segmentation. 2.2. Camera-based semantic occupancy prediction Occupancy segmentation, which segments a voxel-based 3D environment model [28, 53], has achieved notable success in the realm of autonomous driving. Original occu- 17997",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 3,
      "text_raw": "Figure 2. The CoHFF Framework consists of four key modules: (1) Occupancy Prediction Task Net, for occupancy feature extraction;\n(2) Semantic Segmentation Task Net, creating semantic plane-based embeddings; (3) V2X Feature Fusion, merging CAV features via\ndeformable self-attention; and (4) Task Feature Fusion, uniting all task features to enhance semantic occupancy prediction.\npancy segmentation methods lean heavily on LiDAR, since\nits point cloud inherits 3D information, aligning naturally\nwith voxel-based environmental models. The recent work\nproposed in [15] explored the collaborative semantic occu-\npancy prediction based on LiDAR. However, with cameras\noffering richer environmental details, camera-driven 3D oc-\ncupancy segmentation is gradually emerging as a novel do-\nmain. Recent work in the past year, e.g. [2, 5, 7, 9, 13, 14,\n19, 20, 23, 29, 30, 32, 34, 37–39, 51, 52] have also delved\ninto methods for achieving semantic occupancy prediction\nbased on RGB data, yielding promising performance, but\nonly for single vehicle perception.\nFurthermore,\nthe\ndatasets\nfor\nthe\nvision-based\n3D Semantic Occupancy Prediction, e.g.\nSemantic-\nKITTI [1], SSC-Benchmark [18], OpenOccupancy [36],\nand Occ3D [33] have been developed specifically for\ncamera-based 3D occupancy segmentation tasks, thus\noffering resources for continued research.\nHowever,\nthose datasets do not support collaborative perception in\nmulti-agent scenarios. Generally, agents sharing different\nperspective information through collaboration can further\nenhance voxel-based occupancy segmentation.\nDue to\nsemantic occupancy prediction offering a more nuanced\n3D environmental understanding than collaborative 3D\nperception methods focused on bounding boxes or BEV\nperception, it likely requires the exchange of more com-\nplex, higher-dimensional features. Determining the most\neffective information for communication to facilitate the\ntransmission of denser, more informative data stands as a\nsignificant challenge.\n2.3. Plane-based features\nTPVFormer [13] decomposes features for occupancy seg-\nmentation into a 3D space. [6] introduced a K-Planes de-\ncomposition technique designed to reconstruct static 3D\nscenes and dynamic 4D videos. Building on the founda-\ntions laid by [6], and drawing inspiration from [13], we\nconsider to project semantically relevant information onto\northogonal planes, facilitating information sharing through\nmore streamlined communication. By sharing these plane-\nbased features, we establish the foundational structure of\nour approach.\n3. Methodology\nOur CoHFF framework consists of four key modules,\nnamely occupancy prediction Task Net, Semantic Segmen-\ntation Task Net, V2X Feature Fusion and Task Feature Fu-\nsion, as shown in Fig. 2.\nIt achieves camera-based col-\nlaborative semantic occupancy prediction by sharing plane-\nbased semantic features via V2X communication.\n3.1. Problem formulation\nGiven a network of CAVs, defined by a global communica-\ntion network represented as an undirect graph G = (N, E).\nFor each CAV i, the set of connected CAVs, is denoted by\nNi = {j|(i, j) ∈E}, where E is the existing communica-\ntion links between two CAVs, and j denotes the index of the\nCAVs connecting to i. We consider the input data in RGB\nformat, and denote Ii as the image data for a CAV i. The\nenvironment model is represented as a 3D voxel grid in one\nhot embedding V ∈RX×Y ×Z×C, where X, Y and Z are\nvoxel grid dimensions. For each CAV i, Vi represents the\npredicted occupancy of voxels, while V(0)\ni\nrepresents the\nground truth of these voxels. The objective of collabora-\ntive semantic occupancy prediction, as aligned with the op-\ntimization problem in [11, 12], is defined as follows:\n \\m\nax \n_\n{\n\\theta , M} \\sum  _i g(\\Phi _\\theta (\\mathcal {I}_i, \\{\\mathcal {M}_{i\\rightarrow j}|j\\in \\mathcal {N}_{i}\\}), \\mathbf {V}_i^{(0)}), \\\\ s.t. \\sum _i |\\{\\mathcal {M}_{i\\rightarrow j}|j\\in \\mathcal {N}_{i}\\}|\\leq B, \\label {eq:cp}\n(1)\nwhere g(·) is the perception metric for optimization. Φ rep-\nresents the model parametrized by θ, and Mi→j denotes\n17998\n",
      "text_clean": "Figure 2. The CoHFF Framework consists of four key modules: (1) Occupancy Prediction Task Net, for occupancy feature extraction; (2) Semantic Segmentation Task Net, creating semantic plane-based embeddings; (3) V2X Feature Fusion, merging CAV features via deformable self-attention; and (4) Task Feature Fusion, uniting all task features to enhance semantic occupancy prediction. pancy segmentation methods lean heavily on LiDAR, since its point cloud inherits 3D information, aligning naturally with voxel-based environmental models. The recent work proposed in [15] explored the collaborative semantic occupancy prediction based on LiDAR. However, with cameras offering richer environmental details, camera-driven 3D occupancy segmentation is gradually emerging as a novel domain. Recent work in the past year, e.g. [2, 5, 7, 9, 13, 14, 19, 20, 23, 29, 30, 32, 34, 37–39, 51, 52] have also delved into methods for achieving semantic occupancy prediction based on RGB data, yielding promising performance, but only for single vehicle perception. Furthermore, the datasets for the vision-based 3D Semantic Occupancy Prediction, e.g. SemanticKITTI [1], SSC-Benchmark [18], OpenOccupancy [36], and Occ3D [33] have been developed specifically for camera-based 3D occupancy segmentation tasks, thus offering resources for continued research. However, those datasets do not support collaborative perception in multi-agent scenarios. Generally, agents sharing different perspective information through collaboration can further enhance voxel-based occupancy segmentation. Due to semantic occupancy prediction offering a more nuanced 3D environmental understanding than collaborative 3D perception methods focused on bounding boxes or BEV perception, it likely requires the exchange of more complex, higher-dimensional features. Determining the most effective information for communication to facilitate the transmission of denser, more informative data stands as a significant challenge. 2.3. Plane-based features TPVFormer [13] decomposes features for occupancy segmentation into a 3D space. [6] introduced a K-Planes decomposition technique designed to reconstruct static 3D scenes and dynamic 4D videos. Building on the foundations laid by [6], and drawing inspiration from [13], we consider to project semantically relevant information onto orthogonal planes, facilitating information sharing through more streamlined communication. By sharing these planebased features, we establish the foundational structure of our approach. 3. Methodology Our CoHFF framework consists of four key modules, namely occupancy prediction Task Net, Semantic Segmentation Task Net, V2X Feature Fusion and Task Feature Fusion, as shown in Fig. 2. It achieves camera-based collaborative semantic occupancy prediction by sharing planebased semantic features via V2X communication. 3.1. Problem formulation Given a network of CAVs, defined by a global communication network represented as an undirect graph G = (N, E). For each CAV i, the set of connected CAVs, is denoted by Ni = {j|(i, j) ∈E}, where E is the existing communication links between two CAVs, and j denotes the index of the CAVs connecting to i. We consider the input data in RGB format, and denote Ii as the image data for a CAV i. The environment model is represented as a 3D voxel grid in one hot embedding V ∈RX×Y ×Z×C, where X, Y and Z are voxel grid dimensions. For each CAV i, Vi represents the predicted occupancy of voxels, while V(0) i represents the ground truth of these voxels. The objective of collaborative semantic occupancy prediction, as aligned with the optimization problem in [11, 12], is defined as follows: \\m ax _ { \\theta , M} \\sum _i g(\\Phi _\\theta (\\mathcal {I}_i, \\{\\mathcal {M}_{i\\rightarrow j}|j\\in \\mathcal {N}_{i}\\}), \\mathbf {V}_i^{(0)}), \\\\ s.t. \\sum _i |\\{\\mathcal {M}_{i\\rightarrow j}|j\\in \\mathcal {N}_{i}\\}|\\leq B, \\label {eq:cp} (1) where g(·) is the perception metric for optimization. Φ represents the model parametrized by θ, and Mi→j denotes 17998",
      "figure_captions": [
        {
          "bbox": [
            50.11199951171875,
            213.61099243164062,
            545.1111450195312,
            244.494384765625
          ],
          "text": "Figure 2. The CoHFF Framework consists of four key modules: (1) Occupancy Prediction Task Net, for occupancy feature extraction; (2) Semantic Segmentation Task Net, creating semantic plane-based embeddings; (3) V2X Feature Fusion, merging CAV features via deformable self-attention; and (4) Task Feature Fusion, uniting all task features to enhance semantic occupancy prediction."
        }
      ],
      "images": [
        {
          "image_id": "p3_cluster_001",
          "page": 3,
          "file_path": "images/page_0003_cluster_001.png",
          "bbox": [
            50.11199951171875,
            71.99681091308594,
            545.1033935546875,
            202.7769775390625
          ],
          "caption": "a block diagram of a computer system",
          "detailed_caption": "The image shows a block diagram of a computer system with a white background. It is a visual representation of the flow of data from one device to another, illustrating the various components of the system. The diagram is composed of several boxes connected by arrows, each box representing a step in the process. The arrows indicate the direction of the data flow, from the top left to the bottom right.",
          "ocr_text": "1Occupancy Prediction Task Net Depth Voxel Voxell Net Embedder Compler Hybrid Feature Fusion Hybrid Features Ego3D3DRGB Image Vox EL Sep.Task Feature Conv Backbone Attentive Fusion3D Feature Semantic Segmentation Task Net2VXX Feature Fusion Fusion Feature Feature Feature RGB Image Backbone Attachments CAVEncoder Task Feature Xxxx Axx Relative Fusion Leamable Feature DYA9 Pose Depth Vovel Vozel Mask Ao Awareness Net Embedded Compler",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>1Occupancy Prediction Task NetDepthVoxelVoxellNetEmbedderComplerHybridFeature FusionHybridFeaturesEgo3D3DRGB ImageVoxEL Sep.Task FeatureConvBackboneAttentiveFusion3D FeatureSemantic Segmentation Task Net2VXX FeatureFusionFusionFeatureFeatureFeatureRGB ImageBackboneAttachmentsCAVEncoderTask FeatureXxxxAxxRelativeFusionLeamableFeatureDYA9 PoseDepthVovelVozelMaskAo AwarenessNetEmbeddedCompler</s>",
            "parsed": {
              "<OCR>": "1Occupancy Prediction Task NetDepthVoxelVoxellNetEmbedderComplerHybridFeature FusionHybridFeaturesEgo3D3DRGB ImageVoxEL Sep.Task FeatureConvBackboneAttentiveFusion3D FeatureSemantic Segmentation Task Net2VXX FeatureFusionFusionFeatureFeatureFeatureRGB ImageBackboneAttachmentsCAVEncoderTask FeatureXxxxAxxRelativeFusionLeamableFeatureDYA9 PoseDepthVovelVozelMaskAo AwarenessNetEmbeddedCompler"
            }
          },
          "paper_caption": "Figure 2. The CoHFF Framework consists of four key modules: (1) Occupancy Prediction Task Net, for occupancy feature extraction; (2) Semantic Segmentation Task Net, creating semantic plane-based embeddings; (3) V2X Feature Fusion, merging CAV features via deformable self-attention; and (4) Task Feature Fusion, uniting all task features to enhance semantic occupancy prediction.",
          "paper_caption_bbox": [
            50.11199951171875,
            213.61099243164062,
            545.1111450195312,
            244.494384765625
          ]
        }
      ]
    },
    {
      "page": 4,
      "text_raw": "the message transmitted from CAV i to CAV j. The size of\nthese messages is constrained by a communication budget\nupper bound B ∈R+.\nConsidering the communication upper bound, instead\nof directly sending high-dimensional voxel-sized features\nFV , we opt to transmit features FP from orthogonal\nplanes. This approach reduces the messages from MFV ∈\nRX×Y ×Z×F\nto MPxz\n∈\nRX×Z×F\nand MPyz\n∈\nRY ×Z×F , where Pxz and Pyz denote the features pro-\njected on the xz- and yz-planes respectively. F represents\nthe length of a single feature vector.\nFor instance, in a\nvoxel space of 100 × 100 × 8 with a feature dimension of\n128, transmitting orthogonal plane features can reduce com-\nmunication volume by 50 ×, from 39.05 MB to 0.78 MB,\nwhich is comparable to existing collaborative perception\nmethods, yet it offers more extensive and detailed seman-\ntic 3D scene information. Based on these considerations,\nwe introduce our framework in the following section.\n3.2. Framework design\nWe divide our method into two distinct pre-communication\ntasks: 3D occupancy prediction and semantic voxel seg-\nmentation. We believe that occupancy features enhance the\nsemantic segmentation performance by providing geometry\ninsight of distinct object classes. Meanwhile, semantic in-\nformation can suggest changes of a voxel occupancy. Based\non this interplay, our approach initially focuses on indepen-\ndent pre-training for each task. Then we fuse the features\nfrom both tasks to learn a combined semantic occupancy\npredictor that yields better performance for each individual\ntask. This assumption is experimentally validated by the ab-\nlation study in Tab. 3. Consequently, our framework com-\nprises two specialized pre-trained networks: an occupancy\nprediction task network and a semantic segmentation task\nnetwork, as shown in Fig. 2.\nOccupancy prediction task network. The occupancy pre-\ndiction necessitates the conversion of 2D image data into\na 3D occupancy grid. We first use an off-the-shelf depth\nprediction network Φdepth(·) to determine the depth of\neach pixel.\nFollowing the work in [11, 12], we employ\nCaDNN [3] for depth estimation. This depth data is then\nembedded into voxel space through a 3D Emedder, re-\nsulting in a preliminary voxel representation. This voxel-\nbased road environment is further completed by a 3D occu-\npancy encoder Φocc(·). Finally, the occupancy task features\nFocc ∈RX×Y ×Z×F is extracted for task fusion.\nSemantic segmentation task network.\nIn the segmen-\ntation network, we process RGB data to generate fea-\nture maps Fseg using Φimg(·), which are then subjected\nto deformable cross-attention [54] to facilitate mapping\nonto a 3D semantic segmentation space. Drawing inspi-\nration from K-Planes [6] and TPVformer [13], we project\nthese features onto three spatially orthogonal planes P =\n{Pxy, Pxz, Pyz}. Among these dense and informative 3D\nfeature representations, two are transmitted via V2X mes-\nsages, i.e. M = {MPxz, MPyz}. The reason behind not\nsending the Pxy plane, is that the we use the Pxy of the ego\nvehicle for reconstructing the 3D features, which facilitates\nthe alignment of the feature space with the detection range\nof interest of ego vehicle.\nBoth networks generate high-dimensional features that\nare fed into a hybrid feature fusion network, thereby form-\ning the core of CoHFF for semantic occupancy prediction.\n3.3. Hybrid feature fusion\nV2X Feature Fusion. Given one CAV j communicating\nto the ego vehicle i, the features of the CAV condensed by\nthe segmentation network can contain overlapping informa-\ntion, particularly regarding semantics in proximity to the\nego vehicle, which the ego vehicle itself can accurately pre-\ndict. We implement a masking technique to selectively fil-\nter these plane-based features of the CAV, before they are\ncommunicated to the ego vehicle. By adjusting a sparsi-\nfication rate hyperparameter, we reduce the volume of the\nCAV´s plane-based features shared during collaboration, in\nline with the communication budget. The compressed mes-\nsage ¯\nM = { ¯\nMPxz, ¯\nMPyz} can be acquired as follows:\n  \\b\na r  {\\m\na\nt hbf\n \n{ P}}\n_ j ^{x\nz } ,\\b\na r {\\mathbf {P}}_j^{yz} \\leftarrow \\mathbf {P}_j^{xz} \\odot \\mathbf {H}_j^{xz} ,\\mathbf {P}_j^{yz} \\odot \\mathbf {H}_j^{yz}, \n(2)\nwhere Hxz\nj\nand Hyz\nj\nrepresent the learnable feature masks\nfor features on x-z and y-z planes.\nAdditionally, we ensure relative pose awareness between\nthe ego vehicle and other CAVs. Specifically, we feed the\nfiltered plane features and the relative pose information into\nan MLP network combined with a Sigmoid function, in line\nwith the methodology proposed in [24].\nWe now attend these pose-aware filtered plane features\nfrom the CAV (¯Pxz\nj , ¯Pyz\nj ) over the three plane features of\nthe ego vehicle (Pxy\ni , Pxz\ni , Pyz\ni ). In particular, we use de-\nformable self-attention to update the all five feature planes.\nThe fusion and updating of these planes are accomplished\nby plane self-attention (PSA), as follows:\n  PSA( \\ mathb f {p})  = D\nA ( \\mat\nh bf  {p},\\mathcal {R},\\{\\mathbf {P}_i, \\bar {\\mathbf {P}}_j^{xz}, \\bar {\\mathbf {P}}_j^{yz}|j\\in \\mathcal {N}_i\\} ), \\label {eq:psa_fusion} \n(3)\nwhere DA(·) is deformable self-attention, p ∈RF is a\nquery and R is a set of reference points, as described in [54].\nPi denotes all the three planes in ego vehicle.\nThe updated 2D plane features are used in the next step\nto reconstruct 3D semantic segmentation features Fseg. The\nsemantic segmentation feature f seg\nx,y,z at a specific Voxel lo-\ncation x, y, z can be reconstructed as follows:\n  \\m\nathbf  {f}\n_{x , y,z}\n^{s e g} =\n \\m a th b f { p}^{xy}_{i,z} + \\bar {\\mathbf {p}}^{xz}_{j,y} + \\bar {\\mathbf {p}}^{yz}_{j,x} \\in \\mathbb {R}^{F}, \\forall j \\in \\mathcal {N}_i, \\label {eq:expand} \n(4)\nwhere ¯pxz\nj,y and ¯pyz\nj,x is plane features from CAV j, and pxy\ni,z\nis the plane (BEV) features from ego vehicle. This idea of\n17999\n",
      "text_clean": "the message transmitted from CAV i to CAV j. The size of these messages is constrained by a communication budget upper bound B ∈R+. Considering the communication upper bound, instead of directly sending high-dimensional voxel-sized features FV , we opt to transmit features FP from orthogonal planes. This approach reduces the messages from MFV ∈ RX×Y ×Z×F to MPxz ∈ RX×Z×F and MPyz ∈ RY ×Z×F , where Pxz and Pyz denote the features projected on the xz- and yz-planes respectively. F represents the length of a single feature vector. For instance, in a voxel space of 100 × 100 × 8 with a feature dimension of 128, transmitting orthogonal plane features can reduce communication volume by 50 ×, from 39.05 MB to 0.78 MB, which is comparable to existing collaborative perception methods, yet it offers more extensive and detailed semantic 3D scene information. Based on these considerations, we introduce our framework in the following section. 3.2. Framework design We divide our method into two distinct pre-communication tasks: 3D occupancy prediction and semantic voxel segmentation. We believe that occupancy features enhance the semantic segmentation performance by providing geometry insight of distinct object classes. Meanwhile, semantic information can suggest changes of a voxel occupancy. Based on this interplay, our approach initially focuses on independent pre-training for each task. Then we fuse the features from both tasks to learn a combined semantic occupancy predictor that yields better performance for each individual task. This assumption is experimentally validated by the ablation study in Tab. 3. Consequently, our framework comprises two specialized pre-trained networks: an occupancy prediction task network and a semantic segmentation task network, as shown in Fig. 2. Occupancy prediction task network. The occupancy prediction necessitates the conversion of 2D image data into a 3D occupancy grid. We first use an off-the-shelf depth prediction network Φdepth(·) to determine the depth of each pixel. Following the work in [11, 12], we employ CaDNN [3] for depth estimation. This depth data is then embedded into voxel space through a 3D Emedder, resulting in a preliminary voxel representation. This voxelbased road environment is further completed by a 3D occupancy encoder Φocc(·). Finally, the occupancy task features Focc ∈RX×Y ×Z×F is extracted for task fusion. Semantic segmentation task network. In the segmentation network, we process RGB data to generate feature maps Fseg using Φimg(·), which are then subjected to deformable cross-attention [54] to facilitate mapping onto a 3D semantic segmentation space. Drawing inspiration from K-Planes [6] and TPVformer [13], we project these features onto three spatially orthogonal planes P = {Pxy, Pxz, Pyz}. Among these dense and informative 3D feature representations, two are transmitted via V2X messages, i.e. M = {MPxz, MPyz}. The reason behind not sending the Pxy plane, is that the we use the Pxy of the ego vehicle for reconstructing the 3D features, which facilitates the alignment of the feature space with the detection range of interest of ego vehicle. Both networks generate high-dimensional features that are fed into a hybrid feature fusion network, thereby forming the core of CoHFF for semantic occupancy prediction. 3.3. Hybrid feature fusion V2X Feature Fusion. Given one CAV j communicating to the ego vehicle i, the features of the CAV condensed by the segmentation network can contain overlapping information, particularly regarding semantics in proximity to the ego vehicle, which the ego vehicle itself can accurately predict. We implement a masking technique to selectively filter these plane-based features of the CAV, before they are communicated to the ego vehicle. By adjusting a sparsification rate hyperparameter, we reduce the volume of the CAV´s plane-based features shared during collaboration, in line with the communication budget. The compressed message ¯ M = { ¯ MPxz, ¯ MPyz} can be acquired as follows: \\b a r {\\m a t hbf { P}} _ j ^{x z } ,\\b a r {\\mathbf {P}}_j^{yz} \\leftarrow \\mathbf {P}_j^{xz} \\odot \\mathbf {H}_j^{xz} ,\\mathbf {P}_j^{yz} \\odot \\mathbf {H}_j^{yz}, (2) where Hxz j and Hyz j represent the learnable feature masks for features on x-z and y-z planes. Additionally, we ensure relative pose awareness between the ego vehicle and other CAVs. Specifically, we feed the filtered plane features and the relative pose information into an MLP network combined with a Sigmoid function, in line with the methodology proposed in [24]. We now attend these pose-aware filtered plane features from the CAV (¯Pxz j , ¯Pyz j ) over the three plane features of the ego vehicle (Pxy i , Pxz i , Pyz i ). In particular, we use deformable self-attention to update the all five feature planes. The fusion and updating of these planes are accomplished by plane self-attention (PSA), as follows: PSA( \\ mathb f {p}) = D A ( \\mat h bf {p},\\mathcal {R},\\{\\mathbf {P}_i, \\bar {\\mathbf {P}}_j^{xz}, \\bar {\\mathbf {P}}_j^{yz}|j\\in \\mathcal {N}_i\\} ), \\label {eq:psa_fusion} (3) where DA(·) is deformable self-attention, p ∈RF is a query and R is a set of reference points, as described in [54]. Pi denotes all the three planes in ego vehicle. The updated 2D plane features are used in the next step to reconstruct 3D semantic segmentation features Fseg. The semantic segmentation feature f seg x,y,z at a specific Voxel location x, y, z can be reconstructed as follows: \\m athbf {f} _{x , y,z} ^{s e g} = \\m a th b f { p}^{xy}_{i,z} + \\bar {\\mathbf {p}}^{xz}_{j,y} + \\bar {\\mathbf {p}}^{yz}_{j,x} \\in \\mathbb {R}^{F}, \\forall j \\in \\mathcal {N}_i, \\label {eq:expand} (4) where ¯pxz j,y and ¯pyz j,x is plane features from CAV j, and pxy i,z is the plane (BEV) features from ego vehicle. This idea of 17999",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 5,
      "text_raw": "Algorithm 1 : CoHFF framework for collaborative\nsemantic occupancy prediction.\n1: for each CAV i in parallel do\n2:\nFocc\ni\n←Φocc(Proj(Φdepth(Ii), Ii)))\n3:\nFimg\ni\n←Φimg(Ii)\n4:\nupdate plane-based features Pxz\ni , Pyz\ni , Pxy\ni\nusing\ndeformable cross- and self-attention [54]\n5:\n¯Pxz\ni , ¯Pyz\ni\n←Pxz\ni\n⊙Hxz\ni , Pyz\ni\n⊙Hyz\ni\n6:\n¯\nMi ←{¯Pxz\ni , ¯Pyz\ni }\n7:\nCAV i broadcasts messages ¯\nMi\n8:\nfor j ∈Ni do\n9:\nCAV i receives messages\n¯\nMj\n10:\nend for\n11:\nupdate {Pi, ¯Pxz\nj , ¯Pyz\nj |j ∈Ni} using self-attention\nbased on (3)\n12:\nreconstruct F seg\nj\nbased on (4)\n\\triangleright VFF\n13:\nVi ←Φtff(Focc\ni\n, Fseg\ni\n, {Fseg\nj\n|j ∈Ni})\n\\triangleright TFF\n14: end for\nsum of projected features for 3D reconstruction is originally\nproposed in [13], with our work adapting it to multi-agent\nscenarios.\nTask Feature Fusion. After retrieving global semantic in-\nformation as Fseg, the final step aims at fusion with features\nFocc from the occupancy prediction task. To accomplish\nthis, Fseg and Focc are concatenated and passed to a 3D\ndepth-wise convolution network [47], in order to produce\nthe final semantic voxel map. This task feature fusion net-\nwork Φtff(·) is implemented as follows:\n  \\ mathbf {V\n_\ni } = \n\\\nP hi ^{\nt\nff } (\\ma t hbf {F}_i^{occ}, \\mathbf {F}_i^{seg}, \\{\\mathbf {F}_j^{seg}|j\\in \\mathcal {N}_i\\})\\in \\mathbb {R}^{X \\times Y \\times Z \\times C}. \n(5)\nThe CoHFF pseudocode is given in Algorithm 1.\n3.4. Losses\nWe train the completion network training using focal loss\nproposed in [21], applying it to a dataset with binary la-\nbels {0, 1}. For both the segmentation network and the hy-\nbrid feature fusion network, we employ a weighted cross-\nentropy loss to train for semantic labels. Notably, in this\ncontext, the label for the empty is also designated as 0.\n4. Dataset\nTo effectively evaluate collaborative semantic occupancy\nprediction, a dataset that supports collaborative percep-\ntion and includes 3D semantic occupancy labels is crucial.\nThus, we enhance the OPV2V dataset [41] by integrating\n12 different 3D semantic occupancy labels, as shown in\nTab. 4 This enhancement is achieved using the high-fidelity\nCARLA simulator [4] and the OpenCDA autonomous driv-\ning simulation framework [43]. We position four semantic\nLiDARs at the original camera sites to precisely capture the\nTable 1. Comparison 3D object detection with AP2 of vehicles.\nApproach\n# Agents\nAP@0.5 AP@0.7\nDiscoNet (NeurIPS 21)\nUp to 7\n36.00\n12.50\nV2X-ViT (ECCV 22)\nUp to 7\n39.82\n16.43\nWhere2Comm (NeurIPS 22)\nUp to 7\n47.30\n19.30\nCoCa3D (CVPR 23)\n71\n69.10\n49.50\nCoHFF\nUp to 7\n48.51\n36.39\nCoCa3D-2 (CVPR 23)\n2\n25.90\n12.60\nCoHFF\n2\n36.63\n27.95\n1 CoCa3D is trained on OPV2V+, where extended agents provide more input\ninformation for better results.\n2 We calculate the 3D IoU by comparing the predicted voxels with the ground\ntruth voxels for each object, rather than using 3D bounding boxes due to the\npotential unnecessary occupancy in 3D bounding boxes.\nTable 2. Comparison of BEV semantic segmentation with IoU in the\nclass of Vehicle, Road and Others.\nApproach\n# Agents\nVehicle\nRoad\nOthers1\nCoBEVT (CoRL 22)\n2\n46.13\n52.41\n-\nCoHFF\n2\n47.40\n63.36\n40.27\nCoBEVT (CoRL 22)\nUp to 7\n60.40\n63.00\n-\nCoHFF\nUp to 7\n64.44\n57.28\n45.89\n1 It refers to additional object classes identified through semantic segmentation pre-\ndictions projected onto the BEV plane. These categories include buildings, fences,\nterrain, poles, vegetation, walls, guard rails, traffic signs, and bridges. The IoU for\nthese objects is calculated and reported as IoU.\nsemantic occupancy ground truth within the cameras’ FoV.\nIn addition, we associate ground truth data from all CAVs\nto create a detailed collaborative ground truth for collabora-\ntive supervision. Furthermore, to comprehensively capture\noccluded semantic occupancies for all CAVs, we include a\nsimulation replay in our data collection process, where each\nCAV is equipped with 18 semantic LiDARs. This strategic\nconfiguration is crucial for effectively evaluating comple-\ntion tasks, as it guarantees extensive data collection, en-\ncompassing areas not visible in direct associated FoV. In\nalignment with the original OPV2V protocol, we replay the\nsimulation and generate a multi-tier ground truth.\n5. Experimental evaluation\n5.1. Experiment setup\nBaselines. Considering the unexplored domain of collabo-\nrative occupancy segmentation, we extend the findings from\nCoHFF to address downstream applications, including BEV\nperception and 3D detection.\nIn our analysis, we eval-\nuate these outcomes with those from state-of-the-art col-\nlaborative perception models that employ multi-view cam-\neras: CoBEVT [40] for BEV perception and CoCa3D [12]\nfor 3D detection. Furthermore, we examine contemporary\n18000\n",
      "text_clean": "Algorithm 1 : CoHFF framework for collaborative semantic occupancy prediction. 1: for each CAV i in parallel do 2: Focc i ←Φocc(Proj(Φdepth(Ii), Ii))) 3: Fimg i ←Φimg(Ii) 4: update plane-based features Pxz i , Pyz i , Pxy i using deformable cross- and self-attention [54] 5: ¯Pxz i , ¯Pyz i ←Pxz i ⊙Hxz i , Pyz i ⊙Hyz i 6: ¯ Mi ←{¯Pxz i , ¯Pyz i } 7: CAV i broadcasts messages ¯ Mi 8: for j ∈Ni do 9: CAV i receives messages ¯ Mj 10: end for 11: update {Pi, ¯Pxz j , ¯Pyz j |j ∈Ni} using self-attention based on (3) 12: reconstruct F seg j based on (4) \\triangleright VFF 13: Vi ←Φtff(Focc i , Fseg i , {Fseg j |j ∈Ni}) \\triangleright TFF 14: end for sum of projected features for 3D reconstruction is originally proposed in [13], with our work adapting it to multi-agent scenarios. Task Feature Fusion. After retrieving global semantic information as Fseg, the final step aims at fusion with features Focc from the occupancy prediction task. To accomplish this, Fseg and Focc are concatenated and passed to a 3D depth-wise convolution network [47], in order to produce the final semantic voxel map. This task feature fusion network Φtff(·) is implemented as follows: \\ mathbf {V _ i } = \\ P hi ^{ t ff } (\\ma t hbf {F}_i^{occ}, \\mathbf {F}_i^{seg}, \\{\\mathbf {F}_j^{seg}|j\\in \\mathcal {N}_i\\})\\in \\mathbb {R}^{X \\times Y \\times Z \\times C}. (5) The CoHFF pseudocode is given in Algorithm 1. 3.4. Losses We train the completion network training using focal loss proposed in [21], applying it to a dataset with binary labels {0, 1}. For both the segmentation network and the hybrid feature fusion network, we employ a weighted crossentropy loss to train for semantic labels. Notably, in this context, the label for the empty is also designated as 0. 4. Dataset To effectively evaluate collaborative semantic occupancy prediction, a dataset that supports collaborative perception and includes 3D semantic occupancy labels is crucial. Thus, we enhance the OPV2V dataset [41] by integrating 12 different 3D semantic occupancy labels, as shown in Tab. 4 This enhancement is achieved using the high-fidelity CARLA simulator [4] and the OpenCDA autonomous driving simulation framework [43]. We position four semantic LiDARs at the original camera sites to precisely capture the Table 1. Comparison 3D object detection with AP2 of vehicles. Approach # Agents AP@0.5 AP@0.7 DiscoNet (NeurIPS 21) Up to 7 36.00 12.50 V2X-ViT (ECCV 22) Up to 7 39.82 16.43 Where2Comm (NeurIPS 22) Up to 7 47.30 19.30 CoCa3D (CVPR 23) 71 69.10 49.50 CoHFF Up to 7 48.51 36.39 CoCa3D-2 (CVPR 23) 2 25.90 12.60 CoHFF 2 36.63 27.95 1 CoCa3D is trained on OPV2V+, where extended agents provide more input information for better results. 2 We calculate the 3D IoU by comparing the predicted voxels with the ground truth voxels for each object, rather than using 3D bounding boxes due to the potential unnecessary occupancy in 3D bounding boxes. Table 2. Comparison of BEV semantic segmentation with IoU in the class of Vehicle, Road and Others. Approach # Agents Vehicle Road Others1 CoBEVT (CoRL 22) 2 46.13 52.41 - CoHFF 2 47.40 63.36 40.27 CoBEVT (CoRL 22) Up to 7 60.40 63.00 - CoHFF Up to 7 64.44 57.28 45.89 1 It refers to additional object classes identified through semantic segmentation predictions projected onto the BEV plane. These categories include buildings, fences, terrain, poles, vegetation, walls, guard rails, traffic signs, and bridges. The IoU for these objects is calculated and reported as IoU. semantic occupancy ground truth within the cameras’ FoV. In addition, we associate ground truth data from all CAVs to create a detailed collaborative ground truth for collaborative supervision. Furthermore, to comprehensively capture occluded semantic occupancies for all CAVs, we include a simulation replay in our data collection process, where each CAV is equipped with 18 semantic LiDARs. This strategic configuration is crucial for effectively evaluating completion tasks, as it guarantees extensive data collection, encompassing areas not visible in direct associated FoV. In alignment with the original OPV2V protocol, we replay the simulation and generate a multi-tier ground truth. 5. Experimental evaluation 5.1. Experiment setup Baselines. Considering the unexplored domain of collaborative occupancy segmentation, we extend the findings from CoHFF to address downstream applications, including BEV perception and 3D detection. In our analysis, we evaluate these outcomes with those from state-of-the-art collaborative perception models that employ multi-view cameras: CoBEVT [40] for BEV perception and CoCa3D [12] for 3D detection. Furthermore, we examine contemporary 18000",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 6,
      "text_raw": "Table 3.\nCoHFF achieves robust IoU and mIoU performance,\nwhen the communication volume (CV) is reduced by setting vari-\nous sparsification rates (Spar. Rate).\nSpar. Rate\n0.00\n0.50\n0.80\n0.95\n0.99\nCV (MB) (↓)\n16.53\n8.27\n3.31\n0.83\n0.17\nIoU (↑)\n50.46\n49.56\n49.53\n48.52\n48.02\nmIoU (↑)\n34.16\n32.97\n32.70\n30.13\n29.48\nmethods that integrate alternative modalities, particularly\nthose blending LiDAR with camera inputs or relying solely\non LiDAR, including DiscoNet [16], V2X-ViT [42] and\nWhere2Comm [11].\nImplementation details. Following the previous work for\ncollaborative perception evaluation on the OPV2V dataset\nused in [11], we utilize a 40×40×3.2 meter detection area\nwith a grid size of 100 x 100 x 8, resulting in a voxel size of\n0.4 m3. We allow CAVs to transmit and share features with\na length of 128 for V2X Feature Fusion. Our experiment\nincorporates the analysis of 12 semantic labels plus an ad-\nditional empty label. We employ CaDNN [3] with 50 depth\ncategories and a single out-of-range category for depth es-\ntimation, as well as ResNet101 [10] and FPN [22] as RGB\nthe image backbone. For Voxel completion, we utilize a 3D\ndepth-wise CNN [47] and use deformable attention [54] in\nhybrid feature fusion.\nEvaluation metrics. Following the evaluation of semantic\noccupancy prediction in previous work, such as [2, 13, 19],\nwe primarily utilize the metric Intersection over Union\n(IoU) for evaluation. This involves calculating IoU for each\nindividual class and the mean IoU (mIoU) across all classes.\nAdditionally, for evaluations in subsequent applications, we\ncompute the Average Precision (AP) at IoU threshold of 0.5\nand 0.7, and BEV 2D IoU to compare with other baselines.\nSpecifically, the AP value is calculated only for voxels la-\nbeled as vehicles, and the IoU is determined for each pair\nof predicted and actual vehicles. For BEV IoU, voxels are\nprojected onto the BEV plane and categorized into the cor-\nresponding semantic classes.\n5.2. Comparison\nCollaborative 3D object detection.\nFirst, we compare\nthe performance of CoHFF in 3D detection applications.\nAs shown in Tab. 1, with up to 7 agents’ collaborative\nperception, CoHFF achieves comparable performance to\nWhere2comm at AP@0.5 and obtains an 88.5% improve-\nment at AP@0.7. We believe this is primarily due to se-\nmantic occupancy prediction, which makes the perception\nresults closer to the actual observed shapes, rather than in-\nferring a non-existent bounding box in the scenarios. We\nalso observe that CoCa3D, on the OPV2V+ dataset [12],\nachieves significantly better performance due to receiving\nmore information from CAVs. To compare directly with\nCoCa3D, we also conduct scenarios where only two agents\ncommunicated at a time. We can see that CoHFF has made\nsignificant improvements at both AP@0.5 and AP@0.7.\nCollaborative BEV segementation. Tab. 2 presents a com-\nparison between CoHFF and CoBEVT in BEV semantic\nsegmentation. Note that errors in height prediction from 3D\nvoxel occupancy mapping to the BEV plane may be over-\nlooked during the projection process. Despite this, CoHFF\nachieves even better performance in predicting vehicles and\nroads in BEV compared to CoBEVT. Additionally, CoHFF\nis capable of detecting a wider range of other semantic cat-\negories in 3D occupancy.\n5.3. Ablation study\nTo validate our hypothesis that independently obtained se-\nmantic and occupancy feature information can simultane-\nously strengthen the original semantic and occupancy tasks,\nwe have decomposed the semantic occupancy prediction\ninto two separate tasks. Tab. 4 shows an ablation study by\naltering the components used. Meanwhile, we also verify\nthe enhancement of collaborative perception over single ve-\nhicle perception in terms of semantic occupancy.\nCoHFF for occupancy prediction. When focusing solely\non binary occupancy predictions (as shown at Occ. Pred. in\nTab. 4), we use voxels processed from raw LiDAR point\nclouds as a reference, and analyze the IoU in different\nsemantic classes based on semantic occupancy in ground\ntruth. It is observed that by utilizing an occupancy pre-\ndiction task network to process depth predictions, the over-\nall prediction accuracy is enhanced. Additionally, signifi-\ncant improvements in predicting large objects in occupancy\nresults are noted by integrating features from a semantic\nsegmentation task network, leading to an increased over-\nall IoU. However, a concurrent decline in the mIoU is ob-\nserved alongside the increase in IoU. This phenomenon is\nattributed to the influence of semantic features, which seem\nto steer the model towards prioritizing easily detectable cat-\negories, potentially at the expense of smaller or less distinct\ncategories. Finally, through collaboration, the overall IoU\nand mIoU are further strengthened on the basis of task fea-\nture fusion.\nCoHFF for semantic segmentation. In our semantic seg-\nmentation task (as shown at Sem. Seg. in Tab. 4), after inte-\ngrating features from occupancy prediction, we observe an\napproximate 2% increase in IoU, but a more substantial over\n41% enhancement in mIoU. We attribute this improvement\nto the features derived from occupancy prediction, which\nseem to aid the easier detection of smaller-scale objects,\nthereby refining their semantic predictions. Consistent with\nthe occupancy prediction task, the final collaboration fur-\nther elevates the results of semantic segmentation.\n18001\n",
      "text_clean": "Table 3. CoHFF achieves robust IoU and mIoU performance, when the communication volume (CV) is reduced by setting various sparsification rates (Spar. Rate). Spar. Rate 0.00 0.50 0.80 0.95 0.99 CV (MB) (↓) 16.53 8.27 3.31 0.83 0.17 IoU (↑) 50.46 49.56 49.53 48.52 48.02 mIoU (↑) 34.16 32.97 32.70 30.13 29.48 methods that integrate alternative modalities, particularly those blending LiDAR with camera inputs or relying solely on LiDAR, including DiscoNet [16], V2X-ViT [42] and Where2Comm [11]. Implementation details. Following the previous work for collaborative perception evaluation on the OPV2V dataset used in [11], we utilize a 40×40×3.2 meter detection area with a grid size of 100 x 100 x 8, resulting in a voxel size of 0.4 m3. We allow CAVs to transmit and share features with a length of 128 for V2X Feature Fusion. Our experiment incorporates the analysis of 12 semantic labels plus an additional empty label. We employ CaDNN [3] with 50 depth categories and a single out-of-range category for depth estimation, as well as ResNet101 [10] and FPN [22] as RGB the image backbone. For Voxel completion, we utilize a 3D depth-wise CNN [47] and use deformable attention [54] in hybrid feature fusion. Evaluation metrics. Following the evaluation of semantic occupancy prediction in previous work, such as [2, 13, 19], we primarily utilize the metric Intersection over Union (IoU) for evaluation. This involves calculating IoU for each individual class and the mean IoU (mIoU) across all classes. Additionally, for evaluations in subsequent applications, we compute the Average Precision (AP) at IoU threshold of 0.5 and 0.7, and BEV 2D IoU to compare with other baselines. Specifically, the AP value is calculated only for voxels labeled as vehicles, and the IoU is determined for each pair of predicted and actual vehicles. For BEV IoU, voxels are projected onto the BEV plane and categorized into the corresponding semantic classes. 5.2. Comparison Collaborative 3D object detection. First, we compare the performance of CoHFF in 3D detection applications. As shown in Tab. 1, with up to 7 agents’ collaborative perception, CoHFF achieves comparable performance to Where2comm at AP@0.5 and obtains an 88.5% improvement at AP@0.7. We believe this is primarily due to semantic occupancy prediction, which makes the perception results closer to the actual observed shapes, rather than inferring a non-existent bounding box in the scenarios. We also observe that CoCa3D, on the OPV2V+ dataset [12], achieves significantly better performance due to receiving more information from CAVs. To compare directly with CoCa3D, we also conduct scenarios where only two agents communicated at a time. We can see that CoHFF has made significant improvements at both AP@0.5 and AP@0.7. Collaborative BEV segementation. Tab. 2 presents a comparison between CoHFF and CoBEVT in BEV semantic segmentation. Note that errors in height prediction from 3D voxel occupancy mapping to the BEV plane may be overlooked during the projection process. Despite this, CoHFF achieves even better performance in predicting vehicles and roads in BEV compared to CoBEVT. Additionally, CoHFF is capable of detecting a wider range of other semantic categories in 3D occupancy. 5.3. Ablation study To validate our hypothesis that independently obtained semantic and occupancy feature information can simultaneously strengthen the original semantic and occupancy tasks, we have decomposed the semantic occupancy prediction into two separate tasks. Tab. 4 shows an ablation study by altering the components used. Meanwhile, we also verify the enhancement of collaborative perception over single vehicle perception in terms of semantic occupancy. CoHFF for occupancy prediction. When focusing solely on binary occupancy predictions (as shown at Occ. Pred. in Tab. 4), we use voxels processed from raw LiDAR point clouds as a reference, and analyze the IoU in different semantic classes based on semantic occupancy in ground truth. It is observed that by utilizing an occupancy prediction task network to process depth predictions, the overall prediction accuracy is enhanced. Additionally, significant improvements in predicting large objects in occupancy results are noted by integrating features from a semantic segmentation task network, leading to an increased overall IoU. However, a concurrent decline in the mIoU is observed alongside the increase in IoU. This phenomenon is attributed to the influence of semantic features, which seem to steer the model towards prioritizing easily detectable categories, potentially at the expense of smaller or less distinct categories. Finally, through collaboration, the overall IoU and mIoU are further strengthened on the basis of task feature fusion. CoHFF for semantic segmentation. In our semantic segmentation task (as shown at Sem. Seg. in Tab. 4), after integrating features from occupancy prediction, we observe an approximate 2% increase in IoU, but a more substantial over 41% enhancement in mIoU. We attribute this improvement to the features derived from occupancy prediction, which seem to aid the easier detection of smaller-scale objects, thereby refining their semantic predictions. Consistent with the occupancy prediction task, the final collaboration further elevates the results of semantic segmentation. 18001",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 7,
      "text_raw": "Table 4. Component ablation study on occupancy prediction (Occ. Pred.), semantic segmentation (Sem. Seg.), and\nsemantic occupancy prediction (Sem. Occ. Pred.) tasks. The components include: Occupancy Prediction Task Net\n(OPTN), Semantic Segmentation Task Net (SSTN), Task Feature Fusion (TFF) and V2X Feature Fusion (VFF). The\ngray color in table cells indicates that the corresponding component is not applicable for the task.\nTask type\nOcc. Pred.\nSem. Seg.\nSem. Occ. Pred.\nOPTN\nRL 1\n✓\n✓\n✓\n✓\n✓\nSSTN\n✓\n✓\n✓\n✓\n✓\nTFF\n✓\n✓\n✓\n✓\n✓\n✓\nVFF (Collaboration)\n✓\n✓\n✓\nIoU (↑)\n49.35\n67.22\n76.62\n86.37\n41.30\n42.11\n51.38\n38.52\n50.46\nmIoU (↑)\n57.12\n64.01\n59.16\n69.15\n21.59\n30.51\n35.91\n24.85\n34.16\nBuilding (5.40%)\n67.50\n68.36\n41.29\n48.41\n9.65\n27.25\n15.06\n21.04\n25.72\nFence (0.85%)\n59.40\n62.05\n51.60\n65.01\n11.67\n30.29\n30.91\n20.50\n27.83\nTerrain (4.80%)\n43.60\n49.78\n68.21\n79.81\n51.18\n51.41\n61.98\n43.93\n48.30\nPole (0.39%)\n66.30\n70.67\n62.31\n64.12\n2.14\n36.80\n40.74\n31.66\n42.74\nRoad (40.53%)\n51.47\n77.78\n91.26\n93.00\n56.82\n60.02\n64.09\n55.83\n61.77\nSide walk (35.64%)\n45.46\n58.46\n74.37\n90.53\n25.22\n16.87\n36.03\n17.31\n39.62\nVegetation (1.11%)\n43.61\n44.43\n38.87\n41.57\n9.12\n22.13\n20.99\n14.49\n20.59\nVehicles (9.14%)\n41.40\n63.53\n59.52\n76.48\n59.58\n69.81\n75.88\n58.55\n63.28\nWall (2.01%)\n71.51\n79.35\n49.63\n81.20\n32.55\n39.80\n58.49\n33.30\n58.27\nGuard rail (0.04%)\n49.67\n46.03\n41.35\n43.33\n1.10\n1.95\n1.80\n1.54\n1.94\nTraffic signs (0.05%)\n68.98\n69.41\n52.35\n62.54\n0.00\n9.77\n11.69\n0.00\n16.33\nBridge (0.04%)\n76.53\n78.23\n79.08\n83.84\n0.00\n0.00\n13.30\n0.00\n3.53\n1 RL (Raw LiDAR) is used as a baseline for the evaluation on the task of occupancy prediction.\nCollaboration enhances semantic occupancy prediction.\nIn the final evaluation of our semantic occupancy prediction\n(see column Sem. Occ. Pred. in Tab. 4), we further demon-\nstrate the benefits brought by collaboration. By collabora-\ntion, the IoU for each category is improved. Notably, some\npreviously undetectable, low-prevalence categories such as\ntraffic signs and bridges can be detected after collaboration.\nUltimately, there is an approximate 31% increase in overall\nIoU and around a 37% enhancement in mIoU.\n5.4. Robustness with low communication budget\nIn Tab. 3, we increase the sparsification rate to mask the\nplane-based features transmitted by CAVs, achieving effi-\ncient V2X information exchange under a low communica-\ntion budget. The CoHFF model exhibits stable IoU perfor-\nmance across various levels of sparsification. Even when\nthe communication volume is shrinked by 97 ×, the accu-\nracy only decreases by 5% compared to the original. Mean-\nwhile, the mIoU drops by 15%. Despite this, due to the\nmodel’s training under collaborative supervision, it still out-\nperforms the non-collaborative approach.\n5.5. Visual analysis\nFig. 3 presents visual results from the CoHFF model, which\nare compared from multiple perspectives with the ground\ntruth data, i.e. the ground truth in the ego vehicle’s FoV\n(Ego GT) and the ground truth across all CAVs FoVs (Col-\nlaborative GT). It is evident that, overall, the model accu-\nrately predicts voxels in various classes such as roads, side-\nwalks, traffic signs, walls, and fences. We particularly focus\non vehicle predictions, as they are among the most critical\ncategories in road environment perception. For clarity, each\nvehicle object in the figure is numbered.\nVehicle geometry completion. The CoHFF model predicts\nmore complete vehicle objects than those in the Ego GT,\nsuch as vehicles 1, 3, 4, and 7. In some instances, the pre-\ndictions even surpass the completeness of vehicle shapes\nfound in Collaborative GT.\nOccluded vehicle detection. CoHFF successfully predicts\nvehicles outside of the FoV, such as vehicle 6, by utilizing\nminimal pixel information. This demonstrates that CoHFF\ncan effectively detect occluded vehicles.\n6. Conclusion\nIn this work, we explore the task of camera-based seman-\ntic occupancy prediction through the lens of collaborative\nperception. We introduce the CoHFF framework, which\nsignificantly enhances the perception performance by over\n30% through integrating features from different tasks and\nvarious CAVs. Since currently no dataset specifically de-\n18002\n",
      "text_clean": "Table 4. Component ablation study on occupancy prediction (Occ. Pred.), semantic segmentation (Sem. Seg.), and semantic occupancy prediction (Sem. Occ. Pred.) tasks. The components include: Occupancy Prediction Task Net (OPTN), Semantic Segmentation Task Net (SSTN), Task Feature Fusion (TFF) and V2X Feature Fusion (VFF). The gray color in table cells indicates that the corresponding component is not applicable for the task. Task type Occ. Pred. Sem. Seg. Sem. Occ. Pred. OPTN RL 1 ✓ ✓ ✓ ✓ ✓ SSTN ✓ ✓ ✓ ✓ ✓ TFF ✓ ✓ ✓ ✓ ✓ ✓ VFF (Collaboration) ✓ ✓ ✓ IoU (↑) 49.35 67.22 76.62 86.37 41.30 42.11 51.38 38.52 50.46 mIoU (↑) 57.12 64.01 59.16 69.15 21.59 30.51 35.91 24.85 34.16 Building (5.40%) 67.50 68.36 41.29 48.41 9.65 27.25 15.06 21.04 25.72 Fence (0.85%) 59.40 62.05 51.60 65.01 11.67 30.29 30.91 20.50 27.83 Terrain (4.80%) 43.60 49.78 68.21 79.81 51.18 51.41 61.98 43.93 48.30 Pole (0.39%) 66.30 70.67 62.31 64.12 2.14 36.80 40.74 31.66 42.74 Road (40.53%) 51.47 77.78 91.26 93.00 56.82 60.02 64.09 55.83 61.77 Side walk (35.64%) 45.46 58.46 74.37 90.53 25.22 16.87 36.03 17.31 39.62 Vegetation (1.11%) 43.61 44.43 38.87 41.57 9.12 22.13 20.99 14.49 20.59 Vehicles (9.14%) 41.40 63.53 59.52 76.48 59.58 69.81 75.88 58.55 63.28 Wall (2.01%) 71.51 79.35 49.63 81.20 32.55 39.80 58.49 33.30 58.27 Guard rail (0.04%) 49.67 46.03 41.35 43.33 1.10 1.95 1.80 1.54 1.94 Traffic signs (0.05%) 68.98 69.41 52.35 62.54 0.00 9.77 11.69 0.00 16.33 Bridge (0.04%) 76.53 78.23 79.08 83.84 0.00 0.00 13.30 0.00 3.53 1 RL (Raw LiDAR) is used as a baseline for the evaluation on the task of occupancy prediction. Collaboration enhances semantic occupancy prediction. In the final evaluation of our semantic occupancy prediction (see column Sem. Occ. Pred. in Tab. 4), we further demonstrate the benefits brought by collaboration. By collaboration, the IoU for each category is improved. Notably, some previously undetectable, low-prevalence categories such as traffic signs and bridges can be detected after collaboration. Ultimately, there is an approximate 31% increase in overall IoU and around a 37% enhancement in mIoU. 5.4. Robustness with low communication budget In Tab. 3, we increase the sparsification rate to mask the plane-based features transmitted by CAVs, achieving efficient V2X information exchange under a low communication budget. The CoHFF model exhibits stable IoU performance across various levels of sparsification. Even when the communication volume is shrinked by 97 ×, the accuracy only decreases by 5% compared to the original. Meanwhile, the mIoU drops by 15%. Despite this, due to the model’s training under collaborative supervision, it still outperforms the non-collaborative approach. 5.5. Visual analysis Fig. 3 presents visual results from the CoHFF model, which are compared from multiple perspectives with the ground truth data, i.e. the ground truth in the ego vehicle’s FoV (Ego GT) and the ground truth across all CAVs FoVs (Collaborative GT). It is evident that, overall, the model accurately predicts voxels in various classes such as roads, sidewalks, traffic signs, walls, and fences. We particularly focus on vehicle predictions, as they are among the most critical categories in road environment perception. For clarity, each vehicle object in the figure is numbered. Vehicle geometry completion. The CoHFF model predicts more complete vehicle objects than those in the Ego GT, such as vehicles 1, 3, 4, and 7. In some instances, the predictions even surpass the completeness of vehicle shapes found in Collaborative GT. Occluded vehicle detection. CoHFF successfully predicts vehicles outside of the FoV, such as vehicle 6, by utilizing minimal pixel information. This demonstrates that CoHFF can effectively detect occluded vehicles. 6. Conclusion In this work, we explore the task of camera-based semantic occupancy prediction through the lens of collaborative perception. We introduce the CoHFF framework, which significantly enhances the perception performance by over 30% through integrating features from different tasks and various CAVs. Since currently no dataset specifically de- 18002",
      "figure_captions": [
        {
          "bbox": [
            50.11199951171875,
            691.4893798828125,
            286.3650817871094,
            713.4069213867188
          ],
          "text": "Fig. 3 presents visual results from the CoHFF model, which are compared from multiple perspectives with the ground"
        }
      ],
      "images": []
    },
    {
      "page": 8,
      "text_raw": "Figure 3. Illustration of collaborative semantic occupancy prediction from multiple perspectives, compared to the ground truth in the\nego vehicle’s FoV and the collaborative FoV across CAVs. This visualization emphasizes the advanced object detection capabilities in\ncollaborative settings, particularly for objects obscured in the ego vehicle’s FoV, such as the vehicle with ID 6.\nsigned for collaborative semantic occupancy prediction ex-\nists, we also extend the OPV2V dataset with 3D semantic\noccupancy labels. Our experiments validate that collabo-\nration yields better semantic occupancy prediction results\nthan single-vehicle approaches.\nLimitation. Although we demonstrate the immense poten-\ntial of collaboration for semantic occupancy prediction us-\ning simulation data, its performance with real-world data\nremains to be verified. The collection and development of\na specialized dataset, repleted with semantic occupancy la-\nbels and derived from multi-agent perception scenarios in\nreal-world settings, are highly anticipated.\n7. Acknowledgements\nThis work was supported by the German Federal Ministry\nfor Digital and Transport (BMVI) in the project ”5GoIng –\n5G Innovation Concept Ingolstadt”.\n18003\n",
      "text_clean": "Figure 3. Illustration of collaborative semantic occupancy prediction from multiple perspectives, compared to the ground truth in the ego vehicle’s FoV and the collaborative FoV across CAVs. This visualization emphasizes the advanced object detection capabilities in collaborative settings, particularly for objects obscured in the ego vehicle’s FoV, such as the vehicle with ID 6. signed for collaborative semantic occupancy prediction exists, we also extend the OPV2V dataset with 3D semantic occupancy labels. Our experiments validate that collaboration yields better semantic occupancy prediction results than single-vehicle approaches. Limitation. Although we demonstrate the immense potential of collaboration for semantic occupancy prediction using simulation data, its performance with real-world data remains to be verified. The collection and development of a specialized dataset, repleted with semantic occupancy labels and derived from multi-agent perception scenarios in real-world settings, are highly anticipated. 7. Acknowledgements This work was supported by the German Federal Ministry for Digital and Transport (BMVI) in the project ”5GoIng – 5G Innovation Concept Ingolstadt”. 18003",
      "figure_captions": [
        {
          "bbox": [
            50.11199951171875,
            545.8819580078125,
            545.111083984375,
            576.766357421875
          ],
          "text": "Figure 3. Illustration of collaborative semantic occupancy prediction from multiple perspectives, compared to the ground truth in the ego vehicle’s FoV and the collaborative FoV across CAVs. This visualization emphasizes the advanced object detection capabilities in collaborative settings, particularly for objects obscured in the ego vehicle’s FoV, such as the vehicle with ID 6."
        }
      ],
      "images": [
        {
          "image_id": "p8_cluster_001",
          "page": 8,
          "file_path": "images/page_0008_cluster_001.png",
          "bbox": [
            50.55952835083008,
            71.99722290039062,
            540.1487426757812,
            535.0479736328125
          ],
          "caption": "A series of images showing different views of a street.",
          "detailed_caption": "The image shows a computer generated image of a map of a city with a lot of different types of buildings, trees, roads, vehicles, walls, and a person standing on the road. At the top of the image, there is text indicating that the map is a representation of the location of the city. The map is composed of various colors and shapes, giving it a unique and interesting look.",
          "ocr_text": "Front Left Right Rear1515Collaborative CT151515161616171717181718181819181919192020202120222222232324252525Cortin Quays Ego Multiview",
          "ocr_task": "<OCR>",
          "ocr_result": {
            "task": "<OCR>",
            "raw": "</s><s>FrontLeftRightRear1515Collaborative CT151515161616171717181718181819181919192020202120222222232324252525Cortin QuaysEgo Multiview</s>",
            "parsed": {
              "<OCR>": "FrontLeftRightRear1515Collaborative CT151515161616171717181718181819181919192020202120222222232324252525Cortin QuaysEgo Multiview"
            }
          },
          "paper_caption": "Figure 3. Illustration of collaborative semantic occupancy prediction from multiple perspectives, compared to the ground truth in the ego vehicle’s FoV and the collaborative FoV across CAVs. This visualization emphasizes the advanced object detection capabilities in collaborative settings, particularly for objects obscured in the ego vehicle’s FoV, such as the vehicle with ID 6.",
          "paper_caption_bbox": [
            50.11199951171875,
            545.8819580078125,
            545.111083984375,
            576.766357421875
          ]
        }
      ]
    },
    {
      "page": 9,
      "text_raw": "References\n[1] J.\nBehley\net\nal.\nTowards\n3D\nLiDAR-based\nse-\nmantic\nscene\nunderstanding\nof\n3D\npoint\ncloud\nse-\nquences: The SemanticKITTI Dataset.\nThe International\nJournal on Robotics Research, 40(8-9):959–967, 2021.\nDOI: 10.1177/02783649211006735. 3\n[2] Anh-Quan Cao and Raoul de Charette. MonoScene: Monoc-\nular 3D semantic scene completion.\nIn 2022 IEEE/CVF\nConference\non\nComputer\nVision\nand\nPattern\nRecog-\nnition\n(CVPR),\npages\n3991–4001.\nIEEE,\n2022.\nDOI: 10.1109/CVPR52688.2022.00396. 2, 3, 6\n[3] Cody Ceading, Ali Harakeh, Julia Chae, and Steven L.\nWaslander.\nCategorical\ndepth\ndistribution\nnet-\nwork for monocular 3D object detection.\nIn 2021\nIEEE/CVF Conference on Computer Vision and Pat-\ntern\nRecognition\n(CVPR),\npages\n8551–8560,\n2021.\nDOI: 10.1109/CVPR46437.2021.00845. 4, 6\n[4] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. Carla: An open urban driving\nsimulator. In Conference on Robot Learning, pages 1–16.\nPMLR, 2017. 5\n[5] Shaoheng Fang,\nZi Wang,\nYiqi Zhong,\nJunhao Ge,\nand Siheng Chen.\nTBP-Former: Learning temporal\nbird’s-eye-view pyramid for joint perception and predic-\ntion in vision-centric autonomous driving.\nIn 2023\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 1368–1378. IEEE, 2023.\nDOI: 10.1109/CVPR52729.2023.00138. 2, 3\n[6] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 12479–12488. IEEE, 2023.\nDOI: 10.1109/CVPR52729.2023.01201. 3, 4\n[7] Aditya Nalgunda Ganesh, Dhruval Pobbathi Badrinath,\nHarshith Mohan Kumar, Priya SS, and Surabhi Narayan.\nOCTraN: 3D occupancy convolutional transformer net-\nwork in unstructured traffic scenarios.\narXiv preprint\narXiv:2307.10934, 2023. 2, 3\n[8] R Hao et al.\nRcooper: A real-world large-scale dataset\nfor roadside cooperative perception.\narXiv preprint\narXiv:2403.10145, 2024. 2\n[9] Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian\nRupprecht, and Daniel Cremers. S4C: Self-supervised se-\nmantic scene completion with neural fields. arXiv preprint\narXiv:2310.07522, 2023. 3\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun.\nDeep\nresidual\nlearning\nfor\nimage\nrecogni-\ntion.\nIn 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 770–778, 2016.\nDOI: 10.1109/CVPR.2016.90. 6\n[11] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Si-\nheng Chen. Where2comm: Communication-efficient collab-\norative perception via spatial confidence maps.\nAdvances\nin Neural Information Processing Systems (NeurIPS), 35:\n4874–4886, 2022. 1, 2, 3, 4, 6\n[12] Yue Hu et al. Collaboration helps camera overtake LiDAR in\n3D detection. In 2023 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 9243–9252.\nIEEE, 2023. DOI: 10.1109/CVPR52729.2023.00892. 1, 2,\n3, 4, 5, 6\n[13] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie\nZhou, and Jiwen Lu.\nTri-perspective view for vision-\nbased 3D semantic occupancy prediction.\nIn 2023\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 9223–9232. IEEE, 2023.\nDOI: 10.1109/CVPR52729.2023.00890. 2, 3, 4, 5, 6\n[14] Haoyi Jiang et al.\nSymphonize 3D semantic scene com-\npletion with contextual instance queries.\narXiv preprint\narXiv:2306.15670, 2023. 2, 3\n[15] Yiming Li, Juexiao Zhang, Dekun Ma, Yue Wang, and Chen\nFeng. Multi-robot scene completion: Towards task-agnostic\ncollaborative perception. In Conference on Robot Learning,\npages 2062–2072. PMLR, 2023. 1, 2, 3\n[16] Yiming Li et al. Learning distilled collaboration graph for\nmulti-agent perception.\nAdvances in Neural Information\nProcessing Systems (NeurIPS), 34:29541–29552, 2021. 1,\n2, 6\n[17] Yiming Li et al. V2X-Sim: Multi-agent collaborative percep-\ntion dataset and benchmark for autonomous driving. IEEE\nRobotics and Automation Letters, 7(4):10914–10921, 2022.\nDOI: 10.1109/LRA.2022.3192802. 2\n[18] Yiming Li et al.\nSSCBench: A large-scale 3D semantic\nscene completion benchmark for autonomous driving. arXiv\npreprint arXiv:2306.09001, 2023. 3\n[19] Yiming Li et al.\nVoxFormer: Sparse voxel transformer\nfor camera-based 3D semantic scene completion.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 9087–9098. IEEE, 2023.\nDOI: 10.1109/CVPR52729.2023.00877. 2, 3, 6\n[20] Zhiqi Li et al.\nFB-OCC: 3D occupancy prediction based\non forward-backward view transformation. arXiv preprint\narXiv:2307.01492, 2023. 3\n[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n42(2):318–327, 2020. DOI: 10.1109/TPAMI.2018.2858826.\n5\n[22] Tsung-Yi Lin et al.\nFeature pyramid networks for object\ndetection.\nIn 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 2117–2125, 2017.\nDOI: 10.1109/CVPR.2017.106. 6\n[23] Jihao Liu et al. Towards better 3D knowledge transfer via\nmasked image modeling for multi-view 3D understanding.\narXiv preprint arXiv:2303.11325, 2023. 2, 3\n[24] Yingfei Liu et al. Petrv2: A unified framework for 3D per-\nception from multi-camera images. In 2023 IEEE/CVF In-\nternational Conference on Computer Vision (CVPR), pages\n3262–3272, 2023. 4\n[25] Yen-Cheng\nLiu,\nJunjiao\nTian,\nNathaniel\nGlaser,\nand\nZsolt\nKira.\nWhen2com:\nMulti-agent\npercep-\ntion\nvia\ncommunication\ngraph\ngrouping.\nIn\n2020\nIEEE/CVF Conference on Computer Vision and Pat-\n18004\n",
      "text_clean": "References [1] J. Behley et al. Towards 3D LiDAR-based semantic scene understanding of 3D point cloud sequences: The SemanticKITTI Dataset. The International Journal on Robotics Research, 40(8-9):959–967, 2021. DOI: 10.1177/02783649211006735. 3 [2] Anh-Quan Cao and Raoul de Charette. MonoScene: Monocular 3D semantic scene completion. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3991–4001. IEEE, 2022. DOI: 10.1109/CVPR52688.2022.00396. 2, 3, 6 [3] Cody Ceading, Ali Harakeh, Julia Chae, and Steven L. Waslander. Categorical depth distribution network for monocular 3D object detection. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8551–8560, 2021. DOI: 10.1109/CVPR46437.2021.00845. 4, 6 [4] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on Robot Learning, pages 1–16. PMLR, 2017. 5 [5] Shaoheng Fang, Zi Wang, Yiqi Zhong, Junhao Ge, and Siheng Chen. TBP-Former: Learning temporal bird’s-eye-view pyramid for joint perception and prediction in vision-centric autonomous driving. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1368–1378. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00138. 2, 3 [6] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12479–12488. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.01201. 3, 4 [7] Aditya Nalgunda Ganesh, Dhruval Pobbathi Badrinath, Harshith Mohan Kumar, Priya SS, and Surabhi Narayan. OCTraN: 3D occupancy convolutional transformer network in unstructured traffic scenarios. arXiv preprint arXiv:2307.10934, 2023. 2, 3 [8] R Hao et al. Rcooper: A real-world large-scale dataset for roadside cooperative perception. arXiv preprint arXiv:2403.10145, 2024. 2 [9] Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, and Daniel Cremers. S4C: Self-supervised semantic scene completion with neural fields. arXiv preprint arXiv:2310.07522, 2023. 3 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. DOI: 10.1109/CVPR.2016.90. 6 [11] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen. Where2comm: Communication-efficient collaborative perception via spatial confidence maps. Advances in Neural Information Processing Systems (NeurIPS), 35: 4874–4886, 2022. 1, 2, 3, 4, 6 [12] Yue Hu et al. Collaboration helps camera overtake LiDAR in 3D detection. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9243–9252. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00892. 1, 2, 3, 4, 5, 6 [13] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for visionbased 3D semantic occupancy prediction. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9223–9232. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00890. 2, 3, 4, 5, 6 [14] Haoyi Jiang et al. Symphonize 3D semantic scene completion with contextual instance queries. arXiv preprint arXiv:2306.15670, 2023. 2, 3 [15] Yiming Li, Juexiao Zhang, Dekun Ma, Yue Wang, and Chen Feng. Multi-robot scene completion: Towards task-agnostic collaborative perception. In Conference on Robot Learning, pages 2062–2072. PMLR, 2023. 1, 2, 3 [16] Yiming Li et al. Learning distilled collaboration graph for multi-agent perception. Advances in Neural Information Processing Systems (NeurIPS), 34:29541–29552, 2021. 1, 2, 6 [17] Yiming Li et al. V2X-Sim: Multi-agent collaborative perception dataset and benchmark for autonomous driving. IEEE Robotics and Automation Letters, 7(4):10914–10921, 2022. DOI: 10.1109/LRA.2022.3192802. 2 [18] Yiming Li et al. SSCBench: A large-scale 3D semantic scene completion benchmark for autonomous driving. arXiv preprint arXiv:2306.09001, 2023. 3 [19] Yiming Li et al. VoxFormer: Sparse voxel transformer for camera-based 3D semantic scene completion. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9087–9098. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00877. 2, 3, 6 [20] Zhiqi Li et al. FB-OCC: 3D occupancy prediction based on forward-backward view transformation. arXiv preprint arXiv:2307.01492, 2023. 3 [21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318–327, 2020. DOI: 10.1109/TPAMI.2018.2858826. 5 [22] Tsung-Yi Lin et al. Feature pyramid networks for object detection. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2117–2125, 2017. DOI: 10.1109/CVPR.2017.106. 6 [23] Jihao Liu et al. Towards better 3D knowledge transfer via masked image modeling for multi-view 3D understanding. arXiv preprint arXiv:2303.11325, 2023. 2, 3 [24] Yingfei Liu et al. Petrv2: A unified framework for 3D perception from multi-camera images. In 2023 IEEE/CVF International Conference on Computer Vision (CVPR), pages 3262–3272, 2023. 4 [25] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira. When2com: Multi-agent perception via communication graph grouping. In 2020 IEEE/CVF Conference on Computer Vision and Pat- 18004",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 10,
      "text_raw": "tern Recognition (CVPR), pages 4106–4115. IEEE, 2020.\nDOI: 10.1109/CVPR42600.2020.00416. 1, 2\n[26] Yen-Cheng Liu et al.\nWho2com:\nCollaborative per-\nception via learnable handshake communication.\nIn\n2020 IEEE International Conference on Robotics and\nAutomation\n(ICRA),\npages\n6876–6883.\nIEEE,\n2020.\nDOI: 10.1109/ICRA40945.2020.9197364. 1, 2\n[27] C. Ma et al.\nHolovic:\nLarge-scale dataset and bench-\nmark for multi-sensor holographic intersection and vehicle-\ninfrastructure cooperative. arXiv preprint arXiv:2403.02640,\n2024. 2\n[28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger.\nOccupancy net-\nworks:\nLearning 3D reconstruction in function space.\nIn 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 4460–4470,\n2019.\nDOI: 10.1109/CVPR.2019.00459. 2\n[29] Ruihang Miao et al.\nOccDepth: A depth-aware method\nfor 3D semantic scene completion.\narXiv preprint\narXiv:2302.13540, 2023. 2, 3\n[30] Chen Min et al.\nOcc-BEV: Multi-camera unified pre-\ntraining via 3D scene reconstruction.\narXiv preprint\narXiv:2305.18829, 2023. 2, 3\n[31] Luis Roldao, Raoul De Charette, and Anne Verroust-\nBlondet. 3D semantic scene completion: A survey. Interna-\ntional Journal of Computer Vision, 130(8):1978–2005, 2022.\n2\n[32] Zhiyu Tan et al. Ovo: Open-vocabulary occupancy. arXiv\npreprint arXiv:2305.16133, 2023. 2, 3\n[33] Xiaoyu Tian et al. Occ3D: A large-scale 3D occupancy pre-\ndiction benchmark for autonomous driving. arXiv preprint\narXiv:2304.14365, 2023. 3\n[34] Wenwen Tong et al. Scene as occupancy. In 2023 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n8406–8415, 2023. 2, 3\n[35] Tsun-Hsuan Wang et al. V2VNet: Vehicle-to-vehicle com-\nmunication for joint perception and prediction. In 2020 Eu-\nropean Conference on Computer Vision (ECCV), pages 605–\n621, Glasgow, UK, 2020. Springer.\nDOI: 10.1007/978-3-\n030-58536-5 36. 1, 2\n[36] Xiaofeng Wang et al. OpenOccupancy: A large scale bench-\nmark for surrounding semantic occupancy perception.\nIn\n2023 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 17850–17859, 2023. 3\n[37] Yuqi Wang, Yuntao Chen, Xingyu Liao, Lue Fan, and Zhaox-\niang Zhang.\nPanoOcc: Unified occupancy representation\nfor camera-based 3D panoptic segmentation. arXiv preprint\narXiv:2306.10013, 2023. 2, 3\n[38] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka.\nPET-\nNeuS: Positional encoding tri-planes for neural surfaces. In\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 12598–12607. IEEE, 2023.\nDOI: 10.1109/CVPR52729.2023.01212.\n[39] Yi Wei et al.\nSurroundOcc: Multi-camera 3D occupancy\nprediction for autonomous driving. In 2023 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n21729–21740, 2023. 2, 3\n[40] Runsheng Xu et al. CoBEVT: Cooperative bird’s eye view\nsemantic segmentation with sparse transformers.\narXiv\npreprint arXiv:2207.02202, 2022. 2, 5\n[41] Runsheng Xu et al. OPV2V: An open benchmark dataset and\nfusion pipeline for perception with vehicle-to-vehicle com-\nmunication. In 2022 International Conference on Robotics\nand Automation (ICRA), pages 2583–2589. IEEE, 2022.\nDOI: 10.1109/ICRA46639.2022.9812038. 2, 5\n[42] Runsheng Xu et al. V2X-VIT: Vehicle-to-everything coop-\nerative perception with vision transformer. In 2022 Euro-\npean Conference on Computer Vision (ECCV), pages 107–\n124. Springer, 2022. DOI: 10.1007/978-3-031-19842-7 7.\n1, 2, 6\n[43] Runsheng Xu et al.\nThe OpenCDA open-source ecosys-\ntem for cooperative driving automation research.\nIEEE\nTransactions on Intelligent Vehicles, 8(4):2698–2711, 2023.\nDOI: 10.1109/TIV.2023.3244948. 5\n[44] Runsheng Xu et al.\nV2V4Real: a real-world large-scale\ndataset for vehicle-to-vehicle cooperative perception.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 13712–13722. IEEE, 2023.\nDOI: 10.1109/CVPR52729.2023.01318. 2\n[45] Kun Yang et al.\nSpatio-temporal domain awareness for\nmulti-agent collaborative perception. In 2023 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n23383–23392, 2023. 1, 2\n[46] Jiawei Yao et al. NDC-Scene: Boost monocular 3D semantic\nscene completion in normalized device coordinates space. In\n2023 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 9455–9465, 2023. 2\n[47] Rongtian Ye, Fangyu Liu, and Liqiang Zhang. 3D depth-\nwise convolution: Reducing model parameters in 3D vision\ntasks.\nIn 32nd Canadian Conference on Artificial Intelli-\ngence (Canadian AI), Proc. 32, pages 186–199. Springer,\n2019. DOI: 10.1007/978-3-030-18305-9 15. 5, 6\n[48] Haibao Yu et al.\nDAIR-V2X: A large-scale dataset for\nvehicle-infrastructure cooperative 3D object detection.\nIn\n2022 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 21361–21370. IEEE, 2022.\nDOI: 10.1109/CVPR52688.2022.02067. 2\n[49] Haibao Yu et al. V2X-Seq: A large-scale sequential dataset\nfor vehicle-infrastructure cooperative perception and fore-\ncasting. In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5486–5495. IEEE,\n2023. DOI: 10.1109/CVPR52729.2023.00531. 2\n[50] Haibao Yu et al. Vehicle-infrastructure cooperative 3D ob-\nject detection via feature flow prediction.\narXiv preprint\narXiv:2303.10552, 2023. 1, 2\n[51] Yunpeng Zhang, Zheng Zhu, and Dalong Du. OccFormer:\nDual-path transformer for vision-based 3D semantic occu-\npancy prediction. In 2023 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 9433–9443, 2023.\n2, 3\n[52] Zaibin Zhang, Lijun Wang, Yifan Wang, and Huchuan Lu.\nBEV-IO: Enhancing bird’s-eye-view 3D detection with in-\nstance occupancy. arXiv preprint arXiv:2305.16829, 2023.\n2, 3\n18005\n",
      "text_clean": "tern Recognition (CVPR), pages 4106–4115. IEEE, 2020. DOI: 10.1109/CVPR42600.2020.00416. 1, 2 [26] Yen-Cheng Liu et al. Who2com: Collaborative perception via learnable handshake communication. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 6876–6883. IEEE, 2020. DOI: 10.1109/ICRA40945.2020.9197364. 1, 2 [27] C. Ma et al. Holovic: Large-scale dataset and benchmark for multi-sensor holographic intersection and vehicleinfrastructure cooperative. arXiv preprint arXiv:2403.02640, 2024. 2 [28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4460–4470, 2019. DOI: 10.1109/CVPR.2019.00459. 2 [29] Ruihang Miao et al. OccDepth: A depth-aware method for 3D semantic scene completion. arXiv preprint arXiv:2302.13540, 2023. 2, 3 [30] Chen Min et al. Occ-BEV: Multi-camera unified pretraining via 3D scene reconstruction. arXiv preprint arXiv:2305.18829, 2023. 2, 3 [31] Luis Roldao, Raoul De Charette, and Anne VerroustBlondet. 3D semantic scene completion: A survey. International Journal of Computer Vision, 130(8):1978–2005, 2022. 2 [32] Zhiyu Tan et al. Ovo: Open-vocabulary occupancy. arXiv preprint arXiv:2305.16133, 2023. 2, 3 [33] Xiaoyu Tian et al. Occ3D: A large-scale 3D occupancy prediction benchmark for autonomous driving. arXiv preprint arXiv:2304.14365, 2023. 3 [34] Wenwen Tong et al. Scene as occupancy. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8406–8415, 2023. 2, 3 [35] Tsun-Hsuan Wang et al. V2VNet: Vehicle-to-vehicle communication for joint perception and prediction. In 2020 European Conference on Computer Vision (ECCV), pages 605– 621, Glasgow, UK, 2020. Springer. DOI: 10.1007/978-3- 030-58536-5 36. 1, 2 [36] Xiaofeng Wang et al. OpenOccupancy: A large scale benchmark for surrounding semantic occupancy perception. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 17850–17859, 2023. 3 [37] Yuqi Wang, Yuntao Chen, Xingyu Liao, Lue Fan, and Zhaoxiang Zhang. PanoOcc: Unified occupancy representation for camera-based 3D panoptic segmentation. arXiv preprint arXiv:2306.10013, 2023. 2, 3 [38] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. PETNeuS: Positional encoding tri-planes for neural surfaces. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12598–12607. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.01212. [39] Yi Wei et al. SurroundOcc: Multi-camera 3D occupancy prediction for autonomous driving. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 21729–21740, 2023. 2, 3 [40] Runsheng Xu et al. CoBEVT: Cooperative bird’s eye view semantic segmentation with sparse transformers. arXiv preprint arXiv:2207.02202, 2022. 2, 5 [41] Runsheng Xu et al. OPV2V: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication. In 2022 International Conference on Robotics and Automation (ICRA), pages 2583–2589. IEEE, 2022. DOI: 10.1109/ICRA46639.2022.9812038. 2, 5 [42] Runsheng Xu et al. V2X-VIT: Vehicle-to-everything cooperative perception with vision transformer. In 2022 European Conference on Computer Vision (ECCV), pages 107– 124. Springer, 2022. DOI: 10.1007/978-3-031-19842-7 7. 1, 2, 6 [43] Runsheng Xu et al. The OpenCDA open-source ecosystem for cooperative driving automation research. IEEE Transactions on Intelligent Vehicles, 8(4):2698–2711, 2023. DOI: 10.1109/TIV.2023.3244948. 5 [44] Runsheng Xu et al. V2V4Real: a real-world large-scale dataset for vehicle-to-vehicle cooperative perception. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13712–13722. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.01318. 2 [45] Kun Yang et al. Spatio-temporal domain awareness for multi-agent collaborative perception. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 23383–23392, 2023. 1, 2 [46] Jiawei Yao et al. NDC-Scene: Boost monocular 3D semantic scene completion in normalized device coordinates space. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9455–9465, 2023. 2 [47] Rongtian Ye, Fangyu Liu, and Liqiang Zhang. 3D depthwise convolution: Reducing model parameters in 3D vision tasks. In 32nd Canadian Conference on Artificial Intelligence (Canadian AI), Proc. 32, pages 186–199. Springer, 2019. DOI: 10.1007/978-3-030-18305-9 15. 5, 6 [48] Haibao Yu et al. DAIR-V2X: A large-scale dataset for vehicle-infrastructure cooperative 3D object detection. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 21361–21370. IEEE, 2022. DOI: 10.1109/CVPR52688.2022.02067. 2 [49] Haibao Yu et al. V2X-Seq: A large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5486–5495. IEEE, 2023. DOI: 10.1109/CVPR52729.2023.00531. 2 [50] Haibao Yu et al. Vehicle-infrastructure cooperative 3D object detection via feature flow prediction. arXiv preprint arXiv:2303.10552, 2023. 1, 2 [51] Yunpeng Zhang, Zheng Zhu, and Dalong Du. OccFormer: Dual-path transformer for vision-based 3D semantic occupancy prediction. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9433–9443, 2023. 2, 3 [52] Zaibin Zhang, Lijun Wang, Yifan Wang, and Huchuan Lu. BEV-IO: Enhancing bird’s-eye-view 3D detection with instance occupancy. arXiv preprint arXiv:2305.16829, 2023. 2, 3 18005",
      "figure_captions": [],
      "images": []
    },
    {
      "page": 11,
      "text_raw": "[53] Yin Zhou and Oncel Tuzel.\nVoxelNet:\nEnd-to-end\nlearning for point cloud based 3D object detection.\nIn\n2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 4490–4499,\n2018.\nDOI: 10.1109/CVPR.2018.00472. 2\n[54] Xizhou Zhu et al.\nDeformable DETR: Deformable trans-\nformers for end-to-end object detection.\nIn 2021 Inter-\nnational Conference on Learning Representations (ICLR),\n2021. 4, 5, 6\n[55] W. Zimmer et al.\nTUMTraf V2X cooperative perception\ndataset. arXiv preprint arXiv:2403.01316, 2024. 2\n18006\n",
      "text_clean": "[53] Yin Zhou and Oncel Tuzel. VoxelNet: End-to-end learning for point cloud based 3D object detection. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4490–4499, 2018. DOI: 10.1109/CVPR.2018.00472. 2 [54] Xizhou Zhu et al. Deformable DETR: Deformable transformers for end-to-end object detection. In 2021 International Conference on Learning Representations (ICLR), 2021. 4, 5, 6 [55] W. Zimmer et al. TUMTraf V2X cooperative perception dataset. arXiv preprint arXiv:2403.01316, 2024. 2 18006",
      "figure_captions": [],
      "images": []
    }
  ]
}